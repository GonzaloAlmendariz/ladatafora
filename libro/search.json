[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R para el análisis de datos",
    "section": "",
    "text": "Agradecimientos\nDeseo expresar, en primer lugar, mi más profundo agradecimiento a mi abuela Nonoy, quien ha sido un pilar fundamental de apoyo emocional durante los momentos más difíciles de mi vida personal, en paralelo al proceso de elaboración de este libro. Su compañía constante, su fortaleza y su amor incondicional han sostenido este proyecto desde sus cimientos. Sin ella, nada de esto habría sido posible.\nExtiendo también mi gratitud a mis hermanas, Ale y Francesca; a mi padre, José Luis; a mi mejor amigo, Diego; y a mi gata Antonia, por brindarme su respaldo emocional y compartir reflexiones sinceras justo cuando más las necesitaba. Asimismo, deseo honrar la memoria de mi madre Sandra, quien, aunque ya no me acompaña físicamente, permanece como la piedra angular de mis convicciones. De ella heredé el amor por la divulgación del conocimiento, la compasión hacia los demás y una profunda valoración por la educación. Ha sido, y sigue siendo, la mayor fuente de inspiración en mi vida.\nQuiero dedicar un agradecimiento especial a un grupo de estudiantes sanmarquinos que, con generosidad y paciencia, leyeron el desarrollo de los capítulos y me ofrecieron valiosas observaciones. Gracias a sus comentarios, fue posible enriquecer los contenidos y acercar aún más esta obra a su público objetivo. Ellos son:\nGabriel Stefano Ancajima Raymundo\nNicole Melody Acosta Cuya\nLuis Buleje Aguirre\nMatías Abel Brito La Madrid\nXiomara Alessandra Esquia Vásquez\nSandra Isabel Ramos Castillo\nGénesis Lorena Paredes Prado\nJhassury Nallel Viera Noriega\nA cada uno de ellos, les extiendo mi más sincero y sentido agradecimiento.\nFinalmente, agradezco a la artista y amiga Rafaela Prado (IG: lapizela) por el diseño de la portada, así como por dar vida a Flor a lo largo de estas páginas.",
    "crumbs": [
      "Agradecimientos"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introducción",
    "section": "",
    "text": "Desde los primeros ciclos de mi formación universitaria se nos transmitió con claridad que la estadística constituye una herramienta fundamental en el quehacer politológico. Su dominio no solo amplía nuestras posibilidades analíticas, sino que habilita un conjunto de métodos rigurosos indispensables para la investigación científica en Ciencia Política. No obstante, al cursar las asignaturas correspondientes, pude constatar con desilusión serias deficiencias en la calidad de la enseñanza, así como una falta de interés docente por fomentar una comprensión intuitiva, crítica y aplicada de los conceptos estadísticos.\nInicialmente consideré que esta situación respondía a una coyuntura específica, atribuible únicamente a mi promoción sanmarquina. Sin embargo, con el tiempo comprendí que se trataba de una carencia estructural que también afectaba a promociones posteriores. Fue a partir de allí que decidí iniciar un proceso de formación autodidacta, muchas veces enfrentándome a la escasez de materiales disponibles en español. Afortunadamente, el conocimiento del idioma inglés, adquirido en mi juventud, me permitió acceder a bibliografía especializada y a comunidades académicas globales. Aún sin preverlo, este proceso fue consolidando en mí una vocación sostenida por la estadística y la divulgación académica.\nMi objetivo, desde entonces, ha sido claro: articular mi interés por compartir conocimiento con el desarrollo de habilidades en estadística y programación, con el fin de aportar a una comprensión accesible, crítica y bien fundamentada de las herramientas estadísticas en español. Considero esencial que el estudiantado no solo aprenda a aplicar técnicas, sino también a comprender su justificación teórica, sus alcances interpretativos y las limitaciones inherentes a sus supuestos.\nLa idea de este libro surgió a inicios de 2024, cuando tuve la oportunidad de ejercer como asistente de cátedra en el curso de estadística con R en la Escuela Profesional de Ciencia Política de la Universidad Nacional Mayor de San Marcos. A ello se sumó mi participación como facilitador en el Centro de Estudios Métrica 23 y en el Taller Reforma Política “Manuel Lorenzo de Vidaurre”, espacios que me permitieron dictar clases presenciales y virtuales sobre teoría estadística y su aplicación práctica en R. La preparación de estos espacios formativos exigió la elaboración de materiales didácticos que, con el tiempo, se fueron consolidando en la propuesta que hoy presento: un compendio introductorio de estadística aplicada, concebido desde la Ciencia Política y orientado a un público amplio, con un enfoque pedagógico y accesible.\nEste proyecto nace también como una respuesta a las inequidades en el acceso al conocimiento. He sido testigo de cómo los recursos de calidad suelen estar restringidos a quienes pueden costear una formación privada o acceder a plataformas educativas con costos elevados. En ese sentido, este libro fue concebido como un recurso libre y lo seguirá siendo. Bajo ninguna circunstancia se restringirá su acceso, porque parte de la convicción de que la educación debe estar al servicio de todas y todos.\nNo poseo, aún, estudios de posgrado ni una trayectoria académica consolidada, pero sí un profundo compromiso con la mejora de la educación estadística en nuestra disciplina. Si usted, lectora o lector, identifica errores, imprecisiones o aspectos susceptibles de mejora, agradeceré profundamente que me lo comunique. Toda retroalimentación será bienvenida y utilizada para seguir perfeccionando este esfuerzo. Le deseo un recorrido enriquecedor por estas páginas: no será un camino sencillo, pero confío en que pueda resultar valioso y estimulante.\nAtentamente,\nGonzalo Almendariz Villanueva",
    "crumbs": [
      "Introducción"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Estructura del libro",
    "section": "",
    "text": "Este libro es para cualquier persona que desee tener una comprensión sólida de los principios y herramientas estadísticas que se utilizan habitualmente en el ámbito de la investigación, la administración pública y el mundo empresarial. Sin embargo, mi formación como politólogo me ha llevado inevitablemente a enfatizar los aspectos prácticos y empíricos, de modo que, aunque describo y explico conceptos matemáticos importantes, el libro está diseñado en última instancia para los científicos sociales que buscan trabajar con una base general y práctica para usar R en el análisis estadístico. Así que no es un libro muy profundo matemáticamente. Aquello no implica que no se haya hecho especial énfasis en que el lector pueda saber considerar herramientas pertinentes, interpretar sus resultados y reconocer las limitaciones que estos acarrean.\nEste libro está organizado en nueve capítulos que abarcan un recorrido intencional (y espero ameno) desde los fundamentos de la estadística y la programación en R, hasta el uso de pruebas estadísticas, modelado y visualización de datos. Aunque la estructura ha sido concebida para que el lector pueda asimilar gradualmente los conceptos necesarios, también se ha procurado que cada capítulo conserve cierta autonomía. Así, el lector tiene la libertad de situarse en el punto del libro que le resulte más pertinente o familiar, siendo consciente, sin embargo, de que el nivel de dificultad incrementa de forma progresiva y que ciertos contenidos requerirán mayor dedicación al introducir nuevos conceptos. Sin más que añadir, procedo con la estructura del libro.\nCapítulo 1: Fundamentos en Estadística\nEste capítulo marca el inicio formal del recorrido. Se abordan conceptos centrales como la definición de datos, la naturaleza de las variables y la implicancia de la variabilidad, todos ellos fundamentales para comprender qué se mide y cómo. También se presenta una introducción sistemática a las técnicas de muestreo, con énfasis en sus modalidades más utilizadas. Finalmente, se introduce y explica la diferencias entre la estadística descriptiva y la estadística inferencial, estableciendo así el marco conceptual para todo lo que vendrá después.\nCapítulo 2: Fundamentos en R\nEl lenguaje R será nuestra herramienta principal a lo largo del libro. En este capítulo se parte de cero: guiamos paso a paso por la instalación de R y RStudio, exploramos su interfaz y aprendemos a organizar un entorno de trabajo estructurado y reproducible. Se introducen conceptos esenciales como objetos, vectores, funciones, paquetes y operaciones básicas, todo explicado de forma accesible pero rigurosa. El enfoque es práctico: se asume que el lector no tiene experiencia previa programando, pero que sí posee el interés genuino por desarrollar habilidades reales para analizar datos con R.\nCapítulo 3: Integración\nEste capítulo representa el corazón operativo del libro. Aquí se empieza a trabajar con bases de datos reales en estructuras tipo data frame, se introduce el flujo de trabajo del Tidyverse y se consolidan los aprendizajes anteriores. Se busca que el lector aprenda a importar, limpiar, transformar y resumir datos utilizando herramientas como readr, janitor, dplyr y tidyr. También se aborda de forma introductoria la visualización con ggplot2, cubriendo los fundamentos de su gramática y también estrategias visuales, diseño de gráficos y mejores prácticas. Es en este capítulo donde las piezas comienzan a encajar: el lenguaje, los datos, las transformaciones y la visualización se integran de forma coherente.\nCapítulo 4: Estadística descriptiva\nEste capítulo se ocupa de todo lo relacionado con estadística descriptiva. Se desarrollan medidas de tendencia central (media, mediana, moda), dispersión (rango, varianza, desviación estándar), posición (cuartiles, percentiles) y forma de la distribución (asimetría, curtosis). También se examina el resumen de variables categóricas a través de tablas de frecuencia y proporciones. Se aplican estos conceptos en R, tanto con funciones específicas como mediante visualizaciones elaboradas con ggplot2. El capítulo culmina con una sección de análisis exploratorio univariado y bivariado, que permite al lector identificar patrones, inconsistencias y relaciones antes de avanzar hacia la inferencia.\nCapítulo 5: Probabilidad\nLa probabilidad es la base de toda la inferencia estadística y aquí se busca generar una base sólida para poder comprender sus técnicas. Empezamos definiendo la probabilidad desde un enfoque frecuentista y poniendo en práctica los conceptos de la probabilidad teórica y la experimental. Proseguimos con las distribuciones de probabilidad y sus características. Se discuten distribuciones discretas y continuas (binomial, Poisson, normal), cómo trabajar con ellas en R. Luego, se hace mayor énfasis en la distribución normal, la regla empírica, cómo interpretar valores z y las áreas bajo la curva. Finalmente se introduce el teorema del límite central como puente para lo demás capítulos.\nCapítulo 6: Estadística inferencial\nA partir de aquí entramos en el terreno de las decisiones y generalizaciones. Este capítulo retoma el concepto del teorema del límite central y se concentra en su aplicación para la estimación por intervalos y el contraste de hipótesis. Se abordan también elementos clave como la significancia estadística, los valores p, los errores tipo I y II, y la validación de supuestos, todo ello acompañado de visualizaciones que buscan facilitar la comprensión e interpretación de los resultados. Se introduce la distribución t de Student y su uso en pruebas de hipótesis con muestras pequeñas. A lo largo del capítulo, se entrelazan los fundamentos teóricos con su aplicación en R, con el objetivo de que el lector pueda evaluar la validez de una afirmación estadística sin perder de vista la importancia del contexto.\nCapítulo 7: Comparando grupos\nAquí se desarrollan técnicas para comparar grupos entre sí. Se estudian las diferencias entre medias y proporciones, tanto en muestras independientes como relacionadas. Se explica cómo aplicar pruebas t, ANOVA y pruebas de chi-cuadrado, abordando sus supuestos y condiciones de uso. También se introducen métodos no paramétricos para situaciones en las que los supuestos tradicionales no se cumplen. Finalmente, el lector es introducido al paquete infer, que permite hacer inferencia estadística mediante simulaciones.\nCapítulo 8: Regresión\nLa regresión es una de las herramientas más populares y versátiles del análisis estadístico y no podía quedar fuera. Este capítulo comienza con la regresión lineal simple: se presenta la ecuación que la define, sus parámetros fundamentales, el proceso de estimación y las principales limitaciones del modelo. Se abordan conceptos clave como la desviación estándar condicional, la relación entre correlación y pendiente, el coeficiente de determinación y la interpretación práctica de cada término. A partir de allí, se introduce la regresión múltiple, incorporando el control de variables, el problema de la multicolinealidad y los modelos con interacción. El capítulo culmina con un breve acercamiento a los modelos lineales generalizados (GLM).\nCapítulo 9: Glosario de funciones útiles\nEste último capítulo funciona como un apéndice técnico. No está pensado para ser leído de principio a fin, sino como una guía de consulta rápida. Se organiza por bloques temáticos que incluyen funciones para vectores, data frames, limpieza de datos, manipulación con dplyr, visualización con ggplot2, paquetes complementarios a ggplot2 para personalizar gráficos, y análisis exploratorio con dlookr. Se busca que el lector pueda encontrar rápidamente el código o función que necesita aplicar en su propio proyecto.\nAdicionalmente, verás que a lo largo del libro, he incluido ejemplos inspirados cuestiones típicas de un investigador social los cuales están protagonizados por Flor, un personaje diseñado para acompañar al lector en los primeros capítulos y mostrar, desde las ciencias sociales, cómo aplicar las herramientas estadísticas para resolver problemas concretos.\nFinalmente, mi objetivo no es que repitas fórmulas ni copies código: quiero que comprendas qué hace cada herramienta, por qué es adecuada en determinado contexto y cuáles son sus limitaciones. Al final, lo que importa no es ejecutar scripts sin errores —porque los vas a cometer, y muchos—, sino construir argumentos sólidos a partir de datos.",
    "crumbs": [
      "Estructura del libro"
    ]
  },
  {
    "objectID": "fundamentos.html",
    "href": "fundamentos.html",
    "title": "1  Fundamentos de la Estadística",
    "section": "",
    "text": "1.1 La estadística y las ciencias sociales\nComo investigador social, te enfrentas al desafío de comprender la realidad social, un reto que involucra la interacción de las personas, las instituciones y los sistemas que las estructuran. Nuestro objeto de estudio no son solo los números, sino las experiencias humanas, los comportamientos y las estructuras que dan forma a nuestras sociedades.\nEn este proceso, la estadística nos permite organizar y describir los datos de manera clara y estructurada, así como poder aplicar las herramientas necesarias para inferir conclusiones válidas a partir de la información disponible. A través de métodos estadísticos, podemos identificar tendencias, asociaciones y relaciones entre variables que puede que no estén a simple vista o deseemos poder tener un una forma objetiva de probarlo. La estadística intenta ser un puente entre la complejidad de los fenómenos sociales y la capacidad de generar conocimiento preciso y fundamentado. Nos ayuda a transformar datos dispersos en información útil y a construir modelos que nos permitan hacer predicciones y recomendaciones basadas en nuestros datos.\nPara hacer el viaje más ameno, en los primeros capítulos nos aventuraremos junto a Flor, una joven investigadora social que, como tú, está comenzando. Flor será nuestra compañera, mostrando, a través de sus propias experiencias y aprendizajes, los desafíos, dudas y obstáculos que surgen al aplicar la estadística en la investigación social.\nLa ciencia estadística brinda un conjunto de herramientas para comprender una realidad caracterizada por la incertidumbre. Pero, ¿a qué herramientas nos referimos? Agresti (2018) divide los métodos en tres categorías: diseño, descripción e inferencia.\nEl diseño es generalmente el primer paso en la investigación. Se refiere a planificar un estudio para recopilar y analizar pruebas empíricas de manera que el investigador pueda responder a las preguntas planteadas (Ragin 2007). Por ejemplo, en una encuesta, el diseño determina cómo seleccionar a las personas que se entrevistarán y cómo elaborar el cuestionario que se administrará.\nLa descripción implica resumir los datos para facilitar su comprensión y permitir extraer información útil. Los datos en bruto, como una lista completa de observaciones persona por persona, son difíciles de interpretar. Por ello, usamos herramientas como gráficos, tablas o medidas resumen (como promedios y porcentajes) para condensar la información.\nPor ejemplo, si realizamos una encuesta sobre la opinión pública respecto a una política gubernamental, en lugar de listar cada respuesta individual, podríamos presentar los resultados mediante un gráfico de barras que muestre el porcentaje de personas a favor, en contra o neutrales. Estos resúmenes visuales y numéricos se denominan estadísticas descriptivas. Utilizamos las estadísticas descriptivas para simplificar los datos a una forma más manejable y comprensible para describir el comportamiento de nuestros datos y explorar posibles asociaciones importantes.\nLa inferencia es el proceso de usar datos para sacar conclusiones sobre algo más amplio. Nos ayuda a entender y generalizar sobre aquello que está sucediendo en el presente y también nos permite predecir cómo podrían comportarse los datos en el futuro o bajo diferentes condiciones. Este enfoque es especialmente útil cuando no podemos observar todas los casos posibles, pero contamos con una muestra representativa.\nPor ejemplo, si realizamos una encuesta a un grupo representativo de personas y descubrimos que el 60% de los participantes apoyan una política, podemos usar esta información para estimar que algo similar podría ocurrir en toda la población. Además, la inferencia puede ayudarnos a predecir cómo cambiarán las ventas de un producto el próximo mes o a entender cómo una nueva medicina podría afectar a los pacientes en un ensayo clínico. Hablaremos más sobre la representatividad en el apartado del muestreo (4.4)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentos de la Estadística</span>"
    ]
  },
  {
    "objectID": "fundamentos.html#la-estadística-y-las-ciencias-sociales",
    "href": "fundamentos.html#la-estadística-y-las-ciencias-sociales",
    "title": "1  Fundamentos de la Estadística",
    "section": "",
    "text": "Flor te da la bienvenida",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentos de la Estadística</span>"
    ]
  },
  {
    "objectID": "fundamentos.html#datos",
    "href": "fundamentos.html#datos",
    "title": "1  Fundamentos de la Estadística",
    "section": "1.2 Datos",
    "text": "1.2 Datos\nEn la investigación social y científica, los datos son nuestra principal fuente de información. Son mediciones, observaciones o registros recopilados para analizar y comprender fenómenos específicos. Por ejemplo, el número de personas en Perú, los departamentos donde nacieron, el nivel de educación en una población, o la relación entre la tasa de empleo y la educación, son todos ejemplos de datos.\nSin datos, nuestra capacidad para explorar, describir e interpretar la realidad sería limitada. Estos son la clave para transformar preguntas en respuestas y evidencia en conocimiento.\nLos datos se generan a partir de mediciones realizadas sobre individuos, grupos, o eventos. Estos pueden recopilarse mediante encuestas, censos, observaciones directas, registros administrativos o experimentos.\nPara poder sistematizar la información debemos tener claro la diferencia entre unidad de observación, observación y variable:\n\nUnidad de observación: Es la entidad sobre la que se recopilan los datos, como una persona, un hogar, una empresa, o una región.\n\nEjemplo: Cada votante en una elección sería una unidad de observación.\n\nObservación: Es un conjunto específico de valores medidos o registrados para una unidad de observación. Generalmente, cada observación corresponde a una fila en un conjunto de datos tabular.\n\nEjemplo: La información de un votante, incluyendo su edad, género y su preferencia electoral, sería una observación.\n\nVariable: Es una característica o atributo medido sobre las unidades de datos. Cada columna en un conjunto de datos representa una variable.\n\nEjemplo: “Edad”, “Nivel educativo” y “Preferencia electoral” son variables.\n\n\nUn conjunto de datos es una colección organizada de todas las observaciones recolectadas. Por lo general, se presenta en formato tabular, donde las filas representan las observaciones y las columnas representan las variables.\n\n\n\nElaboración propia",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentos de la Estadística</span>"
    ]
  },
  {
    "objectID": "fundamentos.html#variables-y-variabilidad",
    "href": "fundamentos.html#variables-y-variabilidad",
    "title": "1  Fundamentos de la Estadística",
    "section": "1.3 Variables y variabilidad",
    "text": "1.3 Variables y variabilidad\nLas variables representan las características o propiedades de los elementos que se están estudiando. Por ejemplo, en un estudio sobre el comportamiento electoral, variables como la edad, el nivel educativo, el ingreso y las preferencias políticas son fundamentales para entender posibles patrones de comportamiento. Cada observación (persona, región, etc.) es única, por lo que la información que se recolecta sobre sus características varía de una observación a otra. Se les denomina “variables” precisamente porque su valor no es fijo, sino que puede cambiar (variar) entre diferentes observaciones o a lo largo del tiempo. Este cambio o variación es precisamente lo que nos interesa, ya que permite entender hasta qué punto esta variación es producto de la aleatoriedad natural que existe al no ser todos iguales, o si podemos identificar factores determinantes que influyen en las características observadas.\n\n1.3.1 Tipos de variables\nEs importante también tener en cuenta que no todas las características son medidas de la misma forma. Los datos recolectados pueden ser de diferentes tipos, lo cual influye en cómo se analizan e interpretan. Los dos tipos principales de datos son numéricos y categóricos.\nVariables Numéricas\nLos datos numéricos se refieren a cualquier tipo de información que puede ser medida y expresada en números. Estos datos pueden ser manipulados matemáticamente y se dividen en dos subcategorías:\n\nVaribles Continuas: Pueden tomar cualquier valor dentro de un rango continuo. Estos datos no están restringidos a valores enteros y pueden incluir fracciones y decimales. Por ejemplo:\n\nIngreso mensual de un hogar: El ingreso puede variar desde cero hasta cualquier cantidad máxima, sin limitaciones específicas. Puede ser 2000.50, 3000.75, etc.\nTiempo dedicado a ver televisión por semana: Este tiempo puede ser fraccionado en minutos y segundos, variando de forma continua. Por ejemplo, 5.5 horas, 10.25 horas, etc.\n\nVaribles Discretas: Toman valores específicos y separados. Estos datos son contables y no pueden ser fraccionados. Por ejemplo:\n\nNúmero de votantes en una elección: Se cuenta en números enteros (1000, 2000, etc.).\nConteo de delitos en una ciudad: El número de incidentes reportados se registra en valores enteros como 150, 300, etc.\n\n\n\n\nVaribles Categóricas\n\nLos datos categóricos se refieren a información que puede ser dividida en grupos distintos, cada uno de los cuales representa una categoría o una cualidad diferente. Estos datos no tienen un valor numérico intrínseco y se dividen en tres subcategorías:\n\nVaribles Nominales: No tienen un orden intrínseco. Estos datos representan categorías sin un orden específico. Por ejemplo:\n\nNacionalidad: Perú, Colombia, Venezuela.\nPreferencias políticas: Liberal, conservador, independiente.\n\nVaribles Binaria: Estos datos pueden ser tratados como casos especiales de las variables nominales. Solo pueden tomar uno de dos valores posibles. También llamados variables dicotómicas, representando una de dos categorías opuestas. Por ejemplo:\n\n0/1: Presencia o ausencia de una característica.\nSí/No: Respuesta a una pregunta específica.\nVerdadero/Falso: Estado de una condición.\n\nVaribles Ordinales: Tienen un orden o jerarquía implícita. Aunque las categorías son ordenadas, la diferencia entre ellas no necesariamente es igual. Por ejemplo:\n\nEscala de satisfacción con el gobierno: Muy insatisfecho, insatisfecho, neutral, satisfecho, muy satisfecho.\nNivel de educación: Primaria, secundaria, universidad, posgrado.\n\n\nEs muy importante tener un conocimiento claro de las variables presentes en el estudio y su forma de medición. Finalmente, son el tipo de variables las que determinarán el tipo de análisis que se va a realizar y la herramienta estadística adecuada.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentos de la Estadística</span>"
    ]
  },
  {
    "objectID": "fundamentos.html#muestreo",
    "href": "fundamentos.html#muestreo",
    "title": "1  Fundamentos de la Estadística",
    "section": "1.4 Muestreo",
    "text": "1.4 Muestreo\nCuando investigamos fenómenos sociales, una de las primeras preguntas que surge es cómo recolectar datos de una población amplia sin la necesidad de entrevistar a cada uno de sus miembros. Esta tarea puede ser especialmente compleja en ciudades grandes o regiones extensas, donde abarcar a toda la población es prácticamente imposible debido a las limitaciones de tiempo, recursos y logística. Por ello, el muestreo es una herramienta indispensable.\nEl muestreo permite estudiar a una parte de la población en lugar de examinarla por completo, logrando así simplificar el proceso de recolección de datos. La idea principal es seleccionar un grupo más pequeño, llamado muestra, que refleje las características de la población total. Y, aunque el muestreo tengas ciertas limitaciones que aclararemos en los próximos capítulos, nos ayuda a responder nuestras preguntas sobre la sociedad de manera más rápida, económica y eficiente.\nAl hablar de muestreo, es inevitable considerar la representatividad. La representatividad asegura que la muestra refleje las características clave de la población total, como edad, género, nivel socioeconómico y región geográfica. Sin representatividad, las conclusiones obtenidas pueden estar sesgadas y no ser aplicables a toda la población. Si no logramos que la muestra sea representativa, es probable que nuestras conclusiones estén sesgadas. Por ejemplo, si en una encuesta sobre transporte público solo incluimos a personas que viven cerca de estaciones principales, no podremos entender las dificultades que enfrentan quienes viven fuera de la periferia. En este caso, los resultados obtenidos no reflejarían la realidad del sistema de transporte para toda la población.\nPara asegurar la representatividad, nuestra selección para la muestra debe ser lo más imparcial posible, y aquí es donde la aleatoriedad juega un papel crucial. Al seleccionar individuos al azar, se minimiza el sesgo. La aleatoriedad ayuda a que cada individuo tenga la misma oportunidad de ser seleccionado, aumentando la probabilidad de que la muestra refleje las características de toda la población. Por ejemplo, si la población tiene un 60% de adultos jóvenes y un 40% de adultos mayores, la aleatoriedad tiende a garantizar que la muestra tenga una proporción similar de ambos grupos. De esta manera, logramos que la muestra sea representativa y que las conclusiones obtenidas puedan ser aplicadas al resto de la población con mayor confianza.\n\n1.4.1 Tipos de muestreo\nComo ya vimos, la representatividad de una muestra es crucial para extrapolar los resultados obtenidos en esta a una población más amplia. Una muestra representativa permite que los resultados sean generalizables a la población accesible y, a partir de esta, a la población objetivo. Una muestra será representativa si todos los sujetos de la población tienen la misma posibilidad de ser seleccionados y si el tamaño de la muestra refleja adecuadamente las características de la población en términos de la distribución de las variables estudiadas. Otzen y Manterola (2017) establecen una clara disntinción:\nTécnicas de muestreo probabilístico\nEn el muestreo probabilístico, cada individuo de la población tiene una probabilidad conocida y no nula de ser seleccionado. Esto garantiza que la muestra sea representativa y que las conclusiones puedan generalizarse a la población total.\n\nAleatorio simple: Cada miembro de la población tiene la misma probabilidad de ser seleccionado. Por ejemplo, en un estudio sobre la percepción de los servicios municipales en una ciudad, se obtiene una lista de los residentes inscritos en el padrón electoral. Luego, asigna un número a cada residente y selecciona al azar a 300 personas. Este enfoque asegura que cada persona de la ciudad tenga la misma probabilidad de ser seleccionada.\n\n\n\nElaboración propia\n\n\nEstratificado: Se divide la población en subgrupos homogéneos o estratos (como género, edad o nivel educativo) y se selecciona una muestra aleatoria dentro de cada uno. Esto asegura que todos los estratos estén representados. Por ejemplo, para estudiar la participación laboral en un país, se divide la población en estratos basados en niveles educativos: primaria, secundaria, técnica y universitaria. Luego selecciona proporcionalmente a los participantes de cada grupo. Por ejemplo, si el 40% de la población tiene nivel secundario, el 40% de los encuestados se selecciona de este estrato, garantizando que todos los niveles educativos estén representados en los resultados.\n\n\n\nElaboración propia\n\n\nSistemático: Se selecciona cada “k-ésimo” individuo de una lista organizada. Por ejemplo, en una encuesta sobre hábitos de consumo en mercados locales, se obtiene una lista de 1,000 comerciantes registrados. Se decide seleccionar a cada décimo comerciante de la lista (k=10), comenzando por un número inicial que se elige al azar. Este método es práctico, especialmente cuando se trabaja con grandes poblaciones.\n\n\n\nElaboración propia\n\n\nPor conglomerados: Se divide la población en grupos naturales, como barrios o comunidades, y se seleccionan algunos de estos conglomerados para estudiar a todos sus miembros. Por ejemplo, en un estudio sobre acceso a educación en comunidades rurales, se seleccionan tres provincias al azar dentro de una región. Luego, en cada provincia, se eligen dos distritos y se encuestan a todas las familias de estos distritos. Este método reduce los costos y el tiempo necesario para cubrir grandes áreas geográficas.\n\n\n\nElaboración propia\n\n\n\nTécnicas de muestreo no probabilístico\nEn el muestreo no probabilístico, la selección de participantes no sigue un principio de aleatoriedad, lo que limita su representatividad. Sin embargo, estas técnicas son útiles en estudios exploratorios o cualitativos.\n\nPor conveniencia: Se seleccionan participantes accesibles y disponibles. Por ejemplo, se realiza un estudio sobre los efectos del alza de precios de los alimentos se realiza entrevistando a las personas que asisten a un mercado local durante un fin de semana. Los participantes son seleccionados por su proximidad al encuestador, lo que permite recolectar datos rápidamente, aunque con un sesgo hacia personas que frecuentan ese mercado en particular o a esa hora en específico.\nIntencional o de juicio: Los investigadores seleccionan a participantes que consideran relevantes para el estudio. Por ejemplo, en un estudio sobre líderes comunitarios, se entrevistaría solo a líderes reconocidos en la comunidad.\n\n\n\n1.4.2 Comparación de métodos de muestreo\n\n\n\n\n\n\n\n\n\nMétodo\nProbabilístico\nVentajas\nLimitaciones\n\n\n\n\nAleatorio simple\nProbabilístico\nIgualdad en la probabilidad de inclusión, sencillo.\nPuede no representar subgrupos pequeños o diversos.\n\n\nEstratificado\nProbabilístico\nRepresentación garantizada de subgrupos importantes.\nRequiere información previa sobre los estratos.\n\n\nSistemático\nProbabilístico\nEficiente y fácil de aplicar.\nIntroduce sesgo si hay patrones en la lista.\n\n\nPor conglomerados\nProbabilístico\nPráctico y económico para poblaciones grandes y dispersas.\nMenor precisión si los conglomerados no son representativos.\n\n\nPor conveniencia\nNo probabilístico\nRápido y económico.\nAlta probabilidad de sesgo y falta de representatividad.\n\n\nIntencional\nNo probabilístico\nÚtil para temas específicos o grupos únicos.\nDepende del criterio del investigador, no es generalizable.\n\n\n\nElegir el método de muestreo adecuado depende de los objetivos del estudio, las características de la población y los recursos disponibles.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentos de la Estadística</span>"
    ]
  },
  {
    "objectID": "fundamentos.html#estadística-descriptiva-e-inferencial",
    "href": "fundamentos.html#estadística-descriptiva-e-inferencial",
    "title": "1  Fundamentos de la Estadística",
    "section": "1.5 Estadística descriptiva e inferencial",
    "text": "1.5 Estadística descriptiva e inferencial\nPerfecto, ya hemos definido qué son los datos, las variables y el muestreo. Hemos seleccionado nuestra muestra y recopilado la información necesaria, y ahora contamos con una base de datos lista para analizar. Pero, ¿por dónde empezamos? El siguiente paso es explorar las dos grandes ramas de la estadística: la estadística descriptiva y la estadística inferencial.\nA lo largo de los próximos capítulos, este libro se centrará en explicar y aplicar estas herramientas en investigaciones sociales. La estadística descriptiva y la inferencial son los pilares que sostendrán gran parte del análisis que realizaremos. Mientras que la descriptiva nos ayudará a organizar y entender los datos que recolectamos, la inferencial nos permitirá extraer conclusiones significativas que trasciendan nuestra muestra.\nEstadística descriptiva\nLa estadística descriptiva se encarga de resumir y describir las características principales de un conjunto de datos. Al trabajar con bases de datos, estas pueden contener cientos o incluso miles de observaciones, lo que puede resultar abrumador. La función de la estadística descriptiva es condensar esta información en un formato más comprensible y manejable, utilizando herramientas como tablas, gráficos y medidas resumen.\n\nTablas y gráficos: Una manera visual de entender la información. Por ejemplo, en un estudio sobre el impacto de una política educativa, podríamos usar gráficos de barras para mostrar el porcentaje de personas que apoyan o rechazan la política, o tablas para presentar la distribución de los encuestados por nivel educativo.\nMedidas de tendencia central: Ayudan a identificar valores representativos dentro de los datos, como la media (promedio), la mediana (valor central) y la moda (valor más frecuente). Por ejemplo, en un estudio sobre ingresos familiares, la media puede indicarnos el ingreso promedio, mientras que la mediana puede ser más representativa si hay grandes diferencias entre los ingresos más bajos y los más altos.\nMedidas de dispersión: Complementan el análisis descriptivo al mostrar la variabilidad de los datos, como el rango, la varianza o la desviación estándar. Por ejemplo, si analizamos el tiempo que las personas dedican al transporte público en diferentes distritos, la desviación estándar nos indica si la experiencia es similar para todos o varía significativamente.\n\nEstadística inferencial\nMientras que la estadística descriptiva se centra en los datos que tenemos, la estadística inferencial da un paso más allá al usar estos datos para hacer generalizaciones sobre una población más amplia. Dado que generalmente trabajamos con muestras y no con toda la población, necesitamos herramientas que nos permitan estimar qué tan representativos son nuestros resultados y qué tan confiables pueden ser las conclusiones que saquemos.\n\nPruebas de hipótesis: Nos ayudan a responder preguntas específicas basadas en los datos. Por ejemplo, en un estudio sobre el impacto del nivel educativo en el apoyo a una política ambiental, podríamos plantear la hipótesis de que “las personas con educación universitaria apoyan más esta política que quienes tienen solo educación básica”. Una prueba de hipótesis evaluaría si los datos respaldan esta afirmación.\nIntervalos de confianza: Indican el rango dentro del cual es probable que se encuentre el valor real de la población. Por ejemplo, si calculamos que el 60% de nuestra muestra apoya una determinada política, un intervalo de confianza nos muestra, con cierto grado de seguridad, un rango donde este porcentaje podría estar en la población general, como entre el 57% y el 63%.\nModelos de regresión: Permiten analizar relaciones entre variables. Por ejemplo, podríamos investigar si existe una relación significativa entre el ingreso familiar y el uso de transporte público subsidiado, controlando otras variables como la distancia al trabajo o el género.\n\nHay que entender que la estadística descriptiva e inferencial son herramientas complementarias. La descriptiva se encarga de organizar y resumir los datos para hacerlos comprensibles, destacando patrones, tendencias y características principales de un conjunto de datos. La estadística inferencial utiliza esos resúmenes para hacer generalizaciones y responder preguntas sobre una población más amplia. A través de la inferencia, podemos ir más allá de los datos observados para hacer estimaciones, probar hipótesis y explorar relaciones entre variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentos de la Estadística</span>"
    ]
  },
  {
    "objectID": "fundamentos.html#flor-y-el-transporte-público-en-lima-metropolitana",
    "href": "fundamentos.html#flor-y-el-transporte-público-en-lima-metropolitana",
    "title": "1  Fundamentos de la Estadística",
    "section": "1.6 Flor y el transporte público en Lima Metropolitana",
    "text": "1.6 Flor y el transporte público en Lima Metropolitana\nFlor se dispuso a comprender cómo perciben los limeños el transporte público. Para ello, comenzó por planificar cuidadosamente el DISEÑO de su investigación, definiendo los elementos esenciales que estructurarían su estudio.\nConsciente de que entrevistar a toda la población de Lima Metropolitana sería inviable, Flor decidió trabajar con una MUESTRA representativa. Para ello, tomó en cuenta todos los distritos de la ciudad, tratándolos como grupos independientes para asegurar una buena cobertura territorial. Luego, dentro de cada distrito, seleccionó al azar algunas calles donde realizaría las entrevistas a sus domicilios. Este método, que combina un muestreo estratificado con un muestreo por conglomerados, lo que permitió incluir tanto zonas céntricas como periféricas, mientras que mantiene el carácter probabilístico.\nEn el diseño, definió su UNIDAD DE OBSERVACIÓN, es decir, las personas usuarias del transporte público en Lima. Estas serían quienes proporcionarían las respuestas a su cuestionario. Para cada una, estableció las VARIABLES que mediría:\n\nVariables continuas, como el tiempo diario dedicado al transporte (en horas).\nVariables categóricas, como el medio principal de transporte utilizado (bus, taxi colectivo, mototaxi).\nVariables ordinales, como el nivel de satisfacción con el servicio, categorizado en Muy insatisfecho, Insatisfecho, Neutral, Satisfecho y Muy satisfecho.\n\nCada conjunto de respuestas proporcionadas por una UNIDAD DE OBSERVACIÓN se convirtió en una OBSERVACIÓN, y todas estas observaciones conformaron la base de datos que Flor construiría para su análisis.\nCon los datos recolectados, organiza su base en una tabla, donde cada fila representa una unidad de observación y cada columna corresponde a una variable medida. Para analizar esta información, utiliza la ESTADÍSTICA DESCRIPTIVA, que le permite resumir y organizar los datos para facilitar su comprensión.\nPrimero, recurre a tablas y gráficos para presentar visualmente la información y observar tendencias generales. Estas herramientas le ayudan a identificar cómo se distribuyen las variables medidas y a detectar patrones iniciales en los datos. La visualización también le permite comparar diferentes categorías y grupos de manera clara.\nLuego, utiliza medidas de resumen para condensar la información. Estas medidas le permiten determinar valores representativos que describen las características generales de las variables. Esto le permite evaluar si ciertas características cambian de manera consistente o si existen variaciones importantes que podrían ser relevantes.\nFlor no solo busca resumir los datos, también quiere entender cómo las variables que ha recopilado interactúan entre sí. Comienza a analizar posibles conexiones entre ellas, lo que le permite identificar patrones que podrían explicar las experiencias de los usuarios del transporte público. Por ejemplo, examina cómo el tiempo de viaje diario (variable continua) podría relacionarse con el nivel de satisfacción (variable ordinal) o cómo el medio de transporte utilizado (variable categórica) podría influir en la percepción de seguridad (variable ordinal).\nCon estas observaciones iniciales, Flor recurre a herramientas de ESTADÍSTICA INFERENCIAL para generalizar los hallazgos obtenidos de su muestra a toda la población de usuarios del transporte público en Lima. Su objetivo es utilizar estas herramientas para convertir las tendencias observadas en su muestra en conclusiones más amplias y fundamentadas.\n\nIntervalos de confianza: Flor estima rangos dentro de los cuales es probable que se encuentren los valores reales de la población. Por ejemplo, analiza qué proporción de usuarios en la población general está insatisfecha con el servicio, basándose en los datos de su muestra y aplicando un intervalo de confianza para definir este rango con un nivel de certeza.\nPruebas de hipótesis: Flor evalúa si las diferencias observadas en el nivel de satisfacción entre los usuarios de diferentes medios de transporte (como buses y taxis colectivos) son significativas. Esto le permite confirmar si las diferencias son reales o si podrían deberse al azar.\nRelaciones entre variables: Explora la relación entre el nivel de ingresos (variable categórica) y el tiempo de viaje diario (variable continua), identificando patrones que podrían reflejar desigualdades estructurales.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentos de la Estadística</span>"
    ]
  },
  {
    "objectID": "fundamentos.html#resumen-del-capítulo",
    "href": "fundamentos.html#resumen-del-capítulo",
    "title": "1  Fundamentos de la Estadística",
    "section": "1.7 Resumen del capítulo",
    "text": "1.7 Resumen del capítulo\nLa estadística es una herramienta fundamental para la recopilación, organización y análisis de datos en una amplia gama de campos, y en nuestro caso, en las ciencias sociales. Permite la conversión de información dispersa en conocimiento para comprender fenómenos sociales y tomar decisiones fundamentadas.\nEl diseño de la investigación es la etapa donde el investigador debe decidir cómo recolectar datos eligiendo una muestra representativa de la cual los resultados puedan ser replicados a una población más amplia. Dependiendo de los objetivos del estudio, se utilizan métodos de muestreo probabilísticos (por ejemplo, muestreo aleatorio simple, estratificado, sistemático y por conglomerados) o métodos no probabilísticos (por ejemplo, muestreo por conveniencia y por juicio).\nLuego tenemos datos y variables. Los datos se agrupan en unidades de observación y cada registro de observación tiene valores de variables. Estas variables pueden ser numéricas o categóricas, y su clasificación afecta la manera en que se analizan.\nLa Estadística Descriptiva ayuda a describir las características básicas de los datos, resaltando el patrón y las tendencias en los datos mediante las tablas, gráficos y medidas que resumen los datos.\nLa Estadística Inferencial generaliza los hallazgos de la muestra a la población utilizando principios de probabilidad y haciendo uso de intervalos de confianza, pruebas de hipótesis y modelos de relación entre las variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentos de la Estadística</span>"
    ]
  },
  {
    "objectID": "fundamentos.html#ejercicios",
    "href": "fundamentos.html#ejercicios",
    "title": "1  Fundamentos de la Estadística",
    "section": "1.8 Ejercicios",
    "text": "1.8 Ejercicios\n1.En un estudio sobre hábitos de transporte, los investigadores registran:\n\nTipo de transporte utilizado (bus, taxi, bicicleta),\nTiempo de viaje diario (en minutos),\nNivel de satisfacción (Muy satisfecho, Satisfecho, Neutral, Insatisfecho, Muy insatisfecho).\n\n¿Cuál de las siguientes clasificaciones de variables es correcta?\n\na) Tiempo de viaje: Continua; Tipo de transporte: Nominal; Nivel de satisfacción: Ordinal\nb) Tiempo de viaje: Discreta; Tipo de transporte: Ordinal; Nivel de satisfacción: Nominal\nc) Tiempo de viaje: Continua; Tipo de transporte: Ordinal; Nivel de satisfacción: Nominal\nd) Tiempo de viaje: Discreta; Tipo de transporte: Nominal; Nivel de satisfacción: Ordinal\n2.Un investigador desea estudiar la percepción de seguridad en parques de una ciudad. Para ello, utiliza un listado de todos los visitantes registrados y selecciona al azar 200 personas. ¿Qué método de muestreo está utilizando?\n\na) Muestreo por conglomerados\nb) Muestreo aleatorio simple\nc) Muestreo estratificado\nd) Muestreo por conveniencia\n3.En un censo escolar, se registran datos como “Edad”, “Grado cursado” y “Preferencia por asignaturas”. ¿Cuál es la unidad de observación?\n\na) La escuela\nb) El grado escolar\nc) Las asignaturas\nd) El estudiante\n4.Para un estudio sobre el uso de transporte público, se divide a la población en grupos según su nivel socioeconómico (alto, medio, bajo) y se selecciona aleatoriamente una muestra proporcional de cada grupo. ¿Qué tipo de muestreo se está aplicando?\n\na) Muestreo aleatorio simple\nb) Muestreo estratificado\nc) Muestreo sistemático\nd) Muestreo por conglomerados\n5.Un investigador organiza una tabla para mostrar el tiempo promedio de espera en paradas de buses según diferentes distritos. ¿Qué tipo de análisis está realizando?\n\na) Estadística descriptiva\nb) Estadística inferencial\nc) Pruebas de hipótesis\nd) Modelos de regresión\n6.Un estudio encuentra que los tiempos de viaje (en horas) más largos se observan en usuarios de transporte público de distritos periféricos. ¿Qué tipos de variables se está utilizando para esta asociación? (2 respuestas)\n\na) Variable continua\nb) Variable categórica nominal\nc) Variable ordinal\nd) Variable binaria\n7.Un investigador selecciona una muestra de cada décima persona en una lista de usuarios de transporte público. ¿Qué tipo de muestreo está utilizando?\n\na) Muestreo aleatorio simple\nb) Muestreo sistemático\nc) Muestreo estratificado\nd) Muestreo por conveniencia\n8.Un estudio clasifica a los usuarios según su nivel de satisfacción con el transporte (Muy satisfecho, Satisfecho, Neutral, Insatisfecho, Muy insatisfecho). ¿Qué tipo de variable es el nivel de satisfacción?\n\na) Continua\nb) Discreta\nc) Ordinal\nd) Nominal\n\n\n\n\nAgresti, Alan. 2018. Statistical methods for the social sciences. Fifth edition. Boston: Pearson.\n\n\nAgresti, Alan, Christine A. Franklin, y Bernhard Klingenberg. 2023. Statistics: the art and science of learning from data. Fifth edition, global edition. Harlow, United Kingdom: Pearson.\n\n\nOtzen, Tamara, y Carlos Manterola. 2017. «Técnicas de Muestreo sobre una Población a Estudio». International Journal of Morphology 35 (1): 227-32. https://doi.org/10.4067/s0717-95022017000100037.\n\n\nRagin, Charles C. 2007. Construcción de la investigación social: Introducción a los métodos y su diversidad. Bogotá, Colombia: Siglo del Hombre Editores.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentos de la Estadística</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html",
    "href": "fundamentosR.html",
    "title": "2  Fundamentos de R",
    "section": "",
    "text": "2.1 R como lenguaje de programación\nPara dar un poco de contexto, cuando decimos que R es un “lenguaje de programación”, nos referimos a que, en este sistema, los usuarios escribimos código, y es la máquina la que se encarga de ejecutar las instrucciones, llevando a cabo las tareas que queramos realizar. A diferencia de otros programas que utilizan botones o menús predefinidos, en R el código es el protagonista: aquí especificamos, paso a paso, las operaciones que queremos ejecutar.\nLlamamos “lenguaje” a estos sistemas porque tienen mucho en común con los idiomas humanos. Así como en español necesitamos un vocabulario para expresar ideas, en R contamos con funciones, objetos y operadores que nos permiten comunicarnos con la computadora. De manera similar, la gramática en un idioma dicta cómo organizar las palabras para construir frases coherentes, y en R la sintaxis define cómo estructurar el código para que sea comprensible y funcional.\nPiensa por un momento en cómo aprendes un nuevo idioma: primero necesitas familiarizarte con su vocabulario, entender las reglas gramaticales que rigen cómo se estructuran las oraciones y luego practicar combinándolos para expresar ideas más complejas. Aprender R no es muy diferente. El código es su vocabulario, compuesto por funciones, objetos y operadores que utilizamos para comunicar instrucciones a la computadora.\nAl igual que hay múltiples idiomas, también existen muchos lenguajes de programación, cada uno adaptado a propósitos específicos. Python, por ejemplo, es conocido por su uso en inteligencia artificial y desarrollo web, mientras que SQL se utiliza para gestionar bases de datos. Si bien esta guia no tocará otros lenguajes espero también te sirva sirve como una excelente introducción al mundo de la programación.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#r-como-lenguaje-de-programación",
    "href": "fundamentosR.html#r-como-lenguaje-de-programación",
    "title": "2  Fundamentos de R",
    "section": "",
    "text": "Ejemplo de código en r. Elaboración propia",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#instalación-de-r-y-rstudio",
    "href": "fundamentosR.html#instalación-de-r-y-rstudio",
    "title": "2  Fundamentos de R",
    "section": "2.2 Instalación de R y RStudio",
    "text": "2.2 Instalación de R y RStudio\nPara utilizar R desde nuestras computadoras, debemos instalar R y RStudio, ya que ambos se utilizan conjuntamente. Sin embargo, no son lo mismo:\n\nR: Es el lenguaje de programación y entorno de software en el que se realizan análisis estadísticos y se crean paquetes. R forma la base que necesitaremos para realizar cálculos, análisis y visualización de datos.\nRStudio: Un entorno de desarrollo integrado (IDE) para R, usado para escribir scripts y ejecutar análisis, así como para producir informes. RStudio proporciona una interfaz más fácil y amigable de navegar además de añadir otras herramientas útiles al momento de trabajar.\n\n\nUn IDE (Entorno de Desarrollo Integrado) es un programa que ayuda de manera inteligente a escribir y ejecutar el código de manera organizada. Es una herramienta todo en uno que cuenta con un área para escribir el código, verificar errores, ejecutarlo y ver los resultados. También tiene características útiles, como resaltado de comandos e intentat predecir tu próximo código y darte sugerencias de opciones mientras se escribe.\n\nPasos para instalar R:\n\nVisita la página oficial de R.\nDirígete a la sección de descargas y selecciona el enlace apropiado para tu sistema operativo (Windows, macOS, Linux).\nSigue las instrucciones de instalación.\n\nPasos para instalar RStudio:\n\nVisita la página oficial de RStudio.\nNavega a la sección de descargas y selecciona “RStudio Desktop”.\nElige el instalador adecuado para tu sistema operativo y sigue las instrucciones de instalación.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#interfaz",
    "href": "fundamentosR.html#interfaz",
    "title": "2  Fundamentos de R",
    "section": "2.3 Interfaz",
    "text": "2.3 Interfaz\nPara empezar, vamos a utilizar RStudio. Lo primero que debemos hacer es familiarizarnos con la interfaz. Al abrirlo, probablemente lo primero que veas es lo siguiente.\n\n\n\nExtraído de RStudio User Guide\n\n\nHay cuatro paneles, cada uno indispensable y con un conjunto de funcionalidades:\n\nSource: Todo el código que escribes en R se realiza en el panel Source. El código R es simplemente tu conjunto de instrucciones en el lenguaje R para que la computadora complete las tareas dadas.\nConsola: La consola es la consola interactiva R. Aquí es donde la mayoría de las funciones se ejecutan al instante. Aquí es donde aparecen los resultados de los cálculos y procedimientos que hemos solicitado al programa. Se utiliza principalmente para una verificación rápida y también para ver el resultado de tus scripts.\nEntorno: La pestaña Entorno es la lista de todos los objetos que has creado en tu trabajo. Algunos ejemplos de objetos son vectores, matrices, marcos de datos, listas, gráficos y funciones.\nArchivos/Gráficos/Paquetes/Ayuda:\n\nArchivos: Son para navegar por el sistema de archivos de tu computadora directamente desde RStudio.\nGráficos: Después de ejecutar el código R, puedes ver las imágenes creadas por el código R.\nPaquetes: La herramienta gestiona los paquetes que has instalado.\nAyuda: El equivalente de Google para R. Puedes buscar cualquier información sobre las funciones e incluso encontrar el tutorial necesario.\n\n\nLa ventaja original del panel Source es la capacidad de editar rápidamente tu código, el cual puedes volver a ejecutar. Este aspecto incluye tanto la reproductividad como la eficiencia; por ejemplo, podemos ajustar nuestro código sin rehacerlo desde cero. Además, podemos guardar nuestro trabajo como un archivo, podemos almacenar nuestros scripts en un editor de computadora, podemos distribuirlo, podemos editarlo o publicar este archivo nuevamente. Pero para trabajar de manera organizada, también es importante tener una buena estructura para almacenar todo lo relacionado con el proyecto.\nCuando trabajamos en proyectos de análisis de datos, es común manejar varios archivos relacionados: tus scripts (donde escribirás el código), los datos que analizarás y los resultados generados. Tener una carpeta específica para cada proyecto te ayuda a mantener todo organizado en un solo lugar. Además, R necesita saber dónde buscar y guardar los archivos, y eso se define mediante el directorio de trabajo, que es simplemente la carpeta donde R guardará y buscará archivos automáticamente.\n\n2.3.1 Crear y configurar tu carpeta de trabajo\n\nCrea una carpeta específica:\n\nEn tu dispositivo, crea una nueva carpeta para tu proyecto (por ejemplo, “MiProyectoR”). Este será el lugar donde guardarás todos los archivos relacionados.\n\nEstablece la carpeta como directorio de trabajo:\n\nEn RStudio, ve al panel Files, ubicado generalmente en la parte inferior derecha.\n\nPuedes usar el botón New Folder para crear una nueva carpeta si aún no lo has hecho.\nNavega hasta la carpeta que creaste y selecciona More &gt; Set As Working Directory. Esto le dice a RStudio que todos los archivos guardados o abiertos estarán relacionados con esta carpeta.\n\nVerás en la consola un mensaje como:\n\n\n\n# setwd(\"ruta/de/tu/carpeta\")\n\n\n\n2.3.2 Crear un archivo de script\nCuando trabajamos en R, utilizamos diferentes tipos de archivos para organizar y guardar nuestro trabajo. Dos formatos comunes son el archivo R Script y el documento Quarto. Ambos se utilizan para escribir código, pero cumplen propósitos diferentes que es importante entender.\nUn archivo R Script es un archivo simple donde escribimos y guardamos nuestras instrucciones de código. Sirve como un registro de los comandos que ejecutamos, permitiendo reutilizarlos o ajustarlos más adelante. Sin embargo, este tipo de archivo no incluye espacio para explicaciones extensas ni muestra los resultados directamente junto al código.\nEn cambio, un documento Quarto va más allá al permitir combinar texto explicativo, bloques de código y los resultados generados (como tablas y gráficos) en un solo archivo. Además, es posible exportar el archivo final en formatos como HTML, PDF o Word, haciéndolo ideal para documentar y compartir análisis de manera profesional (de hecho, este libro esta hecho en Quarto). Esta capacidad de integrar explicación, análisis y presentación en un mismo lugar hace que Quarto sea particularmente útil para aprender y comunicar análisis de datos. En este libro, vamos a utilizar documentos Quarto.\n\nAccede a la ventana Files:\n\nEn RStudio, localiza el panel Files. Este panel muestra el contenido de la carpeta de trabajo que configuraste previamente.\n\nCrea un nuevo documento Quarto:\n\nEn el panel Files, haz clic en el botón New File y selecciona Quarto Document.\n\nAparecerá un cuadro de diálogo donde puedes configurar el título del documento, tu nombre y el formato de salida inicial (HTML es una buena opción para empezar). Haz clic en Create.\n\nGuarda automáticamente en la carpeta de trabajo:\n\nCuando crees el documento desde la ventana Files, este se guardará automáticamente en tu carpeta de trabajo configurada. No necesitas realizar pasos adicionales para seleccionar la ubicación.\n\n\nEn un archivo Quarto puedes combinar texto explicativo (en formato Markdown) con bloques (chunks) de código en R. Los bloques de código son las secciones donde escribirás las instrucciones que deseas ejecutar, y están delimitados por tres backticks (```) seguidos del lenguaje que estás utilizando (en este caso, r). Para agregar un bloque de código de manera rápida, puedes utilizar el atajo de teclado:\n\nCtrl + Alt + I en Windows y Linux.\n\nCmd + Option + I en Mac.\n\nEste atajo insertará automáticamente un nuevo chunk en tu documento, con la estructura básica para que puedas empezar a escribir tu código. Un chunk de código tiene este formato:\n```{r}\n# Aquí escribes tu código en R\n```\nDentro del chunk, puedes incluir cualquier instrucción que se ejecutará cuando proceses el documento. Esto permite mantener el texto explicativo y el código separados pero integrados en el mismo archivo, lo que facilita la organización y presentación del análisis.\nEste enfoque es primordial para que trabajes directamente desde tu carpeta de trabajo, asegurando que todos los archivos relacionados con el proyecto estén organizados y listos para reproducir o compartir.\n\n\n2.3.3 Flujo de trabajo\nCuando trabajamos con un documento Quarto, seguimos un flujo estructurado que combina texto explicativo, bloques de código y los resultados generados en un único archivo. Este flujo de trabajo organiza y centraliza nuestro análisis, garantizando que sea reproducible y fácil de compartir. Para ello, debemos entener el rol de cada panel.\nLa consola en RStudio es el núcleo donde se procesan las instrucciones que escribimos. Cada vez que ejecutas un bloque de código, este se envía a la consola, donde R lo interpreta y devuelve un resultado. Piensa en la consola como el “cerebro” que recibe tus órdenes, las ejecuta y genera una respuesta, ya sea un cálculo, un gráfico o una tabla. La consola de RStudio es la sección donde puedes escribir código directamente y ejecutarlo inmediatamente al presionar Enter.\nAunque puedes escribir directamente en la consola, las instrucciones que ingresas allí no se guardan automáticamente, por lo que no es ideal para documentar o reproducir tu trabajo.\n\nLa fuente, que en nuestor caso es un documento Quarto, actúa como un registro estructurado de tu análisis.\n\nDentro de él, puedes combinar texto explicativo con bloques de código llamados chunks. Para ejecutar un chunk completo:\n\nHaz clic en el botón Play que aparece en la esquina superior derecha del chunk.\nAlternativamente, coloca el cursor dentro del chunk y presiona:\n\nCtrl + Shift + Enter en Windows y Linux.\nCmd + Shift + Enter en Mac.\n\n\nAl ejecutar el chunk, todas las líneas dentro de él se enviarán a la consola para ser procesadas.\n\nEstos chunks permiten escribir instrucciones que la consola ejecutará, pero al estar integrados en el documento, también puedes guardarlos y reproducirlos en el futuro. Esto hace que el documento Quarto sea una herramienta ideal para registrar tus pasos, explicar tus decisiones y presentar los resultados de forma clara. Los gasos generalmente son los siguentes:\n\nEsta lógica nos permite:\n\nEscribir el código en un chunk: Cada chunk de código se delimita por tres backticks (```) y se utiliza para dar instrucciones específicas a R. Por ejemplo:\n\n5 + 5\n\n[1] 10\n\n\nEste código le dice a R que sume 5 y 5. Cuando lo ejecutas, R procesa la operación en la consola y devuelve el resultado.\nAnotar el código con comentarios: Para aclarar el propósito de una línea de código o hacer notas útiles, utilizamos el carácter #. Todo lo que escribas después de este símbolo será un comentario y no se ejecutará. Por ejemplo:\n\n5 + 5 # Esto es una suma\n\n[1] 10\n\n\nAquí, el comentario sirve para explicar lo que hace esta línea.\nTexto explicativo: Una de las ventajas de Quarto es que permite intercalar texto explicativo con los chunks de código. Esto facilita documentar el análisis y explicar los resultados, haciendo que el trabajo sea más comprensible y presentable.\n\n\n\nConsola vs. documento Quarto: ¿cuándo usar cada uno?\nEscribir directamente en la consola: Es útil para pruebas rápidas o cálculos sencillos que no necesitas guardar. Devuelve el resultado inmediatamente, pero no queda registrado.\nEscribir en el documento Quarto: Es ideal para cualquier análisis que quieras documentar, reproducir o compartir. Al escribir en un chunk, puedes ejecutar el código igual que en la consola, pero además quedará registrado junto con tus explicaciones y los resultados obtenidos.\n\nCuando completes tu análisis, haz clic en el botón Render en la parte superior del archivo Quarto.\n\nPresiona este botón para ejecutar todo el código en la consola y también para producir el documento final con el formato seleccionado HTML (con más extensiones incluso puede exportar a PDF o Word). Garantiza que tu trabajo esté organizado, documentado y listo para ser publicado.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#objetos",
    "href": "fundamentosR.html#objetos",
    "title": "2  Fundamentos de R",
    "section": "2.4 Objetos",
    "text": "2.4 Objetos\nEn R, cualquier entidad que se cree y preserve en la memoria durante una sesión se llama objeto. Puede ser un número, un texto, un conjunto de datos, una función, un gráfico y así sucesivamente. Puedes almacenar datos en objetos y luego usarlos para cálculos o análisis. Para poder ah\\\\hacer ello, utilizamos el operador = o &lt;- para asignar un valor a un objeto. Por ejemplo, podemos asignar el resultado de una operación a un objeto:\n\n# Asignar el valor 10 al objeto 'a'\na = 10\n\n# Asignar el resultado de una suma al objeto 'b'\nb &lt;- 4 + 7\n\nUna vez que un objeto ha sido asignado, podemos utilizarlo en operaciones posteriores:\n\n# Sumar las objetos 'a' y 'b'\na + b\n\n[1] 21\n\n\n\n2.4.1 Nombrar un objeto\nPodemos declarar (nombrar) a un objeto como deseemos, pero su nombre no debe tener espacios. Por eso, a veces usamos guiones bajos (_) para conectar dos palabras, por ejemplo, mi_objeto o resultado_final.\nLas objetos se almacenan en el entorno (environment) de R y lo puedes apreciar en la ventana superior derecha del entorno de RStudio.\n\n\n\nElaboración propia",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#funciones",
    "href": "fundamentosR.html#funciones",
    "title": "2  Fundamentos de R",
    "section": "2.5 Funciones",
    "text": "2.5 Funciones\nAnteriormente mencionamos que R es un lenguaje de programación que nos permite comunicarnos con la computadora para que esta realice tareas utilizando nuestros datos (input) y nos devuelva un resultado (output). Para que estas tareas se lleven a cabo, primero debemos indicarle a la computadora cómo hacerlo, y estas indicaciones se dan a través de funciones. Las funciones son instrucciones representadas por palabras seguidas de paréntesis, que le dicen a R que ejecute una acción específica.\n\n\n\nElaboración propia\n\n\nLas funciones en R pueden recibir argumentos (los valores que se colocan dentro del paréntesis). Estos argumentos pueden ser tanto los datos a procesar como instrucciones adicionales que le indican a la función cómo debe realizar la tarea.\nPor ejemplo, la función sum() sirve para sumar múltiples valores.\n\n# Sumar 10, 20 y 30 usando la función sum()\nsum(10, 20, 30)\n\n[1] 60\n\n\nLa función round() redondea un número, y podemos especificar cuántos decimales queremos conservar. Aquí, el primer argumento es el número a redondear, y el segundo es una instrucción que indica cuántos decimales mantener.\n\nround(2.54934, 2)\n\n[1] 2.55\n\n\nLas funciones son fundamentales para trabajar en R, son la columna vertebral del lenguaje y nos permiten realizar tareas de todo tipo, desde cálculos simples hasta análisis complejos de datos. Muchas funciones ya vienen incluidas en R, pero otras han sido creadas por la comunidad y se encuentran en paquetes (que exploraremos más adelante). Además, tú mismo puedes crear tus propias funciones (Consulta el glosario de operaciones útiles, 12.0).\nPor eso, aquí tienes una tabla con una clasificación que te ayudará a explorarlas:\n\n\n\n\n\n\n\n\nCategoría\nFunción\nDescripción\n\n\n\n\nMatemáticas y Estadísticas\n\n\n\n\nRealizan cálculos matemáticos o análisis estadísticos sobre los datos.\nsum()\nSuma todos los valores proporcionados.\n\n\n\nmean()\nCalcula el promedio de un conjunto de datos.\n\n\n\nsd()\nCalcula la desviación estándar de un conjunto de datos.\n\n\n\nround()\nRedondea un número al número de decimales especificado.\n\n\n\nlog()\nCalcula el logaritmo de un número en la base especificada (por defecto, base e).\n\n\nManipulación de Datos\n\n\n\n\nEstas funciones permiten estructurar, transformar y filtrar conjuntos de datos para su análisis.\nlength()\nDevuelve la longitud de un vector o lista.\n\n\n\nhead()\nMuestra las primeras filas de un conjunto de datos.\n\n\n\ntail()\nMuestra las últimas filas de un conjunto de datos.\n\n\n\nsubset()\nExtrae subconjuntos de datos según una condición especificada.\n\n\n\nmerge()\nCombina dos conjuntos de datos en función de una clave común.\n\n\nVisualización\n\n\n\n\nAyudan a crear gráficos y representaciones visuales de los datos para facilitar su interpretación.\nplot()\nCrea gráficos básicos a partir de datos.\n\n\n\nhist()\nGenera un histograma para datos continuos.\n\n\n\nboxplot()\nCrea un diagrama de caja para representar la dispersión de los datos.\n\n\n\nbarplot()\nGenera un gráfico de barras.\n\n\nControl de Flujo\n\n\n\n\nPermiten estructurar el código para que se ejecute en función de condiciones específicas.\nif()\nEjecuta una acción si se cumple una condición.\n\n\n\nelse()\nProporciona una alternativa cuando la condición de if no se cumple.\n\n\n\nfor()\nItera a través de una secuencia y ejecuta una acción para cada elemento.\n\n\n\nwhile()\nRepite una acción mientras se cumpla una condición.\n\n\nEntrada/Salida\n\n\n\n\nFacilitan leer datos de archivos y guardar los resultados en diversos formatos.\nread.csv()\nLee datos desde un archivo CSV y los convierte en un marco de datos.\n\n\n\nwrite.csv()\nGuarda un marco de datos en un archivo CSV.\n\n\n\nreadRDS()\nCarga datos guardados en formato RDS.\n\n\n\nsaveRDS()\nGuarda datos en formato RDS.\n\n\nModelado Estadístico\n\n\n\n\nPermiten ajustar modelos y realizar análisis estadísticos avanzados.\nlm()\nAjusta modelos lineales simples o múltiples.\n\n\n\nglm()\nAjusta modelos lineales generalizados.\n\n\n\nsummary()\nProporciona un resumen estadístico de un modelo o conjunto de datos.\n\n\nTransformación de Texto\n\n\n\n\nAyudan a operar sobre cadenas de texto, como combinarlas, dividirlas o cambiar su formato.\npaste()\nCombina varias cadenas de texto en una sola.\n\n\n\nstrsplit()\nDivide una cadena en partes según un separador.\n\n\n\ntoupper()\nConvierte texto a mayúsculas.\n\n\n\ntolower()\nConvierte texto a minúsculas.\n\n\nTrabajo con Fechas\n\n\n\n\nFacilitan trabajar con datos que incluyen fechas y horas.\nSys.Date()\nDevuelve la fecha actual del sistema.\n\n\n\nas.Date()\nConvierte datos en formato de fecha.\n\n\n\ndifftime()\nCalcula la diferencia entre dos fechas.\n\n\n\nNota: No te precupes, no debes conocerlas todas aún. A medida que avancemos en el libro, exploraremos muchas de estas funciones en detalle. Sin embargo, te animo a echar un vistazo a su diversidad para que te familiarices con las capacidades que R ofrece.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#paquetes",
    "href": "fundamentosR.html#paquetes",
    "title": "2  Fundamentos de R",
    "section": "2.6 Paquetes",
    "text": "2.6 Paquetes\nEn R, muchas de las cosas que queremos hacer se pueden realizar con “paquetes”. En R, un paquete es generalmente un conjunto de herramientas con funciones específicas para realizar tareas específicas. Existen paquetes para prácticamente todo. Por ejemplo, el paquete dplyr (6.5) es un paquete ampliamente utilizado para la manipulación de datos como filtrar valores o crear nuevas columnas. Pero antes, necesitamos entender la diferencia entre instalar un paquete y cargar un paquete en R.\n\n2.6.1 Instalar un paquete\nMe gusta pensar que instalar un paquete en R es similar a ir a la tienda y comprar una caja de herramientas. Después de comprarla, la mueves a tu propio almacén para asegurarte de que la tendrás cuando la necesites. Esto solo necesita hacerse una vez por paquete, a menos que quieras actualizarlo (tener la última versión).\nCon R, instalamos un paquete con una función llamada install.packages() seguida del nombre del paquete entre comillas.\n\ninstall.packages('dplyr')\n\n\n\n2.6.2 Cargar un paquete\nCuando cargas un paquete en R, es como si sacaras la caja de herramientas del almacén y la pusieras en tu mesa de trabajo. Solo entonces las herramientas dentro de la caja te son accesibles para tus proyectos. Haces esto cada vez que comienzas un nuevo documento o una sesión de R.\nPara hacerlo, usamos la función library().\nTambién puedes lograr esto con :: si quieres usar una función completa de otro paquete, pero no quieres cargar el paquete completo. Esto es como sacar una herramienta específica de la caja, pero no poner toda la caja en tu mesa de trabajo.\nPor ejemplo, el paquete psych tiene una función describe que produce un resumen estadístico detallado de un conjunto de datos dado. Si queremos aplicarlo en un conjunto de datos predeterminado en R como iris, podemos hacerlo sin cargar todo psych:\n\n# Usar la función describe del paquete psych con el conjunto de datos iris\npsych::describe(iris)\n\n             vars   n mean   sd median trimmed  mad min max range  skew\nSepal.Length    1 150 5.84 0.83   5.80    5.81 1.04 4.3 7.9   3.6  0.31\nSepal.Width     2 150 3.06 0.44   3.00    3.04 0.44 2.0 4.4   2.4  0.31\nPetal.Length    3 150 3.76 1.77   4.35    3.76 1.85 1.0 6.9   5.9 -0.27\nPetal.Width     4 150 1.20 0.76   1.30    1.18 1.04 0.1 2.5   2.4 -0.10\nSpecies*        5 150 2.00 0.82   2.00    2.00 1.48 1.0 3.0   2.0  0.00\n             kurtosis   se\nSepal.Length    -0.61 0.07\nSepal.Width      0.14 0.04\nPetal.Length    -1.42 0.14\nPetal.Width     -1.36 0.06\nSpecies*        -1.52 0.07\n\n\nProporciona un resumen estadístico de los datos: media, desviación estándar, mínimo, máximo, etc., todo sin tener que cargar todo el paquete psych. Si nunca has escuchado sobre estas cosas antes, no te asustes, vamos a usar psych en la sección de Estadística Descriptiva (7.0) para calcular e interpretar estadísticas descriptivas para nuestros datos.\nRecuerda:\n\nCuando instalas un paquete, estás tomando la caja de herramientas y asegurándola en tu propio almacén.\nCargar un paquete, entonces, es poner esa caja de herramientas en tu mesa de trabajo para que puedas usar sus herramientas.\nImportar una función del paquete es como sacar una herramienta de la caja.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#elementos",
    "href": "fundamentosR.html#elementos",
    "title": "2  Fundamentos de R",
    "section": "2.7 Elementos",
    "text": "2.7 Elementos\nEn R, los elementos son las unidades básicas que componen los datos. Estos elementos tienen un tipo de dato que define su naturaleza, cómo pueden manipularse y las operaciones que pueden realizarse con ellos. Los tipos de elementos en R están directamente relacionados con los tipos de variables utilizados en análisis estadístico que vimos en el capítulo anterior, como variables numéricas (continuas y discretas), categóricas (nominales y ordinales) y lógicas (binarias).\n\n2.7.1 Tipos de elementos\na. Logical (Lógicos)\n\nRepresentan valores binarios: TRUE o FALSE.\nAsociados a variables dicotómicas en estadística.\n\n\nx &lt;- TRUE   # Elemento de tipo logical\ny &lt;- FALSE\n\nÚtil para variables lógicas o binarias, como “aprobado” (TRUE) y “reprobado” (FALSE).\nb. Integer (Enteros)\n\nRepresentan números enteros, como conteos o cantidades.\nSe definen agregando una “L” después del número.\n\n\nx &lt;- 42L  # Elemento de tipo integer\ny &lt;- -3L\n\nÚtil para variables numéricas discretas, como número de hijos.\nc. Numeric (Numéricos)\n\nRepresentan números decimales o reales, como medidas continuas.\nIncluyen valores positivos, negativos y fracciones.\n\n\nx &lt;- 7.85  # Elemento de tipo numeric\ny &lt;- -23.2\n\nDirectamente relacionado con variables numéricas continuas, como peso o altura.\nd. Character (Cadenas de Texto)\n\nRepresentan texto o cadenas de caracteres.\nUtilizados para datos categóricos o nominales.\n\n\nx &lt;- \"Peru\"  # Elemento de tipo character\ny &lt;- \"Colombia\"\n\n\n\n\n\n\n\n\n\nTipo de Variable\nDefinición\nEn R\n\n\n\n\nNuméricas Continuas\nValores dentro de un rango continuo, como altura, peso, temperatura.\nNumeric: Representa decimales o reales.\n\n\nNuméricas Discretas\nValores enteros que representan conteos, como número de personas o hijos.\nInteger: Representa números enteros, sin decimales.\n\n\nCategóricas Nominales\nCategorías sin un orden inherente, como colores o géneros.\nCharacter: Representa texto (e.g., “rojo”, “azul”).\n\n\nCategóricas Ordinales\nCategorías con un orden lógico, como niveles educativos (bajo, medio, alto).\nCharacter o Factor: Puede usarse texto, aunque los factores facilitan el orden.\n\n\nVariables Lógicas (Binarias)\nSolo dos valores posibles, como sí o no, aprobado o reprobado.\nLogical: Representa valores TRUE o FALSE.\n\n\n\nPor ejemplo:\n\n# Creamos distintos tipos de objetos\n\nnum = 3.5 \nlogic = FALSE\nchr = \"Peru\"\n\n# Los podemos mostrar con solo llamar el nombre de la objeto\n\nnum\n\n[1] 3.5\n\nlogic\n\n[1] FALSE\n\nchr\n\n[1] \"Peru\"\n\n\nUna forma muy práctica de asegurarnos qué tipo de elemento es la objeto es llamar a la función class(), simplemente tomando como argumento el nombre del objeto:\n\nclass(num)\n\n[1] \"numeric\"\n\n\nHagámoslo para el resto:\n\nclass(logic)\n\n[1] \"logical\"\n\nclass(chr)\n\n[1] \"character\"\n\n\n\n\n2.7.2 Operaciones básicas\nEn R, los valores numéricos y lógicos son elementos computables. Es decir, pueden utilizarse para realizar operaciones matemáticas mediante operadores aritméticos. Los operadores más comunes incluyen:\n\nAdición: +\nSustracción: -\nMultiplicación: *\nDivisión: /\nExponencial: ^\n\nA continuación, algunos ejemplos prácticos:\n\n# Operaciones básicas\n4 + 7      # Suma: 11\n\n[1] 11\n\n10 - 6     # Resta: 4\n\n[1] 4\n\n6 * 5      # Multiplicación: 30\n\n[1] 30\n\n(4 + 2) / 2  # División: 3\n\n[1] 3\n\n# Exponencial: 2 elevado a la potencia 5\n2^5       # Resultado: 32\n\n[1] 32\n\n\nLos valores lógicos (TRUE y FALSE) también pueden ser usados en operaciones matemáticas. R los interpreta como números:\n\nTRUE se considera igual a 1\nFALSE se considera igual a 0\n\n\nFALSE + TRUE\n\n[1] 1\n\n\n\nFALSE + FALSE\n\n[1] 0\n\n\n\nTRUE + TRUE\n\n[1] 2\n\n\nUsar valores lógicos de esta forma puede ser muy útil, especialmente al trabajar con condiciones o conteos.\nA diferencia de los valores numéricos y lógicos, los elementos caracteres (character) no pueden participar en operaciones matemáticas. Sin embargo, pueden ser manipulados mediante funciones específicas para manejar texto, como paste() y paste0(), que permiten concatenar (unir) cadenas de texto.\n\ntexto1 = \"Hola\"\ntexto2 = \"Mundo\"\n\n\n# Concatenar texto con un espacio\npaste(texto1, texto2)  \n\n[1] \"Hola Mundo\"\n\n\n\n# Concatenar texto sin espacio\npaste0(texto1, texto2)  \n\n[1] \"HolaMundo\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#tipos-de-objetos",
    "href": "fundamentosR.html#tipos-de-objetos",
    "title": "2  Fundamentos de R",
    "section": "2.8 Tipos de objetos",
    "text": "2.8 Tipos de objetos\nUna vez establecido que un elemento se refiere al tipo de dato en sí mismo (como numérico, lógico o de caracteres). Ahora nos centraremos en los objetos. Un objeto es una estructura de datos que puede contener uno o más elementos. Los tipos de objetos principales incluyen vectores, matrices, listas, dataframes y factores.\n\nVectores:\n\nSon una secuencia de elementos del mismo tipo.\nPueden ser de tipo numérico, lógico, o de caracteres.\nSe pueden crear utilizando la función c().\n\n\n\n# Ejemplo de vector numérico\nnumeros = c(1, 2, 3, 4, 5)\n   \n# Ejemplo de vector de caracteres\nletras = c(\"a\", \"b\", \"c\", \"d\", \"e\")\n\nnumeros\n\n[1] 1 2 3 4 5\n\nletras\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n\n\n\n\nEl tipo de vector depende del tipo de elementos que contenga. Elaboración propia\n\n\n\nMatrices:\n\nSon una colección de elementos del mismo tipo, organizada en filas y columnas.\nSe pueden crear utilizando la función matrix().\n\n\n\n# Ejemplo de matriz numérica\nmatriz_numerica = matrix(1:9, nrow = 3, ncol = 3)\n   \n# Ejemplo de matriz de caracteres\nmatriz_caracteres = matrix(letters[1:9], nrow = 3, ncol = 3)\n\nmatriz_numerica\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\nmatriz_caracteres\n\n     [,1] [,2] [,3]\n[1,] \"a\"  \"d\"  \"g\" \n[2,] \"b\"  \"e\"  \"h\" \n[3,] \"c\"  \"f\"  \"i\" \n\n\n\n\n\nEl tipo de matriz también depende del tipo de elementos que contenga. Elaboración propia\n\n\n\nListas:\n\nSon una colección de objetos en un orden determinado que pueden ser de distintos tipos\nSe pueden crear utilizando la función list().\n\n\n\n# Ejemplo de lista\nmi_lista = list(numeros, letras, matriz_numerica)\n\nmi_lista\n\n[[1]]\n[1] 1 2 3 4 5\n\n[[2]]\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n[[3]]\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\n\nUna lista es capaz de almacenar elementos e incluso objetos de todo tipo, incluso otras listas. Elaboración propia\n\n\n\nFactores:\n\nSon una forma de sistematizar datos categóricos, puedes utilizarlos para ordenar tus variables ordinales o nominales.\nSe pueden crear utilizando la función factor().\n\n\n\n# Ejemplo de un factor\npreferencias_politicas = c(\"Liberal\", \"Conservador\", \"Progresista\", \n                           \"Liberal\", \"Conservador\", \"Progresista\", \n                           \"Liberal\", \"Conservador\")\n\n# Ejemplo de factor ordenado\ntemperaturas = c(\"Alta\", \"Baja\", \"Alta\", \"Media\", \"Baja\")\ntemperaturas = factor(temperaturas, \n                      levels = c(\"Baja\", \"Media\", \"Alta\"), \n                      ordered = TRUE)\n\n\n\n\nElaboración propia\n\n\n\nDataFrames:\n\nSon estructuras de datos tabulares similares a las tablas de una base de datos u hoja de cálculo.\nLas columnas pueden tener diferentes tipos de datos.\nSe pueden crear utilizando la función data.frame().\n\n\n\n# Ejemplo de data frame\ndatos = data.frame(numeros, letras, temperaturas)\n\ndatos\n\n  numeros letras temperaturas\n1       1      a         Alta\n2       2      b         Baja\n3       3      c         Alta\n4       4      d        Media\n5       5      e         Baja\n\n\n\n\n\nLos data.frames son uno de los objetos más comunes para almacenar conjuntos de datos. Elaboración propia\n\n\nNos detendremos en este capítulo en los vectores, pues son el tipo de objeto que, junto con el data.frame, encontraremos más frecuentemente en nuestro trabajo diario analizando datos. El resto de ellos los veremos con mayor detalle en el transcurso de la guía.\n\n2.8.1 Vectores\nComo ya habíamos dicho, un vector en R es una forma de almacenar y organizar una secuencia de elementos (numeric, logical, character) que podemos usar para diversas operaciones. Los vectores pueden contener números, texto o valores lógicos, y puedes realizar operaciones en todo el conjunto de valores en un vector a la vez.\nPara crear un vector solo debemos escribir la función c() y poner cada elemento como argumento de esta forma:\n\nc(2, 4, 6)\n\n[1] 2 4 6\n\n\nPodemos asignarle este vector a una objeto y R lo reconocerá\n\nmi_vector = c(2, -4, 6)  \n\nmi_vector\n\n[1]  2 -4  6\n\n\nFunciona de la misma forma para elementos lógicos y de texto\n\nmi_vector_log = c(FALSE, TRUE, TRUE, FALSE)  \n\nmi_vector_chr = c(\"Hey\", \"un placer\")  \n\nmi_vector_log  \n\n[1] FALSE  TRUE  TRUE FALSE\n\nmi_vector_chr\n\n[1] \"Hey\"       \"un placer\"\n\n\n\n\n2.8.2 Operaciones con vectores\nAdemás de crearlos, también podemos realizar operaciones matemáticas directamente sobre ellos. Supongamos que tienes un vector con los valores:\n\nmi_vector = c(2, 4, 6, 3, 9, 12)\n\nSi sumas un número, este se aplica a cada elemento del vector:\n\nmi_vector + 1\n\n[1]  3  5  7  4 10 13\n\n\nMultiplicar un vector por un número aplica la operación a cada elemento:\n\nmi_vector * 2\n\n[1]  4  8 12  6 18 24\n\n\nEsto funciona con cualquier operador aritmético que necesites.\nSi tienes dos vectores de igual longitud, R combina los elementos en la misma posición para realizar la operación:\n\notro_vector = c(1, 2, 3, 4, 14, 7)\n\n\nmi_vector + otro_vector\n\n[1]  3  6  9  7 23 19\n\n\n\nmi_vector * otro_vector\n\n[1]   2   8  18  12 126  84\n\n\nAdemás, R tiene funciones integradas para calcular estadísticas básicas sobre un vector. Estas son algunas de las más comunes:\n\nSuma total de elementos:\n\n\nsum(mi_vector)\n\n[1] 36\n\n\n\nPromedio de los valores:\n\n\nmean(mi_vector)\n\n[1] 6\n\n\n\nMáximo y mínimo del vector:\n\n\nmax(mi_vector)  \n\n[1] 12\n\nmin(mi_vector)  \n\n[1] 2\n\n\n\nProducto de todos los elementos:\n\n\nprod(mi_vector)\n\n[1] 15552\n\n\nAdicionalmente, puedes realizar comparaciones directamente sobre los elementos de un vector. Esto genera un nuevo vector lógico (TRUE o FALSE) basado en si se cumple o no la condición.\nEjemplo:\n\n# Siendo el vector\nmi_vector\n\n[1]  2  4  6  3  9 12\n\n\n\nmi_vector &gt; 3\n\n[1] FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n\n\nRecuerda que R interpreta TRUE = 1 y FALSE = 0. Combinar operaciones lógicas con funciones como sum() te permite contar cuántos elementos cumplen una condición:\n\nsum(mi_vector &gt; 3)\n\n[1] 4\n\n\nSi deseas verificar si todos los elementos cumplen una condición, puedes usar all():\n\nall(mi_vector &gt; 1)\n\n[1] TRUE\n\n\nPara verificar si algún elemento cumple una condición, utiliza any():\n\nany(mi_vector &gt; 18)\n\n[1] FALSE\n\n\n\n\n2.8.3 Selección de elementos en vectores\nPuedes extraer elementos específicos usando corchetes []. Para acceder a un elemento en una posición específica, indica su índice dentro de los corchetes. Esto significa acceder a un valor según su posición dentro del vector. En R, las posiciones comienzan en 1, no en 0 como en otros lenguajes de programación (como Python, por eejmplo).\n\n# Seleccionar el primer elemento del vector\nmi_vector[1]\n\n[1] 2\n\n\nEsto selecciona únicamente el elemento en la posición 1. Si quisieras acceder al tercer elemento, solo tienes que cambiar el índice:\n\n# Seleccionar el tercer elemento del vector\nmi_vector[3]\n\n[1] 6\n\n\nSi necesitas varios elementos al mismo tiempo, puedes utilizar la función c() para indicar los índices que deseas seleccionar. Por ejemplo, si quieres el primer y el tercer elemento:\n\n# Seleccionar el primer y cuarto elemento del vector\nmi_vector[c(1, 4)]\n\n[1] 2 3\n\n\nTambién puedes seleccionar un rango de elementos usando el operador :, que genera una secuencia de números. Por ejemplo, para seleccionar todos los elementos desde la posición 2 hasta la posición 4:\n\n# Seleccionar un rango de elementos (del segundo al quinto)\nmi_vector[2:5]\n\n[1] 4 6 3 9\n\n\nEn lugar de acceder a los elementos por su posición, también puedes utilizar operadores de comparación para filtrar elementos basándote en condiciones. Los operadores de comparación en R incluyen:\n\n&lt; para menor que.\n&gt; para mayor que.\n&lt;= para menor o igual que.\n&gt;= para mayor o igual que.\n== para igualdad.\n!= para desigualdad.\n\nCuando usas un operador de comparación en un vector, R evalúa cada elemento y selecciona los que cumplan la condición (TRUE). Por ejemplo:\n\n# Seleccionar elementos mayores que 10\nmi_vector[mi_vector &gt; 10]\n\n[1] 12\n\n\nEsto devuelve todos los elementos del vector que son mayores que 10. Si quisieras filtrar los elementos que son menores o iguales a 6, lo harías así:\n\n# Seleccionar elementos menores o iguales a 6\nmi_vector[mi_vector &lt;= 6]\n\n[1] 2 4 6 3\n\n\nLa comparación también funciona para buscar elementos iguales o diferentes a un valor específico. Por ejemplo, si quisieras encontrar solo los elementos iguales a 25:\n\n# Seleccionar elementos iguales a 9\nmi_vector[mi_vector == 9]\n\n[1] 9\n\n\nO si quisieras excluir un valor en particular, puedes usar !=:\n\n# Seleccionar elementos diferentes de 12\nmi_vector[mi_vector != 12]\n\n[1] 2 4 6 3 9\n\n\nCuando necesitas aplicar más de una condición al mismo tiempo, puedes combinar operadores de comparación con los operadores lógicos & y |. Estas combinaciones te permiten filtrar elementos basados en condiciones más complejas.\nCuando utilizas & , estás pidiendo que todas las condiciones sean verdaderas al mismo tiempo para que un elemento sea seleccionado. Esto significa que cada elemento del vector debe cumplir cada condición individualmente.\nSupongamos que quieres seleccionar elementos que sean mayores que 5 y al mismo tiempo menores que 10. La lógica es que un valor solo será seleccionado si cumple ambas condiciones. Así es como lo harías:\n\n# Seleccionar elementos mayores que 5 y menores que 10\nmi_vector[mi_vector &gt; 5 & mi_vector &lt; 10]\n\n[1] 6 9\n\n\nPor otro lado, | selecciona elementos que cumplan al menos una de las condiciones. Esto significa que un elemento será seleccionado si cumple la primera condición, la segunda, o ambas.\nSi quisieras seleccionar elementos que sean menores que 5 o mayores que 10, usarías |:\n\n# Seleccionar elementos menores que 10 o mayores que 5\nmi_vector[mi_vector &lt; 10 | mi_vector &gt; 10]\n\n[1]  2  4  6  3  9 12\n\n\nUna forma sencilla de recordar esto:\n- & (y lógico): Solo selecciona valores cuando todas las condiciones son verdaderas.\n- | (o lógico): Selecciona valores cuando cualquiera de las condiciones es verdadera.\nCuando trabajas con vectores nombrados, puedes seleccionar elementos directamente usando sus nombres. Primero, debes asignar nombres a los elementos del vector. Esto se puede hacer al momento de crearlo o utilizando la función names():\n\n# Crear un vector con nombres\nnames(mi_vector) = c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\n\nUna vez que los elementos tienen nombres, puedes acceder a ellos indicando el nombre entre comillas:\n\n# Seleccionar un elemento por su nombre\nmi_vector[\"c\"]\n\nc \n6 \n\n\nTambién puedes seleccionar varios elementos utilizando un vector de nombres:\n\n# Seleccionar varios elementos por sus nombres\nmi_vector[c(\"b\", \"d\")]\n\nb d \n4 3 \n\n\nLos nombres son especialmente útiles cuando los elementos tienen un significado asociado, ya que hacen que el código sea más fácil de leer y entender.\nPor último, debe saber también saber que puedes guardar los elementos seleccionados en un nuevo objeto para trabajar con ellos más adelante. Por ejemplo, si quisieras guardar todos los elementos mayores que 8 en un nuevo vector:\n\n# Guardar elementos mayores que 8 en un nuevo objeto\nvalores_altos &lt;- mi_vector[mi_vector &gt; 8]\n\nDe manera similar, podrías guardar los elementos menores o iguales a 4 en otro vector:\n\n# Guardar elementos menores o iguales a 4 en un nuevo objeto\nvalores_bajos &lt;- mi_vector[mi_vector &lt;= 4]\n\nAl almacenar los resultados en nuevos objetos, puedes realizar análisis posteriores o manipular solo una parte específica de tus datos sin alterar el vector original.\n\nvalores_altos\n\n e  f \n 9 12 \n\n\n\nvalores_bajos\n\na b d \n2 4 3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#flor-y-el-análisis-de-datos-electorales",
    "href": "fundamentosR.html#flor-y-el-análisis-de-datos-electorales",
    "title": "2  Fundamentos de R",
    "section": "2.9 Flor y el análisis de datos electorales",
    "text": "2.9 Flor y el análisis de datos electorales\nFlor decidió analizar los patrones de participación electoral en su región. Había encontrado un archivo con los porcentajes de participación de las últimas elecciones municipales y comenzó organizando su trabajo.\nLo primero que hizo Flor fue crear una carpeta en su computadora llamada “Proyecto_Electoral” y establecerla como su DIRECTORIO DE TRABAJO en RStudio. Esto aseguraba que todos los archivos y scripts se gestionaran de manera ordenada:\n\n# setwd(\"C:/Usuarios/Flor/Documentos/Proyecto_Electoral\")\n\nCon esta configuración, estaba lista para trabajar en su análisis.\nFlor sabía que los datos que manejaría en su análisis podían representarse mediante diferentes TIPOS DE ELEMENTOS en R. Decidió repasar estos conceptos creando algunos ejemplos:\n\nELEMENTOS NUMÉRICOS: Para almacenar valores continuos, como porcentajes de participación:\n\n\nporcentaje = 68.5\nclass(porcentaje)  \n\n[1] \"numeric\"\n\n\n\nELEMENTOS DE TEXTO: Para identificar las elecciones por su año:\n\n\neleccion = \"2012\"\nclass(eleccion)  \n\n[1] \"character\"\n\n\n\nELEMENTOS LÓGICOS: Para indicar si la participación superaba el promedio:\n\n\nalta_participacion = TRUE\nclass(alta_participacion)  \n\n[1] \"logical\"\n\n\nSatisfecha con este repaso, Flor pasó a trabajar con TIPOS DE OBJETOS más complejos. Recopiló los porcentajes de participación de seis elecciones consecutivas y los almacenó en un VECTOR, que es un tipo de OBJETO que contiene ELEMENTOS del mismo tipo:\n\nparticipacion = c(67.5, 74.6, 65.8, 78.3, 68.3, 66.7)\n\nAl ser un VECTOR NUMÉRICO, podía realizar operaciones directamente sobre todos sus elementos. Calculó el promedio de participación con la función mean():\n\npromedio = mean(participacion)\npromedio  \n\n[1] 70.2\n\n\nPara hacer su VECTOR más legible, asignó nombres a los elementos, representando los años de las elecciones:\n\nnames(participacion) = c(\"2012\", \"2014\", \"2016\", \"2018\", \"2020\", \"2022\")\nparticipacion\n\n2012 2014 2016 2018 2020 2022 \n67.5 74.6 65.8 78.3 68.3 66.7 \n\n\nEsto resultó en un VECTOR nombrado, donde cada valor estaba asociado con su respectivo año. Luego, usó una OPERACIÓN LÓGICA para identificar las elecciones con participación superior al promedio. Esto devolvió un VECTOR LÓGICO, otro tipo de OBJETO que almacena ELEMENTOS TRUE o FALSE:\n\n# El elemento objeto fue nombrado previamente\nparticipacion &gt; promedio \n\n 2012  2014  2016  2018  2020  2022 \nFALSE  TRUE FALSE  TRUE FALSE FALSE \n\n\nLuego, filtró los valores mayores al promedio utilizando esta condición:\n\nparticipacion_alta = participacion[participacion &gt; promedio]\nparticipacion_alta\n\n2014 2018 \n74.6 78.3 \n\n\nDe esta manera, Flor creó un nuevo VECTOR que contenía solo los años con alta participación.\nFlor recordó que R tiene una gran cantidad de PAQUETES que extienden sus funcionalidades. Decidió usar el PAQUETE psych para obtener un resumen estadístico más detallado de su VECTOR de participación. Primero, instaló el PAQUETE:\n\n# install.packages(\"psych\")\n\nLuego, lo cargó en su sesión de trabajo:\nCon la función describe() del PAQUETE psych, generó un resumen estadístico que incluía medidas como la media, la desviación estándar y los valores mínimo y máximo:\n\npsych::describe(participacion)\n\n   vars n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 6 70.2 5.05   67.9    70.2 2.45 65.8 78.3  12.5  0.6    -1.65 2.06\n\n\nHablaremos con mayor detalle sobre describe y como podemos interpretar sus resultados en el capítulo 4.\n\n\n\nSaber R te da poder",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#resumen-del-capítulo",
    "href": "fundamentosR.html#resumen-del-capítulo",
    "title": "2  Fundamentos de R",
    "section": "2.10 Resumen del capítulo",
    "text": "2.10 Resumen del capítulo\nR es un lenguaje de programación diseñado específicamente para el análisis estadístico y la creación de gráficos. Es gratuito, de código abierto, y cuenta con una comunidad activa que amplía continuamente sus capacidades. RStudio es el entorno donde se trabaja con R, organizando el código, los resultados y las herramientas en paneles funcionales: la Fuente para escribir y editar scripts, la Consola para ejecutar comandos, el Entorno para explorar objetos, y un panel adicional para gestionar gráficos, archivos y paquetes.\nLas funciones son bloques de código que realizan tareas específicas, como cálculos, transformación de datos o visualización gráfica. Por ejemplo, sum() calcula la suma de valores. Los paquetes en R son colecciones de funciones especializadas que amplían las capacidades del lenguaje. Para usarlos, primero deben instalarse con install.packages() y luego cargarse con library().\nLos elementos en R representan datos básicos y están asociados a tipos como numéricos (para valores continuos), enteros (para conteos), lógicos (TRUE o FALSE), y caracteres (texto). Estos tipos se relacionan con variables estadísticas, como valores continuos o categorías. Los elementos lógicos se usan para realizar comparaciones, y los numéricos permiten operaciones matemáticas básicas,.\nLos objetos en R son estructuras que almacenan datos y facilitan su organización. Los principales tipos son: vectores (secuencia de elementos del mismo tipo), matrices (estructuras tabulares del mismo tipo), listas (colecciones heterogéneas), factores (categorías), y data frames (estructuras tabulares donde cada columna puede almacenar diferentes tipos de elementos).\nLos vectores son el tipo de objeto más básico en R y contienen elementos del mismo tipo. Se pueden realizar operaciones matemáticas o estadísticas directamente sobre ellos.También es posible filtrar elementos con índices o condiciones lógicas,",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "fundamentosR.html#ejercicios",
    "href": "fundamentosR.html#ejercicios",
    "title": "2  Fundamentos de R",
    "section": "2.11 Ejercicios",
    "text": "2.11 Ejercicios\n1. En RStudio, puedes escribir código tanto en el script como en la consola. ¿Cuál es la principal diferencia entre estos dos lugares?\n\na) El código escrito en el script se guarda para futuras referencias, mientras que el de la consola no se guarda automáticamente\nb) El script permite ejecutar múltiples líneas de código, mientras que la consola solo ejecuta una línea a la vez\nc) La consola es solo para pruebas rápidas y no permite editar el código como el script\nd) Todas las anteriores\n2. ¿Qué pasos debes seguir para instalar y luego cargar el paquete dplyr en R?\n\na) Ejecutar las funciones install.packages(\"dplyr\") y luego library(dplyr) en ese orden\nb) Escribir install.packages(\"dplyr\") y luego automáticamente el paquete estará disponible en la sesión\nc) Solo ejecutar library(dplyr) si ya tienes el paquete instalado previamente\nd) Instalar el paquete una sola vez y luego no necesitas cargarlo manualmente nunca más\n3. Tienes el siguiente vector en R:\n\nmi_vector = c(3, 6, 9, 12)\n\n¿Qué operación correctamente suma 5 a cada elemento del vector?\n\na) sum(mi_vector + 5)\nb) mi_vector + 5\nc) mi_vector[mi_vector + 5]\nd) mi_vector &lt;- 5 + mi_vector\n4. ¿Qué tipo de datos representan los siguientes valores en R?\n\n- TRUE/FALSE\n- 12.5, 4.3, 0.8\n- “Rojo”, “Azul”, “Verde”\n\nLógico; Numérico; Carácter\n\nOrdinal; Continuo; Nominal\n\nLógico; Continuo; Texto\n\nEntero; Lógico; Nominal\n\n5. Dado el siguiente vector:\n\nvalores = c(2, 4, 6, 8, 10)\n\n¿Qué código devuelve únicamente los elementos mayores a 6?\n\na) valores[valores &gt; 6]\nb) subset(valores &gt; 6)\nc) valores[6]\nd) valores[valores &lt; 6]\n6. ¿Qué comando te permite verificar el tipo de datos almacenado en un objeto en R?\n\na) describe() b) sum()\nc) mean()\nd) class()\n7. Un investigador trabaja con el siguiente objeto en R:\n\ntransporte &lt;- c(\"Bus\", \"Taxi\", \"Bicicleta\", \"Bus\", \"Bicicleta\")\n\n¿Qué tipo de objeto es transporte?\n\na) Matriz\nb) Vector\nc) Lista\nd) Data Frame\n8. ¿Cuál de las siguientes opciones define correctamente este factor en R?\n\nfactor(c(\"Alto\", \"Medio\", \"Bajo\"),\n       levels = c(\"Bajo\", \"Medio\", \"Alto\"),\n       ordered = TRUE)\n\n[1] Alto  Medio Bajo \nLevels: Bajo &lt; Medio &lt; Alto\n\n\na) Una variable nominal\nb). Un vector de caracteres\nc). Un vector lógico\nd). Una variable ordinal\n9. Dado el siguiente vector:\n\nnumeros &lt;- c(2, 4, 6, 8)\n\n¿Qué resultado devuelve la operación numeros * 2 + 1?\na) Un vector con valores: 3, 5, 7, 9\nb) Un vector con valores: 5, 9, 13, 17\nc) Un vector con valores: 6, 9, 12, 15\nd) Un vector con valores: 4, 8, 12, 16\n10. Supongamos que tienes el siguiente vector:\n\nmi_vector &lt;- c(3, 6, 9, 12, 15, 18)\n\n¿Qué código selecciona correctamente los elementos que son mayores a 5 y menores a 15?\n\na) mi_vector[mi_vector &gt; 5 | mi_vector &lt; 15]\nb) mi_vector[mi_vector &gt; 5 & mi_vector &lt; 15]\nc) mi_vector[5 & 15]\nd) subset(mi_vector &gt; 5 & mi_vector &lt; 15)\n\n\n\n\nDataCamp. 2023. «All About R: A Popular Language for Data Science». https://www.datacamp.com/blog/all-about-r.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos de R</span>"
    ]
  },
  {
    "objectID": "integración.html",
    "href": "integración.html",
    "title": "3  Integración",
    "section": "",
    "text": "3.1 Data.frames\nCuando recolectamos o hacemos uso de conjuntos de datos, estos suelen estar almacenados en estructuras tabulares, lo que facilita su comprensión y análisis. Una estructura tabular se refiere a una organización de los datos donde cada columna representa una variable (es decir, una característica o atributo que estamos observando), y cada fila corresponde a una observación (un registro individual de los datos, como un caso o instancia). En R, la forma más común de trabajar con este tipo de estructuras es a través de objetos denominados data.frames.\nRecuerda que, en el capítulo anterior, cuando exploramos los tipos de objetos en R, vimos que, a diferencia de una matriz (que también tiene una forma tabular pero almacena datos de un solo tipo), un data.frame permite que cada columna contenga datos de diferentes tipos. Esto significa que podemos trabajar simultáneamente con variables numéricas, categóricas o lógicas dentro de una misma estructura.\nImaginemos que queremos crear un conjunto de datos con tres variables:\n- Edad: numérica, que representa la edad de los encuestados.\n- Género: categórica, con valores como “Masculino” y “Femenino”.\n- Medio de comunicación preferido: categórica, con valores como “Televisión”, “Radio”, “Internet” y “Prensa Escrita”.\nPodemos construir este data.frame en R utilizando vectores para cada variable y luego combinándolos:\n# Crea vectores para cada variable\nedad = c(25, 30, 35, 40, 28)\ngenero = c(\"Femenino\", \"Masculino\", \"Femenino\", \n           \"Masculino\", \"Femenino\")\nmedio_comunicacion = c(\"Internet\", \"Televisión\", \"Radio\", \n                        \"Prensa Escrita\", \"Internet\")\n\n# Combina los vectores en un data.frame\nencuesta = data.frame(Edad = edad, \n                       Género = genero, \n                       Medio_de_comunicación_preferido = medio_comunicacion)\nEl resultado es una tabla que muestra la edad, el género y medio de comunicación preferidos de los encuestados, estructurados en filas y columnas.\nencuesta\n\n  Edad    Género Medio_de_comunicación_preferido\n1   25  Femenino                        Internet\n2   30 Masculino                      Televisión\n3   35  Femenino                           Radio\n4   40 Masculino                  Prensa Escrita\n5   28  Femenino                        Internet\nAdemás, peudes seleccionar una columna de data.frame de forma individual utilizando el signo $\nencuesta$Medio_de_comunicación_preferido\n\n[1] \"Internet\"       \"Televisión\"     \"Radio\"          \"Prensa Escrita\"\n[5] \"Internet\"\nPara observar el data.frame de manera visual, RStudio ofrece herramientas muy convenientes. Podemos localizar el nombre del data.frame en el panel “Entorno” (Environment) y hacer clic en el ícono de la tabla que aparece al lado. Esto abrirá una vista interactiva en forma de hoja de cálculo, donde podrás explorar las filas y columnas de tu conjunto de datos.\nOtra forma, desde la consola, es usar la función View(). Por ejemplo, si tu data.frame se llama encuesta, simplemente escribe el siguiente código en la consola:\n# View(encuesta)\nAunque esta es una forma de generar un data.frame, es importante mencionar que, en la práctica, la mayoría de los conjuntos de datos no se crean desde cero. Normalmente, los datos provienen de otras fuentes, como archivos de texto, hojas de cálculo o bases de datos. Por ello, uno de los pasos más importantes al trabajar con datos es importarlos desde estas fuentes, lo cual veremos en más detalle más adelante.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integración</span>"
    ]
  },
  {
    "objectID": "integración.html#data.frames",
    "href": "integración.html#data.frames",
    "title": "3  Integración",
    "section": "",
    "text": "Elaboración propia",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integración</span>"
    ]
  },
  {
    "objectID": "integración.html#el-flujo-de-trabajo-en-el-tidyverse",
    "href": "integración.html#el-flujo-de-trabajo-en-el-tidyverse",
    "title": "3  Integración",
    "section": "3.2 El flujo de trabajo en el Tidyverse",
    "text": "3.2 El flujo de trabajo en el Tidyverse\nComo vimos en el anterior capítulo, un paquete en R es una colección de funciones y datos que extienden las capacidades básicas del lenguaje. Los paquetes te permiten realizar tareas específicas de manera más eficiente. Al instalar y cargar un paquete, se accede a un conjunto de herramientas especializadas que simplifican tu trabajo.\nDebido a que existe una multitud enorme de paquetes, sugiero les eches un vistazo siempre que puedas. Debido al propósito de este libro, nos centraremos especialmente en uno de estos que, a nivel personal, es muy intuitivo y poderoso: El Tidyverse. (Wickham et al. 2019)\nEl tidyverse es un sistema coherente de paquetes para la manipulación, exploración y visualización de datos que comparten una lógica de diseño común. Estos paquetes son fundamentales en el flujo de trabajo de un analista de datos y, en mi opinión personal, son algunos de los más comprensibles e intuitivos que existen en el mundo de la programación.\nAlgunas características del tidyverse:\n\nTodos los paquetes del tidyverse siguen principios de diseño similares y se entienden entre sí.\nLa sintaxis y las funciones están diseñadas para ser fáciles de entender y usar.\n\n\n\n\nExtraído de teachdatascience.com\n\n\nLa imagen ilustra el flujo de trabajo típico al analizar datos usando los paquetes del tidyverse. Recuerda que debemos seguir un orden, pues esto asegura que podamos manejar nuestros datos de manera coherente, reproducible y eficiente.\nEl flujo de trabajo incluye las siguientes etapas principales:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integración</span>"
    ]
  },
  {
    "objectID": "integración.html#importación-y-resumen",
    "href": "integración.html#importación-y-resumen",
    "title": "3  Integración",
    "section": "3.3 Importación y resumen",
    "text": "3.3 Importación y resumen\nEl primer paso en el flujo de trabajo es importar los datos desde fuentes externas al entorno de R. En las ciencias sociales, los investigadores suelen trabajar con bases de datos provenientes de encuestas, experimentos o datos recolectados en plataformas como hojas de cálculo de Excel, herramientas de encuestas en línea o programas de análisis estadístico como SPSS o Stata. Aunque R incluye funciones base (viene por defecto y no necesitan cargar algún paquete) para la importación de datos, como read.csv(), en este capítulo nos enfocaremos en los paquetes especializados como readr, readxl y haven, que son más actuales y generan tibbles, una versión mejorada de los data.frames.\nUn ejemplo interesante es el caso de la Encuesta Nacional de Hogares (Enaho), cuyos datos son publicados por las instituciones encargadas en diferentes formatos: CSV, Stata (.dta) y SPSS (.sav). Esto demuestra cómo una misma base de datos puede estar disponible en múltiples formatos para adaptarse a las herramientas utilizadas por distintos analistas. Por ejemplo, un investigador que utiliza Excel podría optar por trabajar con el formato CSV, mientras que alguien que trabaja con Stata o SPSS puede preferir los archivos nativos de esos programas.\n\n3.3.1 Importación\nLos archivos que utilizaremos a lo largo de este libro están disponibles en la carpeta de archivos del libro, la cual se recomienda descargar y guardar en una carpeta de trabajo en tu computadora. Para seguir los ejemplos en este capítulo, asegúrate de tener los archivos en tu carpeta de trabajo y configurar tu directorio de trabajo en RStudio.\nEl formato CSV (Comma-Separated Values) es uno de los más utilizados debido a su simplicidad y compatibilidad. Cada fila en un archivo CSV representa una observación, y los valores dentro de cada fila están separados por comas.\nUn ejemplo de cómo podría lucir un archivo CSV:\nID,Edad,Género,Ingreso\n1,25,Femenino,1500\n2,30,Masculino,2000\n3,45,Femenino,2500\nEl paquete readr es parte del tidyverse y está diseñado para leer archivos CSV.\n\n# Cargamos el paquete readr\nlibrary(readr)\n\nAl importar un conjunto de datos debemos nombrarlo.\n\n# Importamos el archivo CSV y lo asignamos a un objeto\nencuesta_csv = read_csv(\"encuesta.csv\")\n\n# Mostramos\nencuesta_csv\n\n# A tibble: 8 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Masculino Televisión            34\n2 Femenino  Redes sociales        25\n3 Femenino  Redes sociales        55\n4 Otro      Radio                 63\n5 Femenino  Televisión            47\n6 Masculino Redes sociales        19\n7 Masculino Redes sociales        29\n8 Masculino Periódico             75\n\n\nLos archivos de Excel son comunes en las ciencias sociales debido a su facilidad de uso y capacidad para almacenar datos tabulares en varias hojas. El paquete readxl (Wickham y Bryan 2023) permite importar estos archivos, ya sea en formato .xls o .xlsx, sin necesidad de tener Excel instalado.\n\n# Cargamos el paquete readxl\nlibrary(readxl)\n\n\n# Importamos los datos desde un archivo Excel\nencuesta_excel = read_excel(\"encuesta.xlsx\")\n\n# Mostramos \nencuesta_excel\n\n# A tibble: 10 × 3\n   genero    medio_comunicación  edad\n   &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n 1 Masculino Televisión            34\n 2 Femenino  Redes sociales        NA\n 3 Femenino  Redes sociales        55\n 4 Otro      Radio                 63\n 5 Femenino  Televisión            NA\n 6 Masculino Redes sociales        19\n 7 Masculino &lt;NA&gt;                  29\n 8 Masculino Periódico             75\n 9 Femenino  Redes sociales        55\n10 Masculino Televisión            34\n\n\nSi el archivo contiene múltiples hojas, podemos especificar cuál importar utilizando el argumento sheet:\n\n# Importamos una hoja específica del archivo Excel\n# encuesta_excel_hoja = read_excel(\"encuesta.xlsx\", \n#                                  sheet = \"Resultados\")\n\nEn ciencias sociales es común trabajar con datos provenientes de software estadístico como Stata o SPSS. Estos formatos incluyen información adicional, como etiquetas de variables, que son importantes para interpretar los datos correctamente. Para trabajar con estos archivos en R, utilizamos el paquete haven (Wickham, Miller, y Smith 2023).\n\n# Cargamos el paquete haven\nlibrary(haven)\n\nImportamos un archivo Stata y lo asignamos a un objeto\n\nencuesta_stata = read_dta(\"encuesta.dta\")\n\n# Mostramos \nencuesta_stata\n\n# A tibble: 5 × 3\n   Edad Género    Medio_de_comunicación_preferido\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;                          \n1    25 Femenino  Internet                       \n2    30 Masculino Televisión                     \n3    35 Femenino  Radio                          \n4    40 Masculino Prensa Escrita                 \n5    28 Femenino  Internet                       \n\n\nO un archivo SPSS\n\nencuesta_spss = read_sav(\"encuesta.sav\")\n\n# Mostramos\nencuesta_spss\n\n# A tibble: 8 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Masculino Televisión            34\n2 Femenino  Redes sociales        25\n3 Femenino  Redes sociales        55\n4 Otro      Radio                 63\n5 Femenino  Televisión            47\n6 Masculino Redes sociales        19\n7 Masculino Redes sociales        29\n8 Masculino Periódico             75\n\n\n\n\n3.3.2 Funciones resumen\nUna vez que hemos importado los datos a R, es fundamental conocer su estructura y contenido antes de proceder con el análisis. Para ello, podemos utilizar una variedad de funciones resumen que nos permiten explorar el tibble y obtener información importante sobre las variables y los datos contenidos allí.\nstr() es una función base que describe la estructura del objeto, incluyendo el número de observaciones, las variables y sus tipos. Es muy parecida a glimpse() con la diferencia que es una función base y por lo tanto no depende del ecosistema tidyverse.\n\n# Información estructural básica del tibble\nstr(encuesta_csv)\n\nspc_tbl_ [8 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ genero            : chr [1:8] \"Masculino\" \"Femenino\" \"Femenino\" \"Otro\" ...\n $ medio_comunicación: chr [1:8] \"Televisión\" \"Redes sociales\" \"Redes sociales\" \"Radio\" ...\n $ edad              : num [1:8] 34 25 55 63 47 19 29 75\n - attr(*, \"spec\")=\n  .. cols(\n  ..   genero = col_character(),\n  ..   medio_comunicación = col_character(),\n  ..   edad = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nglimpse() es una función del tidyverse que proporciona una vista compacta del tibble, mostrando los nombres de las variables, sus tipos de datos y una muestra de valores.\n\n# Cargamos el paquete tibble\nlibrary(tibble)\n# Exploramos la estructura general del tibble\nglimpse(encuesta_csv)\n\nRows: 8\nColumns: 3\n$ genero             &lt;chr&gt; \"Masculino\", \"Femenino\", \"Femenino\", \"Otro\", \"Femen…\n$ medio_comunicación &lt;chr&gt; \"Televisión\", \"Redes sociales\", \"Redes sociales\", \"…\n$ edad               &lt;dbl&gt; 34, 25, 55, 63, 47, 19, 29, 75\n\n\nsummary() proporciona estadísticas descriptivas básicas para cada columna, como mínimos, máximos, medias, medianas y conteos de valores para variables categóricas. De especial utilidad para el próximo capítulo.\n\n# Resumen estadístico de las variables\nsummary(encuesta_csv)\n\n    genero          medio_comunicación      edad      \n Length:8           Length:8           Min.   :19.00  \n Class :character   Class :character   1st Qu.:28.00  \n Mode  :character   Mode  :character   Median :40.50  \n                                       Mean   :43.38  \n                                       3rd Qu.:57.00  \n                                       Max.   :75.00  \n\n\nhead() muestra las primeras filas del tibble, permitiendo observar una muestra inicial de los datos. Puedes indicar cuanto valores deseas que devuelva especificando el segundo argumento, por defecto son seis.\n\n# Visualizamos las primeras seis filas\nhead(encuesta_csv)\n\n# A tibble: 6 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Masculino Televisión            34\n2 Femenino  Redes sociales        25\n3 Femenino  Redes sociales        55\n4 Otro      Radio                 63\n5 Femenino  Televisión            47\n6 Masculino Redes sociales        19\n\n\n\n# Visualizamos las primeras dos filas\nhead(encuesta_csv, 2)\n\n# A tibble: 2 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Masculino Televisión            34\n2 Femenino  Redes sociales        25\n\n\ntail() es similar a head(), pero muestra las últimas filas del tibble.\n\n# Visualizamos las últimas seis filas\ntail(encuesta_csv)\n\n# A tibble: 6 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Femenino  Redes sociales        55\n2 Otro      Radio                 63\n3 Femenino  Televisión            47\n4 Masculino Redes sociales        19\n5 Masculino Redes sociales        29\n6 Masculino Periódico             75\n\n\n\n# Visualizamos las últimas dos filas\ntail(encuesta_csv, 2)\n\n# A tibble: 2 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Masculino Redes sociales        29\n2 Masculino Periódico             75\n\n\ndim() devuelve las dimensiones del tibble, es decir, el número total de filas y columnas.\n\n# Verificamos las dimensiones del tibble\ndim(encuesta_csv)\n\n[1] 8 3\n\n\ncolnames() muestra los nombres de las columnas (variables) del tibble\n\n# Consultamos los nombres de las columnas\ncolnames(encuesta_csv)\n\n[1] \"genero\"             \"medio_comunicación\" \"edad\"              \n\n\nPerfecto, ya tenemos el primer paso cubierto. Manos a la obra.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integración</span>"
    ]
  },
  {
    "objectID": "integración.html#limpieza",
    "href": "integración.html#limpieza",
    "title": "3  Integración",
    "section": "3.4 Limpieza",
    "text": "3.4 Limpieza\nUna vez que hemos importado los datos, el siguiente paso es limpiarlos. Este proceso consiste en identificar y corregir problemas comunes como valores faltantes, nombres de columnas inconsistentes, duplicados y tipos de datos incorrectos. La limpieza asegura que los datos estén en un estado coherente y listo para ser transformado o analizado.\n\n\n\nExtraído de: https://www.teraflow.ai/3-big-benefits-of-data-cleansing/\n\n\nManejo de valores faltantes\nEl manejo de valores faltantes es uno de los aspectos más complejos en la limpieza de datos, y un tema importante a considerar al trabajar con conjuntos de datos. Un valor perdido o NA en R no es lo mismo que un 0 o un espacio vacío. Un valor perdido (o NA, que significa “Not Available”) es una celda que no contiene información en absoluto, lo que puede ocurrir por diversas razones, como un error en la recolección de los datos, una respuesta no proporcionada en una encuesta o una omisión involuntaria al momento de ingresar los datos. Por ejemplo:\n\nencuesta_excel\n\n# A tibble: 10 × 3\n   genero    medio_comunicación  edad\n   &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n 1 Masculino Televisión            34\n 2 Femenino  Redes sociales        NA\n 3 Femenino  Redes sociales        55\n 4 Otro      Radio                 63\n 5 Femenino  Televisión            NA\n 6 Masculino Redes sociales        19\n 7 Masculino &lt;NA&gt;                  29\n 8 Masculino Periódico             75\n 9 Femenino  Redes sociales        55\n10 Masculino Televisión            34\n\n\nEn R, puedes detectar estos valores con funciones como is.na(), que devuelve un valor lógico (TRUE o FALSE) indicando si un valor es NA. Seleccionamos la columna.\n\nis.na(encuesta_excel$edad)\n\n [1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nRecuerda que puedes sumar un vector lógico para contar los TRUE. En este caso los valores perdidos.\n\n# Cantidad de valores perdidos \nsum(is.na(encuesta_excel$edad)) \n\n[1] 2\n\n\nUna de las formas más simples de manejar valores faltantes es eliminarlos por completo. Esto puede hacerse utilizando la función drop_na() del paquete tidyr (Wickham, Vaughan, y Girlich 2024), que elimina las filas que contienen al menos un valor NA en cualquier columna. Esta es una solución rápida, pero es importante ser cauteloso, ya que puede resultar en la pérdida de información valiosa si hay muchos datos faltantes.\n\n# Cargamos tidyr\nlibrary(tidyr)\n\n\n# Eliminamos filas con valores faltantes\ndrop_na(encuesta_excel)\n\n# A tibble: 7 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Masculino Televisión            34\n2 Femenino  Redes sociales        55\n3 Otro      Radio                 63\n4 Masculino Redes sociales        19\n5 Masculino Periódico             75\n6 Femenino  Redes sociales        55\n7 Masculino Televisión            34\n\n\nComparemos\n\n# Podemos nombrarlo\nencuesta_sin_na = drop_na(encuesta_excel)\n\n# Presta atención a las dimensiones del tibble original y del tibble sin NA\ndim(encuesta_excel)\n\n[1] 10  3\n\ndim(encuesta_sin_na)\n\n[1] 7 3\n\n\nSi queremos ser más específicos y eliminar valores faltantes solo en una columna particular, podemos usar:\n\n# Eliminamos filas donde la columna 'edad' tiene NA\nencuesta_sin_na = drop_na(encuesta_excel, edad)\n\nencuesta_sin_na\n\n# A tibble: 8 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Masculino Televisión            34\n2 Femenino  Redes sociales        55\n3 Otro      Radio                 63\n4 Masculino Redes sociales        19\n5 Masculino &lt;NA&gt;                  29\n6 Masculino Periódico             75\n7 Femenino  Redes sociales        55\n8 Masculino Televisión            34\n\n\nAunque eliminar valores faltantes puede ser un enfoque válido en algunos casos, no siempre es ideal. Si eliminamos demasiadas filas, podemos perder una cantidad significativa de información, lo que podría alterar los resultados de nuestro análisis. Por eso, en lugar de eliminar, muchas veces es preferible imputar los valores faltantes, es decir, reemplazarlos con un valor estimado. Por ejemplo, algunas estrategias comunes para imputar valores incluyen reemplazar por el promedio en el caso de variables numéricas o reemplazar por la moda en variables categóricas donde los valores faltantes pueden ser reemplazados por el valor más frecuente (la moda).\nPara las siguentes técnicas de limpieza utilizaremos el conjunto de datos encuesta_problematica. Te animo a que puedas identificar que posible problemas tiene antes de seguir adelante.\n\ndatos_problema = read.csv('encuesta_problematica.csv')\n\ndatos_problema\n\n  ID Nombre.PARTICIPANTE EDAD..años. Ingreso.Mensual género\n1  1         Luis Bartra          23          1500.0      F\n2  2        Carlos Gómez          27          2000.0      M\n3  3      Cristina Pérez          23          1500.0      F\n4  4         María López          45          2500.0      F\n5  5       Javier Muller          35          3800.0      M\n6  6          Liz García          29          1800.5      M\n\n\nSi te diste cuenta, utilizamos read.csv en lugar de read_csv del paquete readr, ya que read_csv detecta automáticamente el tipo de dato de cada columna, incluso si hay errores en los valores. Para el propósito de este ejemplo, donde queremos ilustrar y solucionar errores comunes en los datos, no lo utilizaremos. ¡Pero ahí tienes otra gran razón para preferir read_csv en tu flujo de trabajo diario!\nEstandarización de nombres de columnas\nLos nombres de las columnas en los conjuntos de datos pueden ser inconsistentes, incluir caracteres especiales o espacios que dificulten el manejo en R.\nPrimero, identificamos los nombres originales del conjunto de datos utilizando la función base names():\n\ncolnames(datos_problema)\n\n[1] \"ID\"                  \"Nombre.PARTICIPANTE\" \"EDAD..años.\"        \n[4] \"Ingreso.Mensual\"     \"género\"             \n\n\nPara resolver esto, utilizamos la función clean_names() del paquete janitor. Esta función transforma automáticamente los nombres de las columnas en un formato limpio y consistente:\n\n# Cargar el paquete janitor\nlibrary(janitor)\n\n# Estandarizar los nombres de las columnas\ndatos_problema = clean_names(datos_problema)\n\nMostramos\n\ncolnames(datos_problema)\n\n[1] \"id\"                  \"nombre_participante\" \"edad_anos\"          \n[4] \"ingreso_mensual\"     \"genero\"             \n\n\nEliminación de duplicados\nEn algunos casos, los datos pueden contener filas duplicadas que distorsionan el análisis. Podemos identificar y eliminar estas filas utilizando la función distinct() de dplyr.\n\n# Cargamos dplyr\nlibrary(dplyr)\n\n# Eliminamos filas duplicadas\ndatos_problema = distinct(datos_problema)\n\nMostramos\n\ndatos_problema\n\n  id nombre_participante edad_anos ingreso_mensual genero\n1  1         Luis Bartra        23          1500.0      F\n2  2        Carlos Gómez        27          2000.0      M\n3  3      Cristina Pérez        23          1500.0      F\n4  4         María López        45          2500.0      F\n5  5       Javier Muller        35          3800.0      M\n6  6          Liz García        29          1800.5      M\n\n\nPara lo siguientes pasos vamos a utilizar como ejemplo el conjunto de datos gapminder. Este conjuto de datos contiene información sobre indicadores socioeconómicos de varios países a lo largo del tiempo. Incluye variables como la esperanza de vida (lifeExp), el PIB per cápita (gdpPercap) y la población a través de los años.\nPuedes descargar el conjunto de datos de datos desde la Datáfora o en el siguiente enlace y guardarla en tu carpeta de trabajo: gapminder.csv.\nDado que el archivo está en formato .csv (valores separados por comas), utilizaremos la función read_csv() del paquete readr, que forma parte del tidyverse, para leer los datos:\n\nlibrary(readr)\n\n# Importamos el archivo CSV\ngapminder = read_csv('gapminder.csv')\n\n# Mostramos las primeras filas del dataset\nhead(gapminder)\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.\n\n\nOtra opción es instalar y cargar el paquete llamado gapminder (Bryan 2023), que contiene el mismo conjunto de datos:\n\n# Cargar el paquete\nlibrary(gapminder)\n\n# Mostramos las primeras filas del dataset\nhead(gapminder)\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integración</span>"
    ]
  },
  {
    "objectID": "integración.html#manipulación",
    "href": "integración.html#manipulación",
    "title": "3  Integración",
    "section": "3.5 Manipulación",
    "text": "3.5 Manipulación\nUna vez limpios nuestros datos pasamos al tercer paso: la manipulación de datos. La manipulación de datos consiste en transformar y preparar los datos para su análisis, lo que puede incluir la creación de nuevas variables, el filtrado de observaciones o la omisión de alguna variable. Para ello, usaremos el paquete dplyr (Wickham et al. 2023) del tidyverse.\ndplyr es un paquete del tidyverse diseñado específicamente para la manipulación de datos. Proporciona un conjunto de funciones que permiten seleccionar, filtrar, ordenar, resumir y transformar datos en data.frames. Una de las características más importantes de dplyr es su uso del “pipe operator” (%&gt;%), que permite encadenar múltiples operaciones de manera secuencial, pasando el resultado de una función directamente como entrada a la siguiente. Esto hace que el código sea más fácil de leer y mantener.\nEl atajo del teclado para el pipe operator (%&gt;%) es:\n\nCtrl + Shift + M (Windows)\nCmd + Shift + M (Mac)\n\nUn sistema de tuberías\nPara poder realizar múltiples acciones en secuencia conectando cada acción con la siguiente a través de “tuberías” debemos utilizar lo que se llama el pipe operator (%&gt;%).\n\nLo iremos viendo, de momento tienes que conocer algunas de las principales funciones de dplyr:\n\nfilter(): Filtra filas de un dataframe según una condición específica.\nselect(): Selecciona columnas específicas de un dataframe.\nmutate(): Crea nuevas columnas o modifica las existentes en un dataframe.\nsummarize(): Resumen estadístico de las columnas de un dataframe.\narrange(): Ordena las filas de un dataframe según una o más variables.\ngroup_by(): Agrupa un dataframe por una o más variables, preparándolo para operaciones de resumen.\n\nPara empezar a trabajar con dplyr recuerda primero cargarlo:\n\n#install.packages('dplyr')\n\n\nlibrary(dplyr)\n\nA continuación, verás que muchas de las funciones terminan con un comando head() , esto es únicamente para evitar mostrar todos los datos del data.frame por motivos puramente estéticos del libro.\n\n3.5.1 Funciones principales\nFilter()\nLa función filter() nos ayuda a filtrar filas bajo una condición.\n\n\n\nElaboración propia\n\n\nUtilizando el pipe operator (%&gt;%) vamos a filtrar nuestros datos únicamente para aquellos que sean del 2007. Para ello nuestra condición sera que la variable year sea igual a 2007.\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;% \n  # Filtramos las filas donde la variable year sea igual a 2007\n  filter(year == 2007) %&gt;% \n  # Mostramos las primeras 5 filas del resultado para fines estéticos\n  head(5)\n\n# A tibble: 5 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       2007    43.8 31889923      975.\n2 Albania     Europe     2007    76.4  3600523     5937.\n3 Algeria     Africa     2007    72.3 33333216     6223.\n4 Angola      Africa     2007    42.7 12420476     4797.\n5 Argentina   Americas   2007    75.3 40301927    12779.\n\n\nPodemos asignarle un propio nombre\n\ngapminder_2007 = gapminder %&gt;%\n  filter(year == 2007)\n\nAhora tenemos otro data.frame filtrado solo por los valores que tenían como año el 2007. Podemos resumirlo de la misma forma.\n\nstr(gapminder_2007)\n\nspc_tbl_ [142 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n $ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n $ year     : num [1:142] 2007 2007 2007 2007 2007 ...\n $ lifeExp  : num [1:142] 43.8 76.4 72.3 42.7 75.3 ...\n $ pop      : num [1:142] 31889923 3600523 33333216 12420476 40301927 ...\n $ gdpPercap: num [1:142] 975 5937 6223 4797 12779 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country = col_character(),\n  ..   continent = col_character(),\n  ..   year = col_double(),\n  ..   lifeExp = col_double(),\n  ..   pop = col_double(),\n  ..   gdpPercap = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nTambién podemos filtrar con más de una condición. Imaginemos que queremos saber los valores del 2002 del Perú\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;% \n  # Filtramos las filas donde year sea igual a 2002\n  filter(year == 2002, \n         # Y donde el país sea igual a 'Peru'\n         country == 'Peru')\n\n# A tibble: 1 × 6\n  country continent  year lifeExp      pop gdpPercap\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Peru    Americas   2002    69.9 26769436     5909.\n\n\nO los países que al año 2007 tenían mas de 100 millones de habitantes.\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;% \n  # Filtramos las filas donde year sea igual a 2007\n  filter(year == 2007, \n         # Y donde la población (pop) sea mayor a 100 millones\n         pop &gt; 100000000)\n\n# A tibble: 10 × 6\n   country       continent  year lifeExp        pop gdpPercap\n   &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bangladesh    Asia       2007    64.1  150448339     1391.\n 2 Brazil        Americas   2007    72.4  190010647     9066.\n 3 China         Asia       2007    73.0 1318683096     4959.\n 4 India         Asia       2007    64.7 1110396331     2452.\n 5 Indonesia     Asia       2007    70.6  223547000     3541.\n 6 Japan         Asia       2007    82.6  127467972    31656.\n 7 Mexico        Americas   2007    76.2  108700891    11978.\n 8 Nigeria       Africa     2007    46.9  135031164     2014.\n 9 Pakistan      Asia       2007    65.5  169270617     2606.\n10 United States Americas   2007    78.2  301139947    42952.\n\n\nSelect()\nUtilizamos la función select() para filtrar por las columnas que son de nuestro interés. Muchas veces vamos a querer seleccionar solo algunas para nuestro análisis.\n\nComo argumentos select() solo necesita el nombre de la columna de interés.\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Seleccionamos solo las columnas country, year y pop\n  select(country, year, pop) %&gt;%\n  head(5)\n\n# A tibble: 5 × 3\n  country      year      pop\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 Afghanistan  1952  8425333\n2 Afghanistan  1957  9240934\n3 Afghanistan  1962 10267083\n4 Afghanistan  1967 11537966\n5 Afghanistan  1972 13079460\n\n\nArrange()\nUtilizamos la función arrange() para poder ordenar las observaciones (filas) que tengamos.\n\nImagina que queremos ordenarlo por quien tenga la menor esperanza de vida.\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Ordenamos las filas por la columna lifeExp en orden ascendente\n  arrange(lifeExp) %&gt;%\n  head(5)\n\n# A tibble: 5 × 6\n  country      continent  year lifeExp     pop gdpPercap\n  &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 Rwanda       Africa     1992    23.6 7290203      737.\n2 Afghanistan  Asia       1952    28.8 8425333      779.\n3 Gambia       Africa     1952    30    284320      485.\n4 Angola       Africa     1952    30.0 4232095     3521.\n5 Sierra Leone Africa     1952    30.3 2143249      880.\n\n\nSi lo queremos de mayor a menor solo debemos que envolverlo en otra función llamada desc()\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Ordenamos las filas por la columna lifeExp en orden descendente\n  arrange(desc(lifeExp)) %&gt;%\n  head(5)\n\n# A tibble: 5 × 6\n  country          continent  year lifeExp       pop gdpPercap\n  &lt;chr&gt;            &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Japan            Asia       2007    82.6 127467972    31656.\n2 Hong Kong, China Asia       2007    82.2   6980412    39725.\n3 Japan            Asia       2002    82   127065841    28605.\n4 Iceland          Europe     2007    81.8    301931    36181.\n5 Switzerland      Europe     2007    81.7   7554661    37506.\n\n\nEsto es fenomenal, pero si tomamos a todos los países en cualquier momento no tenemos una fotografía muy clara de los hechos. Quizás convendría mejor analizar los datos únicamente centrándonos en un año. Podemos filtrar los valores solo para el año 2007 con filter() y luego conectarlo con arrange() y desc() para ordenarlo de mayor a menor. Para concatenar acciones solo debemos conectarlas con el pipe operator (%&gt;%)\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Filtramos las filas donde year sea igual a 2007\n  filter(year == 2007) %&gt;%\n  # Ordenamos las filas por la columna lifeExp en orden descendente\n  arrange(desc(lifeExp)) %&gt;%\n  head(5)\n\n# A tibble: 5 × 6\n  country          continent  year lifeExp       pop gdpPercap\n  &lt;chr&gt;            &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Japan            Asia       2007    82.6 127467972    31656.\n2 Hong Kong, China Asia       2007    82.2   6980412    39725.\n3 Iceland          Europe     2007    81.8    301931    36181.\n4 Switzerland      Europe     2007    81.7   7554661    37506.\n5 Australia        Oceania    2007    81.2  20434176    34435.\n\n\nMutate()\nUsamos la función mutate() para modificar columna existentes o crear nuevas.\n\nCrear una nueva columna con el Producto Interno Bruto (PIB)\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Creamos una nueva columna llamada PIB_Bruto calculando gdpPercap * pop\n  mutate(PIB_Bruto = gdpPercap * pop) %&gt;%\n  head(5)\n\n# A tibble: 5 × 7\n  country     continent  year lifeExp      pop gdpPercap   PIB_Bruto\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779. 6567086330.\n2 Afghanistan Asia       1957    30.3  9240934      821. 7585448670.\n3 Afghanistan Asia       1962    32.0 10267083      853. 8758855797.\n4 Afghanistan Asia       1967    34.0 11537966      836. 9648014150.\n5 Afghanistan Asia       1972    36.1 13079460      740. 9678553274.\n\n\nCrear una nueva columna con el PIB en miles de millones:\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Creamos una nueva columna llamada PBI_Mil_Millones \n  # dividiendo el PIB bruto entre mil millones\n  mutate(PBI_Mil_Millones = \n           (gdpPercap * pop) / 1000000000) %&gt;%\n  head(5)\n\n# A tibble: 5 × 7\n  country     continent  year lifeExp      pop gdpPercap PBI_Mil_Millones\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.             6.57\n2 Afghanistan Asia       1957    30.3  9240934      821.             7.59\n3 Afghanistan Asia       1962    32.0 10267083      853.             8.76\n4 Afghanistan Asia       1967    34.0 11537966      836.             9.65\n5 Afghanistan Asia       1972    36.1 13079460      740.             9.68\n\n\nTambién podemos utilizar mutate() para modificar una variable existente, sobreescribiéndola. Supongamos que queremos aumentar en un 10% el PBI per cápita de todos los países.\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Modificamos la columna gdpPercap aumentando su valor en un 10%\n  mutate(gdpPercap = gdpPercap * 1.1) %&gt;%\n  head(5)\n\n# A tibble: 5 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      857.\n2 Afghanistan Asia       1957    30.3  9240934      903.\n3 Afghanistan Asia       1962    32.0 10267083      938.\n4 Afghanistan Asia       1967    34.0 11537966      920.\n5 Afghanistan Asia       1972    36.1 13079460      814.\n\n\n\nhead(gapminder %&gt;%\n       mutate(PBI_Mil_Millones = (gdpPercap * pop)/1000000000)\n  \n)\n\n# A tibble: 6 × 7\n  country     continent  year lifeExp      pop gdpPercap PBI_Mil_Millones\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.             6.57\n2 Afghanistan Asia       1957    30.3  9240934      821.             7.59\n3 Afghanistan Asia       1962    32.0 10267083      853.             8.76\n4 Afghanistan Asia       1967    34.0 11537966      836.             9.65\n5 Afghanistan Asia       1972    36.1 13079460      740.             9.68\n6 Afghanistan Asia       1977    38.4 14880372      786.            11.7 \n\n\nFinalmente, podemos concatenarlo con el resto de operaciones de la misa forma.\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Seleccionamos solo las columnas de interés\n  select(country, year, pop, gdpPercap) %&gt;%\n  # Filtramos las filas donde year sea igual a 2007\n  filter(year == 2007) %&gt;%\n  # Ordenamos las filas por la columna gdpPercap en orden descendente\n  arrange(desc(gdpPercap)) %&gt;%\n  # Creamos una nueva columna llamada PBI calculando gdpPercap * pop\n  mutate(PBI = (gdpPercap * pop)) %&gt;%\n  # Mostramos las primeras 5 filas para facilitar la visualización\n  head(5)\n\n# A tibble: 5 × 5\n  country        year       pop gdpPercap     PBI\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Norway         2007   4627926    49357. 2.28e11\n2 Kuwait         2007   2505559    47307. 1.19e11\n3 Singapore      2007   4553009    47143. 2.15e11\n4 United States  2007 301139947    42952. 1.29e13\n5 Ireland        2007   4109086    40676. 1.67e11\n\n\nSummarise() / Reframe()\nCuando queremos obtener estadísticas resumidas de una o más variables (por ejemplo, calcular promedios, totales o conteos) usamos funciones como summarise(). Esta nos permite condensar la información y obtener un nuevo dataframe con solo los valores que nos interesan. En versiones recientes de dplyr, te va a aparecer reframe() en su lugar: esto se debe a que el paquete está actualizando algunos nombres para que reflejen mejor lo que hacen. No te preocupes, lo esencial es lo siguiente: usa summarise() si quieres un resumen compacto, y reframe() si necesitas conservar más de una fila por grupo o mayor flexibilidad.\n\nSupongamos que queremos conocer la esperanza de vida media.\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Calculamos la esperanza de vida media\n  summarise(espvida_media = mean(lifeExp))\n\n# A tibble: 1 × 1\n  espvida_media\n          &lt;dbl&gt;\n1          59.5\n\n\nPodemos calcular dos cosas a la vez. La esperanza de vida media y el total de la población\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Calculamos la esperanza de vida media y el total de la población\n  reframe(espvida_media = mean(lifeExp), popTotal = sum(pop))\n\n# A tibble: 1 × 2\n  espvida_media    popTotal\n          &lt;dbl&gt;       &lt;dbl&gt;\n1          59.5 50440465801\n\n\nIntentemos esta vez calcular la esperanza de vida media y el total de la población en América en 2002.\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Filtramos las filas donde year sea igual a 2002\n  # y el continente sea \"Americas\"\n  filter(year == 2002, continent == \"Americas\") %&gt;%\n  # Calculamos la esperanza de vida media y el total de la población\n  reframe(espvida_media = mean(lifeExp), popTotal = sum(pop))\n\n# A tibble: 1 × 2\n  espvida_media  popTotal\n          &lt;dbl&gt;     &lt;dbl&gt;\n1          72.4 849772762\n\n\nSin embargo, el verdadero poder de reframe() es cuando lo combinamos con la siguiente función.\nGroup_by()\nSupongamos que queremos calcular la esperanza de vida y el total de la población de todos los países cada año. Para ello podríamos realizar el siguiente código\n\n# Empezando por el primer año...\ngapminder %&gt;%\n  filter(year == 1952) %&gt;%\n  summarize(espvida_media = mean(lifeExp), popTotal = sum(pop))\n\n# A tibble: 1 × 2\n  espvida_media   popTotal\n          &lt;dbl&gt;      &lt;dbl&gt;\n1          49.1 2406957150\n\n\nLo tenemos para un año, pero si quisiéramos aplicarlo para cada año esto sería bastante tedioso y quereriría muchas líneas de código, sin contar con que el resultado estará en dataframes separados. Para esto la función group_by() entra en acción.\nLa función group_by le dice a dplyr que realice las operaciones por grupos en vez de aplicarlas a todo el datset a la vez.\n\nPara ello simplemente tenemos que especificar la variable a agrupar, en este caso, los años.\n\n# Tomamos el conjunto de datos gapminder\ngapminder %&gt;%\n  # Agrupamos los datos por la variable year\n  group_by(year) %&gt;%\n  # Calculamos la esperanza de vida media y el total de la población por año\n  summarise(espvida_media = mean(lifeExp), popTotal = sum(pop))\n\n# A tibble: 12 × 3\n    year espvida_media   popTotal\n   &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1  1952          49.1 2406957150\n 2  1957          51.5 2664404580\n 3  1962          53.6 2899782974\n 4  1967          55.7 3217478384\n 5  1972          57.6 3576977158\n 6  1977          59.6 3930045807\n 7  1982          61.5 4289436840\n 8  1987          63.2 4691477418\n 9  1992          64.2 5110710260\n10  1997          65.0 5515204472\n11  2002          65.7 5886977579\n12  2007          67.0 6251013179\n\n\nY listo! El nuevo data.frame ahora muestra los datos para cada año en que se tomaron los datos.\n\n\n3.5.2 Dplyr en acción\nAhora que hemos aprendido las funciones básicas de dplyr, vamos a ponerlas en práctica con ejemplos concretos.\nCalcular la esperanza de vida media y el total de población de cada continente en el 2007, ordenado por mayor esperanza de vida\nPodemos filtrar los datos para el año 2007, agruparlos por continente, calcular la esperanza de vida media y el total de población, y luego ordenar los resultados por esperanza de vida media en orden descendente.\n\ngapminder %&gt;%\n# Filtramos los datos para el año 2007\n  filter(year == 2007) %&gt;%  \n# Agrupamos los datos por continente\n  group_by(continent) %&gt;% \n# Calculamos la esperanza de vida media y el total de población\n  summarise(espvida_media = mean(lifeExp), popTotal = sum(pop)) %&gt;%  \n# Ordenamos los resultados por esperanza de vida media en orden descendente\n  arrange(desc(espvida_media))  \n\n# A tibble: 5 × 3\n  continent espvida_media   popTotal\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;\n1 Oceania            80.7   24549947\n2 Europe             77.6  586098529\n3 Americas           73.6  898871184\n4 Asia               70.7 3811953827\n5 Africa             54.8  929539692\n\n\nCrear una nueva columna con el PIB total (PIB per cápita * población) y filtrar los países con un PIB total mayor a 1 billón en 2007\nVamos a crear una nueva columna para el PIB total, filtrar los datos para el año 2007, y luego seleccionar solo los países con un PIB total mayor a 1 billón.\n\ngapminder %&gt;%\n# Creamos una nueva columna para el PIB total\n  mutate(PIB_Total = gdpPercap * pop) %&gt;% \n# Filtramos los datos para el año 2007 y PIB total mayor a 1 billón\n  filter(year == 2007, PIB_Total &gt; 1e12) %&gt;% \n# Seleccionamos las columnas de interés\n  select(country, continent, PIB_Total) %&gt;%  \n# Ordenamos los resultados por PIB total en orden descendente\n  arrange(desc(PIB_Total))  \n\n# A tibble: 13 × 3\n   country        continent PIB_Total\n   &lt;chr&gt;          &lt;chr&gt;         &lt;dbl&gt;\n 1 United States  Americas    1.29e13\n 2 China          Asia        6.54e12\n 3 Japan          Asia        4.04e12\n 4 India          Asia        2.72e12\n 5 Germany        Europe      2.65e12\n 6 United Kingdom Europe      2.02e12\n 7 France         Europe      1.86e12\n 8 Brazil         Americas    1.72e12\n 9 Italy          Europe      1.66e12\n10 Mexico         Americas    1.30e12\n11 Canada         Americas    1.21e12\n12 Spain          Europe      1.17e12\n13 Korea, Rep.    Asia        1.15e12\n\n\nCalcular la esperanza de vida media y el total de población de cada continente a lo largo del tiempo\nEn este ejemplo, queremos ver cómo ha cambiado la esperanza de vida media y la población total en cada continente a lo largo del tiempo.\n\n# Creamos 'x'\nx = gapminder %&gt;% \n# Agrupamos los datos por continente y año\n  group_by(continent, year) %&gt;% \n# Calculamos la esperanza de vida media y el total de población\n  summarise(espvida_media = mean(lifeExp), popTotal = sum(pop)) %&gt;% \n# Ordenamos los resultados por continente y año\n  arrange(continent, year)  \n# Seleccionamos únicamente los primeros 12 (África) por motivos estéticos\nhead(x, 12) \n\n# A tibble: 12 × 4\n# Groups:   continent [1]\n   continent  year espvida_media  popTotal\n   &lt;chr&gt;     &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1 Africa     1952          39.1 237640501\n 2 Africa     1957          41.3 264837738\n 3 Africa     1962          43.3 296516865\n 4 Africa     1967          45.3 335289489\n 5 Africa     1972          47.5 379879541\n 6 Africa     1977          49.6 433061021\n 7 Africa     1982          51.6 499348587\n 8 Africa     1987          53.3 574834110\n 9 Africa     1992          53.6 659081517\n10 Africa     1997          53.6 743832984\n11 Africa     2002          53.3 833723916\n12 Africa     2007          54.8 929539692\n\n\nComparar la esperanza de vida y el PIB per cápita entre dos países específicos (Perú y Chile) en 2007\nVamos a filtrar los datos para los países Perú y Chile en el año 2007, y comparar la esperanza de vida y el PIB per cápita entre ellos.\n\ngapminder %&gt;%\n# Filtramos los datos para Perú y Chile en el año 2007\n  filter(year == 2007, country %in% c(\"Peru\", \"Chile\")) %&gt;%\n# Seleccionamos las columnas de interés\n  select(country, lifeExp, gdpPercap) %&gt;%  \n# Ordenamos los resultados por país\n  arrange(country)  \n\n# A tibble: 2 × 3\n  country lifeExp gdpPercap\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Chile      78.6    13172.\n2 Peru       71.4     7409.\n\n\nCrear una nueva columna con la relación PIB per cápita y esperanza de vida, y analizar esta relación para los países en Asia en 2007\nPodemos crear una nueva columna para la relación entre PIB per cápita y esperanza de vida, y luego analizamos esta relación para los países en Asia en 2007.\n\nx = gapminder %&gt;%\n# Filtramos los datos para los países en Asia en 2007\n  filter(year == 2007, continent == \"Asia\") %&gt;%\n# Creamos una nueva columna para la relación PIB per cápita / esperanza de vida\n  mutate(PIB_vs_vida = gdpPercap / lifeExp) %&gt;% \n# Seleccionamos las columnas de interés\n  select(country, PIB_vs_vida) %&gt;% \n# Ordenamos los resultados por la relación PIB per cápita / esperanza de vida en orden descendente\n  arrange(desc(PIB_vs_vida))  \n\nhead(x, 10)\n\n# A tibble: 10 × 2\n   country          PIB_vs_vida\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 Kuwait                  610.\n 2 Singapore               589.\n 3 Hong Kong, China        483.\n 4 Bahrain                 394.\n 5 Japan                   383.\n 6 Taiwan                  366.\n 7 Israel                  316.\n 8 Saudi Arabia            298.\n 9 Korea, Rep.             297.\n10 Oman                    295.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integración</span>"
    ]
  },
  {
    "objectID": "integración.html#visualización",
    "href": "integración.html#visualización",
    "title": "3  Integración",
    "section": "3.6 Visualización",
    "text": "3.6 Visualización\nLa última etapa del flujo de trabajo es la visualización de datos. Poder visualizar la información no solo nos ayuda a a anticipar ciertos patrones y comportamientos, sino que también facilita la comunicación de estos hallazgos a otros. Es importante reconocer que tendemos a dejarnos llevar más por los gráficos que por los resúmenes numéricos, aunque ambos sean igualmente importantes.\nggplot2 es uno de los paquetes más populares y poderosos en R para la creación de gráficos. Para generar gráficos adecuados en R, necesitamos considerar dos elementos fundamentales: el elemento estadístico y el elemento de diseño. La finalidad de un gráfico es resumir y transmitir información de manera adecuada para el análisis que estamos realizando, sin olvidar la importancia de un diseño atractivo. ggplot2 (Wickham 2016) consigue ambos objetivos.\nAntes de empezar a usar ggplot2, necesitamos instalarlo y cargarlo. de R.\n\n# Instalación del paquete ggplot2 (en caso no esté instalado)\n#install.packages('ggplot2')\n\n# Carga del paquete ggplot2\nlibrary(ggplot2)\n\n\n3.6.1 Fundamentos de ggplot2\nLa creación de gráficos con ggplot2 se basa en la gramática de los gráficos (Grammar of Graphics). Esta gramática define una serie de componentes que forman parte de un gráfico:\n\nDatos: El conjunto de datos que queremos visualizar.\nEstética: La asignación de variables a aspectos visuales del gráfico, como posición, color, tamaño, etc.\nGeometría: El tipo de gráfico (barras, puntos, líneas, etc.).\nFacetas: Subdivisiones del gráfico en múltiples paneles basados en una o más variables.\nEstadísticas: Transformaciones estadísticas que se aplican a los datos antes de graficarlos.\nCoordenadas: El sistema de coordenadas usado (cartesiano, polar, etc.).\nTema: Elementos de diseño y formato del gráfico.\n\nPiensa en cada uno de ellos como una capa que va encima. Para construir un gráfico en ggplot2, concatenamos diferentes capas usando el operador +. Cada capa añade un nuevo componente al gráfico.\n\n\n\nExtraído de: https://cristoichkov.github.io\n\n\nVamos a empezar con un gráfico de dispersión básico utilizando el conjunto de datos gapminder. Este conjunto contiene datos sobre el Producto Interno Bruto (PIB) per cápita, la esperanza de vida y otros indicadores para varios países a lo largo del tiempo.\nEmpezamos agregando la primera capa, los datos. Para comenzar, creamos la base del gráfico con la función ggplot(), y como argumento el conjunto de datos gapminder.\n\nggplot(gapminder)\n\n\n\n\n\n\n\n\nComo ves, los datos por si solos únicamente nos dan un recuadro blanco, debemos añadir el resto de capas. Definimos las estéticas del gráfico con la función aes(), asignando el PIB per cápita al eje x (gdpPercap) y la esperanza de vida al eje y (lifeExp).\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp))\n\n\n\n\n\n\n\n\nAhora que R sabe qué datos vamos a utilizar y qué variables tomar como coordenadas, necesitamos especificar qué tipo de gráfico queremos crear. En ggplot2, hay muchas geoms (geometrías), y cada una proporciona un tipo de gráfico diferente. Por ejemplo, geom_bar crea un gráfico de barras y geom_line crea un gráfico de líneas. En este caso, como ambas variables (PIB per cápita y esperanza de vida) son numéricas, usaremos un gráfico de dispersión (puntos) para visualizar la relación entre ellas. Para ello, usamos geom_point().\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point()\n\n\n\n\n\n\n\n\nEn este ejemplo, aes(x = gdpPercap, y = lifeExp) define la estética del gráfico, asignando el Producto Interno Bruto per cápita al eje x y la esperanza de vida al eje y. geom_point() especifica que queremos un gráfico de dispersión. Ya puedes lanzarte a sacar tus primeras conclusiones.\nAlgo que podemos notar es que el gráfico se ve un poco apretado al principio. Para mejorar la visualización, podemos usar una escala logarítmica en el eje x. Esto transforma los datos para que las diferencias entre los puntos se aprecien mejor, especialmente cuando algunos países tienen un PIB per cápita muy alto y otros muy bajo. Solo recuerda que usar una escala logarítmica cambia la manera en que interpretamos el gráfico, así que debemos tenerlo en cuenta.\nUsamos scale_x_log10() para transformar el eje x a una escala logarítmica.\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nEn este caso, hemos añadido scale_x_log10() para aplicar la escala logarítmica en el eje x. Esto clarifica la relación entre las variables, permitiéndonos observar que un mayor PIB per cápita generalmente se asocia con una mayor esperanza de vida.\nTambién podemos añadir otros elementos estéticos para mejorar la visualización, como el color de los puntos basado en el continente. Para ello, modificamos aes() para incluir color = continent, lo que añade color a los puntos según el continente. Recuerda que continent es una variable categórica del conjunto de datos.\n\nggplot(gapminder, aes(x = gdpPercap, \n                      y = lifeExp, \n                      color = continent)) +\n  geom_point() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nTe animo a que sigas intentando identificar patrones importantes. Por ejemplo, ¿dónde se encuentran los puntos asociados a determinados continentes? ¿Existe alguna relación entre el continente y las otras dos variables?\nEste gráfico ya es de por si sumamente útil pero si deseamos poder comunicarlo cada uno de los elementos deben estar debidamente nombrados. Para ello, es importante añadir etiquetas y títulos para mejorar la interpretación del gráfico. Esto lo podemos hacer agregando una capa más con la función labs() y usando como argumentos title, x, y y color.\nUsamos labs() para añadir un título y etiquetas a los ejes.\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Relación entre PIB per cápita y Esperanza de Vida\",\n          x = \"PIB per cápita (log10)\",\n          y = \"Esperanza de Vida\",\n          color = \"Continente\")\n\n\n\n\n\n\n\n\nOtra capa opcional que también podemos agregar es el facet. El facetado permite dividir el gráfico en múltiples paneles basados en una o más variables. Por ejemplo, podemos crear un gráfico separado para cada continente.\nUsamos facet_wrap(~ continent) para dividir el gráfico en múltiples paneles, uno para cada continente.\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Relación entre PIB per cápita y Esperanza de Vida\",\n      x = \"PIB per cápita (log10)\",\n      y = \"Esperanza de Vida\",\n      color = \"Continente\") +\n  facet_wrap(~ continent) +\n# Puedes quitar la leyenda con esta capa adicional\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nTenemos la última capa, los themes. La capa de themes tiene una multitud de funciones que permiten modificar de forma precisa el tamaño, posición o forma de casi todos los elementos del gráfico. Por ejemplo, en el gráfico anterior eliminamos la leyenda usando un theme. Los themes permiten personalizar la apariencia del gráfico de manera detallada.Esta guía no tiene como objetivo explorar la complejidad de los themes en profundidad, así que nos centraremos en los themes predeterminados.\n¿Recuerdas el gráfico que generamos antes de los facets? Vamos a volver a utilizarlo para ejemplificar los diversos temas en ggplot.\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Relación entre PIB per cápita y Esperanza de Vida\",\n          x = \"PIB per cápita (log10)\",\n          y = \"Esperanza de Vida\",\n          color = \"Continente\")\n\n\n\n\n\n\n\n\nPero no tenemos que volver a escribirlo. Verás, los gráficos también son un tipo de objeto en R así que puedes ser almacenarlos en forma de una variable. De esta forma:\n\ngrafico = ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Relación entre PIB per cápita y Esperanza de Vida\",\n          x = \"PIB per cápita (log10)\",\n          y = \"Esperanza de Vida\",\n          color = \"Continente\")\n\nAhora solo basta con llamarlo por su recien adquirido nombre\n\ngrafico\n\n\n\n\n\n\n\n\nComo decíamos, existen varios themes predeterminados en ggplot2. Los themes en ggplot2 permiten personalizar la apariencia de los gráficos de manera detallada. Existen varios themes predeterminados que se pueden usar para cambiar el aspecto de los gráficos sin necesidad de realizar configuraciones complejas.\nYa que nombramos al objeto gráfico, podemos añadir una capa fácilmente con el signo +\nEl theme_minimal es un tema limpio y sencillo, con un fondo blanco y líneas de cuadrícula ligeras.\n\ngrafico + theme_minimal()\n\n\n\n\n\n\n\n\nEl theme_gray es el tema predeterminado en ggplot2, con un fondo gris claro y líneas de cuadrícula blancas.\n\ngrafico + theme_gray()\n\n\n\n\n\n\n\n\nEl theme_bw es similar al theme_gray, pero utiliza un fondo blanco y es útil para presentaciones en blanco y negro.\n\ngrafico + theme_bw()\n\n\n\n\n\n\n\n\nEl theme_classic se asemeja los gráficos clásicos de R base, con un fondo blanco y ejes negros.\n\ngrafico + theme_classic()\n\n\n\n\n\n\n\n\nEl theme_void elimina todos los elementos no esenciales del gráfico, dejando solo los datos. Es útil para gráficos personalizados donde se desea agregar y tener control de cada cosa de forma manual.\n\ngrafico + theme_void()\n\n\n\n\n\n\n\n\nEl theme_dark utiliza un fondo oscuro con líneas de cuadrícula claras.\n\ngrafico + theme_dark()\n\n\n\n\n\n\n\n\nCada uno de estos themes puede ser modificado y combinado con otros elementos para crear gráficos personalizados según las necesidades específicas del análisis. En particular, utilizaré el theme_minimal por preferencia personal. Pero recuerdas que puedes utilizar el que se adapte mejor a tus necesidades.\n\n\n3.6.2 Tipos de gráficos\nAsí como existen diferentes tipos de variables, también hay diversos gráficos que se pueden utilizar para representarlas adecuadamente. A continuación, se presentan algunos de los gráficos más importantes y que se utilizarán en los próximos capítulos.\nGráfico de barras\nUn gráfico de barras es útil para visualizar la frecuencia de categorías o la comparación de cantidades entre diferentes categorías.\nPara crear un gráfico de barras en R, podemos usar la función geom_bar(). Si solo queremos contar las observaciones en una sola variable, podemos usar geom_bar() sin especificar ambos ejes:\n\n# Cuenta simple\nggplot(gapminder, aes(x = continent, fill = continent)) +\n  geom_bar() +\n  labs(\n    title = \"Frecuencia de países por continente\",\n    x = \"Continente\",\n    y = \"Frecuencia\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEn este caso, el argumento fill se utiliza para rellenar las barras con colores diferentes según la categoría (en este caso, el continente). La diferencia entre fill y color es que fill se utiliza para rellenar el interior de las formas (como barras o áreas), mientras que color se usa para definir el color de los bordes de las formas.\nPero cuando queremos especificar ambos ejes, como cuando estamos comparando cantidades específicas entre categorías, usamos geom_col y especificamos ambos ejes en aes:\nPor ejemplo, vamos a contar el número de países en cada continente que tienen una esperanza de vida mayor a 75 años en el año 2007.\nPrimero preparamos los datos con dplyr, filtrando los datos para el año 2007 y luego contando los países por continente con una esperanza de vida mayor a 75 años.\n\ndatos &lt;- gapminder %&gt;%\n  filter(year == 2007, lifeExp &gt; 75) %&gt;%\n  group_by(continent) %&gt;%\n  summarise(\n    n = n_distinct(country)  # Contar el número de países únicos\n  )\n\nAquí tenemos los datos preparados:\n\ndatos\n\n# A tibble: 5 × 2\n  continent     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Africa        1\n2 Americas     10\n3 Asia          9\n4 Europe       22\n5 Oceania       2\n\n\nLuego, creamos el gráfico de barras especificando ambas coordenadas con geom_col():\n\n# Podemos usar la función reorder() para ordenar de mayor a menor en el gráfico\nggplot(datos, aes(x = reorder(continent, n), y = n, fill = continent)) +\n  geom_col() +\n  labs(title = \"Países con esperanza de vida mayor a 75 años en 2007\",\n       x = \"Continente\",\n       y = \"Número de Países\",\n       fill = \"Continente\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nPodemos invertir fácilmente el gráfico usando coord_flip():\n\nggplot(datos, aes(x = reorder(continent, n), y = n, fill = continent)) +\n  geom_col() +\n  labs(title = \"Países con esperanza de vida mayor a 75 años en 2007\",\n       x = \"Continente\",\n       y = \"Número de Países\",\n       fill = \"Continente\") +\n  theme_minimal() +\n  coord_flip()\n\n\n\n\n\n\n\n\nHistograma\nUn histograma es útil para visualizar la distribución de una variable numérica. Muestra la frecuencia de valores en intervalos específicos.\nPara crear un histograma en R, usamos la función geom_histogram():\n\nggplot(gapminder, aes(x = lifeExp)) +\n  geom_histogram(binwidth = 5, fill = \"blue\", color = \"black\") +\n  labs(title = \"Distribución de la esperanza de vida\",\n       x = \"Esperanza de vida\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nGráfico de líneas\nUn gráfico de líneas es útil para visualizar la tendencia de una variable a lo largo del tiempo.\nPara preparar los datos, calculamos la esperanza de vida media por año:\n\ngapminder %&gt;%\n  group_by(year) %&gt;%\n  summarise(lifeExp_mean = mean(lifeExp))\n\n# A tibble: 12 × 2\n    year lifeExp_mean\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1  1952         49.1\n 2  1957         51.5\n 3  1962         53.6\n 4  1967         55.7\n 5  1972         57.6\n 6  1977         59.6\n 7  1982         61.5\n 8  1987         63.2\n 9  1992         64.2\n10  1997         65.0\n11  2002         65.7\n12  2007         67.0\n\n\nOtra ventaja de usar dplyr y ggplot juntos es que podemos unir ambos códigos con un pipe operator. Usamos geom_line(color = \"blue\") para crear el gráfico de líneas:\n\ngapminder %&gt;%\n  group_by(year) %&gt;%\n  summarise(lifeExp_mean = mean(lifeExp)) %&gt;%\n  ggplot(aes(x = year, y = lifeExp_mean)) +\n      geom_line(color = \"blue\") +\n  labs(title = \"Tendencia de la esperanza de vida media a lo largo del tiempo\",\n       x = \"Año\",\n       y = \"Esperanza de vida media\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBoxplot\nUn boxplot es útil para visualizar la distribución de una variable y comparar distribuciones entre diferentes categorías. Hablaremos de él de forma mas rigurosa en el siguiente capítulo\nPara crear el boxplot de la esperanza de vida por continente, usamos geom_boxplot():\n\nggplot(gapminder, aes(x = continent, y = lifeExp, fill = continent)) +\n  geom_boxplot() +\n  labs(title = \"Distribución de la esperanza de vida por continente\",\n       x = \"Continente\",\n       y = \"Esperanza de vida\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nEstos son algunos de los tipos de gráficos que utilizaremos para analizar datos en los próximos capítulos. Cada uno tiene su utilidad específica y pueden añadirse más detalles en función del objetivo que se tenga para la investigación.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integración</span>"
    ]
  },
  {
    "objectID": "integración.html#flor-y-el-análisis-del-bienestar-social",
    "href": "integración.html#flor-y-el-análisis-del-bienestar-social",
    "title": "3  Integración",
    "section": "3.7 Flor y el análisis del bienestar social",
    "text": "3.7 Flor y el análisis del bienestar social\nFlor encontró un conjunto de datos abierto sobre indicadores de bienestar social, que incluía información reciente sobre ingresos, niveles educativos y acceso a servicios básicos en diversas regiones.\nFlor descargó un archivo CSV y lo IMPORTÓ a R.\n\nlibrary(readr)\nbienestar = read_csv(\"datos_bienestar.csv\")\n\nAnaliza la estructura de los datos\n\nglimpse(bienestar)\n\nRows: 1,020\nColumns: 4\n$ Region               &lt;chr&gt; \"Sur\", \"Oeste\", \"Sur\", \"Este\", \"Este\", \"Norte\", \"…\n$ `Ingreso Mensual`    &lt;dbl&gt; 2018, 1705, 3321, 3101, 1293, 2424, 1783, 843, 26…\n$ `Nivel Educativo`    &lt;chr&gt; \"Secundaria\", \"Superior\", \"Secundaria\", \"Primaria…\n$ `Acceso a Servicios` &lt;dbl&gt; 73.6, 77.2, 99.5, 95.1, 62.3, 86.9, 79.1, 58.1, 9…\n\n\nAl explorar los datos, encontró variables como:\n\nregion: La región de cada observación. - ingreso_mensual: El ingreso promedio mensual de cada hogar.\nnivel_educativo: Nivel educativo mas alto alcanzado por la mayoría (Primaria, Secundaria, Superior).\nacceso_servicios: Porcentaje de hogares con acceso a servicios básicos (agua y electricidad).\n\nPrimero, los nombres de las columnas no eran uniformes y contenían espacios que complicaban su manipulación. Decidió solucionar estos problemas antes de continuar.\n\nlibrary(janitor)\n\nbienestar = clean_names(bienestar)\n\ncolnames(bienestar)\n\n[1] \"region\"             \"ingreso_mensual\"    \"nivel_educativo\"   \n[4] \"acceso_a_servicios\"\n\n\nAdemás, al revisar el conjunto de datos, Flor notó algunos VALORES FALTANTES en ingreso_mensual. Se dispone a comprobar.\n\nsum(is.na(bienestar$ingreso_mensual)) \n\n[1] 51\n\n\nTenemos 51 valores perdidos. En este caso en particular, tomó la decisión de eliminarlos.\n\nlibrary(tidyr)\nbienestar = drop_na(bienestar)\n\nFlor decidió explorar las diferencias en el ingreso promedio por región y cómo estas se relacionaban con el nivel educativo y el acceso a servicios básicos. Para ello tuvo que MANIPULAR los datos usando dplyr.\n\nlibrary(dplyr)\n\nCalculó el ingreso promedio por región\n\n# Agrupa por región y calcula el ingreso promedio\ningreso_por_region = bienestar %&gt;%\n  group_by(region) %&gt;%\n  summarise(ingreso_promedio = mean(ingreso_mensual))\n\ningreso_por_region\n\n# A tibble: 5 × 2\n  region ingreso_promedio\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Centro            2537.\n2 Este              2408.\n3 Norte             2524.\n4 Oeste             2538.\n5 Sur               2513.\n\n\nFlor notó que algunas regiones tenían ingresos más altos. Esto la llevó a preguntarse si el acceso a servicios básicos podría explicar parte de estas diferencias.Flor creó una tabla que mostraba el ingreso promedio y el acceso promedio a servicios básicos por región.\n\n# Agrupam por región y calcula el ingreso promedio y acceso promedio\nbienestar_region = bienestar %&gt;%\n  group_by(region) %&gt;%\n  summarise(\n    ingreso_promedio = mean(ingreso_mensual),\n    acceso_servicios_promedio = mean(acceso_a_servicios)\n  )\n\nbienestar_region\n\n# A tibble: 5 × 3\n  region ingreso_promedio acceso_servicios_promedio\n  &lt;chr&gt;             &lt;dbl&gt;                     &lt;dbl&gt;\n1 Centro            2537.                      87.5\n2 Este              2408.                      85.8\n3 Norte             2524.                      87.9\n4 Oeste             2538.                      88.2\n5 Sur               2513.                      87.3\n\n\nUna alternativa que planteó para simplificar la comparación fue clasificar los ingresos en “Altos” y “Bajos” según el promedio nacional. Para ello, integró la función ifelse() a su estructura de dplyr. ifelse() está estructurada para evaluar condiciones lógicas. En este caso, se escribe como ifelse(ingreso_mensual &gt; promedio_nacional, \"Alto\", \"Bajo\"), donde la condición (ingreso_mensual &gt; promedio_nacional) se evalúa para cada fila del vector. Si es verdadera (TRUE), se asigna el valor \"Alto\", y si es falsa (FALSE), se asigna \"Bajo\".\n\n# Calcula el ingreso promedio nacional\npromedio_nacional = mean(bienestar$ingreso_mensual)\n\npromedio_nacional\n\n[1] 2508.653\n\n\n\n# Crea una nueva columna con la clasificación de ingresos\nbienestar = bienestar %&gt;%\n  mutate(\n    categoria_ingreso = \n      ifelse(ingreso_mensual &gt; promedio_nacional, # Condición\n             \"Alto\",                              # Si la condición es verdadera\n             \"Bajo\")                              # Si la condición es falsa\n  )\n\nhead(bienestar, 10)\n\n# A tibble: 10 × 5\n   region ingreso_mensual nivel_educativo acceso_a_servicios categoria_ingreso\n   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt;            \n 1 Sur               2018 Secundaria                    73.6 Bajo             \n 2 Oeste             1705 Superior                      77.2 Bajo             \n 3 Sur               3321 Secundaria                    99.5 Alto             \n 4 Este              3101 Primaria                      95.1 Alto             \n 5 Este              1293 Secundaria                    62.3 Bajo             \n 6 Norte             2424 Primaria                      86.9 Bajo             \n 7 Centro            1783 Primaria                      79.1 Bajo             \n 8 Este               843 Primaria                      58.1 Bajo             \n 9 Centro            2620 Superior                      97.4 Alto             \n10 Sur               2437 Superior                      91.6 Bajo             \n\n\n\n\n\nManipular datos es parte esencial de un análisis bien estrcuturado\n\n\nFlor era consciente de que podía interpretar mejor sus resultados visualmente asi que decidió usar ggplot2 para crear VISUALIZACIONES.\n\nlibrary(ggplot2)\n\nQuería mostrar cómo variaba el ingreso promedio entre regiones. Usó geom_col() para crear un gráfico de barras.\n\n# Gráfico de barras del ingreso promedio por región\nggplot(ingreso_por_region, aes(x = reorder(region, ingreso_promedio), \n                               y = ingreso_promedio, \n                               fill = region)) +\n  geom_col() +\n  labs(\n    title = \"Ingreso Promedio por Región\",\n    x = \"Región\",\n    y = \"Ingreso Promedio\"\n  ) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLuego, quería explorar la relación entre el ingreso y el acceso a servicios básicos.\n\n# Gráfico de dispersión por localidad\nggplot(bienestar, aes(x = acceso_a_servicios, \n                      y = ingreso_mensual, \n                      color = region)) +\n # Le puso puntos con transparencia para mejor visualización\n   geom_point(size = 3, alpha = 0.7) +  \n  labs(\n    title = \"Relación entre Acceso a Servicios Básicos e Ingreso por Localidad\",\n    x = \"Acceso a Servicios Básicos (%)\",\n    y = \"Ingreso Mensual\",\n    color = \"Región\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFinalmente, Flor decidió analizar cómo variaba el ingreso por nivel educativo, usando facetas para separar cada región.\n\n# Gráfico de facetas\nggplot(bienestar, aes(x = nivel_educativo, \n                      y = ingreso_mensual, \n                      fill = nivel_educativo)) +\n  geom_boxplot() +\n  facet_wrap(~ region) +\n  labs(\n    title = \"Ingreso Mensual por Nivel Educativo y Región\",\n    x = \"Nivel Educativo\",\n    y = \"Ingreso Mensual\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nLos gráficos nos ayudan a representar el comportamiento de nuestras variables y la relación entre ellas",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integración</span>"
    ]
  },
  {
    "objectID": "integración.html#resumen-del-capítulo",
    "href": "integración.html#resumen-del-capítulo",
    "title": "3  Integración",
    "section": "3.8 Resumen del capítulo",
    "text": "3.8 Resumen del capítulo\nEl análisis de datos en R se estructura como un flujo lógico que empieza con la importación de datos, continúa con su limpieza, exploración y transformación, y finaliza con visualizaciones claras. En este libro el flujo lo hemos ejecutado principalmente con herramientas del tidyverse, una colección de paquetes diseñados para ser compatibles entre sí y mantener una sintaxis consistente.\nLos datos se trabajan sobre estructuras tabulares, como data.frame y tibble, que permiten manejar variables de distintos tipos. Para importar datos se utilizan funciones como read_csv() para archivos CSV, read_excel() para Excel y read_dta() o read_sav() para formatos de Stata y SPSS.\nLa limpieza de datos es crítica. Se identifican y eliminan valores faltantes con is.na() y drop_na(), se estandarizan nombres con clean_names() del paquete janitor, se eliminan duplicados con distinct() y se corrigen tipos de datos con as.numeric() o funciones equivalentes. La inspección estructural se apoya en funciones como str(), glimpse(), summary(), head() y dim().\nLa transformación y manipulación se organiza mediante dplyr. Con filter() se filtran filas, select() permite aislar columnas, mutate() crea o modifica variables, arrange() ordena observaciones, group_by() agrupa por categorías, y summarise() / reframe() extrae estadísticas resumidas. El uso del operador %&gt;% permite encadenar acciones una tras de otra.\nPara visualizar se emplea ggplot2, que construye gráficos en capas a partir de una gramática declarativa. Se definen las estéticas con aes(), se elige la geometría adecuada (geom_point(), geom_col(), geom_histogram(), geom_boxplot()…), se pueden usar facetas con facet_wrap(), transformar escalas como con scale_x_log10() y ajustar diseño con theme_minimal() u otros temas prediseñados. Las visualizaciones se optimizan añadiendo títulos y etiquetas con labs() y organizando el gráfico en objetos que pueden modificarse sucesivamente.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integración</span>"
    ]
  },
  {
    "objectID": "integración.html#ejercicios",
    "href": "integración.html#ejercicios",
    "title": "3  Integración",
    "section": "3.9 Ejercicios",
    "text": "3.9 Ejercicios\n1. Tienes un archivo llamado datos_bienestar.csv. ¿Cuál es la forma correcta de cargarlo en R usando el paquete readr?\n\nread.csv(\"datos_bienestar.csv\")\nread_csv(\"datos_bienestar.csv\")\nlibrary(readr) seguido de read_csv(\"datos_bienestar.csv\")\nlibrary(dplyr) seguido de read_csv(\"datos_bienestar.csv\")\n\n2. Después de importar los datos, Flor notó que los nombres de las columnas contenían espacios y caracteres especiales. ¿Qué paquete y función usó para estandarizar los nombres de las columnas?\n\ntidyr y drop_na()\njanitor y clean_names()\ndplyr y mutate()\ntibble y glimpse()\n\n3. ¿Cómo puedes contar cuántos valores faltantes (NA) hay en la columna ingreso_mensual de un dataframe llamado bienestar?\n\nsum(bienestar$ingreso_mensual == NA)\nis.na(bienestar$ingreso_mensual)\nsum(is.na(bienestar$ingreso_mensual))\nmeasure(is.na(bienestar$ingreso_mensual))\n\n4. Flor quiere trabajar únicamente con las filas donde la región sea “Norte”. ¿Qué código es correcto para este propósito?\n\nfilter(bienestar$region == \"Norte\")\nbienestar %&gt;% filter(region == \"Norte\")\nbienestar %&gt;% select(region == \"Norte\")\nbienestar %&gt;% filter(region = \"Norte\")\n\n5. Flor quiere crear una nueva columna llamada acceso_clasificado que clasifique el acceso a servicios básicos en “Alto” si es mayor al 70% y “Bajo” si no. ¿Qué código es correcto?\n\nmutate(acceso_clasificado = ifelse(acceso_servicios &gt; 70, \"Alto\", \"Bajo\"))\nmutate(acceso_servicios = ifelse(acceso_servicios &gt; 70, \"Alto\", \"Bajo\"))\nbienestar %&gt;% mutate(acceso_clasificado = ifelse(acceso_servicios &gt; 70, \"Alto\", \"Bajo\"))\nbienestar &lt;- mutate(acceso_clasificado = ifelse(acceso_servicios &gt; 70, \"Alto\", \"Bajo\"))\n\n6. Flor quiere calcular el ingreso promedio por región. ¿Qué código es correcto?\n\nbienestar %&gt;% summarize(region, mean(ingreso_mensual))\ngroup_by(bienestar$region) %&gt;% summarize(mean(ingreso_mensual))\nbienestar %&gt;% group_by(region) %&gt;% summarize(ingreso_promedio = mean(ingreso_mensual))\ngroup_by(bienestar) %&gt;% summarize(region, mean(ingreso_mensual))\n\n7. Flor desea visualizar el ingreso promedio por región en un gráfico de barras. ¿Qué código es correcto?\n\n\n\nggplot(bienestar, aes(x = region, \n                      y = mean(ingreso_mensual))) +\n  geom_col()\n\n\n\nggplot(bienestar %&gt;% \n         group_by(region) %&gt;% \n         summarise(ingreso_promedio = mean(ingreso_mensual)),\n       aes(x = region, \n           y = ingreso_promedio)) +\n  geom_col()\n\n\n\nggplot(bienestar %&gt;% \n         group_by(region), \n       aes(x = region, \n           y = mean(ingreso_mensual))) +\n  geom_bar()\n\n\n\nggplot(bienestar, \n       aes(x = region, \n           y = ingreso_mensual)) +\n  geom_bar(stat = \"summary\", \n           fun = \"mean\")\n8. Flor quiere crear un gráfico de dispersión para observar la relación entre el ingreso mensual y el acceso a servicios básicos, con puntos coloreados según la región. ¿Qué código es correcto?\n\n\n\nggplot(bienestar, \n       aes(x = acceso_servicios, \n           y = ingreso_mensual, \n           color = region)) +\n  geom_point()\n\n\n\nggplot(bienestar, \n       aes(x = ingreso_mensual, \n           y = acceso_servicios, \n           fill = region)) +\n  geom_point()\n\n\n\nggplot(bienestar, \n       aes(x = acceso_servicios, \n           y = ingreso_mensual)) +\n  geom_point(color = region)\n\n\n\nggplot(bienestar, aes(x = acceso_servicios, \n                      y = ingreso_mensual)) +\n  geom_point(aes(color = region))\n9. Flor quiere analizar cómo varía el ingreso mensual según el nivel educativo en cada región. Usa facetas para separar por región. ¿Qué código es correcto?\n\n\n\nggplot(bienestar, aes(x = nivel_educativo, \n                      y = ingreso_mensual, \n                      fill = nivel_educativo)) +\n  geom_boxplot() +\n  facet_grid(~ region)\n\n\n\nggplot(bienestar, aes(x = nivel_educativo, \n                      y = ingreso_mensual, \n                      fill = nivel_educativo)) +\n  geom_boxplot() +\n  facet_wrap(~ region)\n\n\n\nggplot(bienestar, aes(x = ingreso_mensual, \n                      y = nivel_educativo, \n                      fill = region)) +\n  geom_boxplot() +\n  facet_wrap(~ region)\n\n\n\nggplot(bienestar, aes(x = nivel_educativo, \n                      y = ingreso_mensual, \n                      fill = region)) +\n  geom_boxplot() +\n  facet_wrap(~ region)\n\n\n\n\nBryan, Jennifer. 2023. «gapminder: Data from Gapminder». https://CRAN.R-project.org/package=gapminder.\n\n\nWickham, Hadley. 2016. «ggplot2: Elegant Graphics for Data Analysis». https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. «Welcome to the tidyverse» 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, y Jennifer Bryan. 2023. «readxl: Read Excel Files». https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, y Davis Vaughan. 2023. «dplyr: A Grammar of Data Manipulation». https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Evan Miller, y Danny Smith. 2023. «haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’ Files». https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, Davis Vaughan, y Maximilian Girlich. 2024. «tidyr: Tidy Messy Data». https://CRAN.R-project.org/package=tidyr.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integración</span>"
    ]
  },
  {
    "objectID": "statdesc.html",
    "href": "statdesc.html",
    "title": "4  Estadística descriptiva",
    "section": "",
    "text": "4.1 Resúmenes numéricos\nSe generan resúmenes numéricos a partir de variables cuantitativas, ya sean continuas o discretas. Estos resúmenes incluyen:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "statdesc.html#resúmenes-numéricos",
    "href": "statdesc.html#resúmenes-numéricos",
    "title": "4  Estadística descriptiva",
    "section": "",
    "text": "Medidas de tendencia central: Nos indican el valor central o típico en el conjunto de datos.\nMedidas de dispersión: Describen qué tan concentrados o dispersos están los datos en torno a la tendencia central.\nMedidas de posición: Ubican un valor dentro del conjunto de datos en función de su orden\nMedidas de forma: Caracterizan la estructura de la distribución en términos de simetría (asimetría) y concentración (curtosis).\n\n\n4.1.1 Medidas de tendencia central\nLas medidas de tendencia central son estadísticas que describen el valor alrededor del cual tienden a concentrarse los datos. Su propósito es identificar un punto representativo que sintetice el comportamiento general de la variable.\nMedia\nLa más popular. Es la suma de todos los valores dividida por el número de observaciones.\nMatemáticamente, se define como:\n\\[\n\\mu = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\\]\nDonde:\n\n\\(\\mu\\) es la media.\n\\(x_i\\) representa cada valor individual en el conjunto de datos.\n\\(n\\) es el número total de observaciones.\n\nPor ejemplo, para calcular la media de la esperanza de vida en el conjunto de datos filtrados al año 2007, utilizamos la función mean() en R, especificando la variable de interés (lifeExp, que representa la esperanza de vida) del conjunto de datos:\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\nPodemos observar que la esperanza de vida media en 2007 fue de 67 años aproximadamente.\nLa media puede entenderse como el punto de equilibrio en un conjunto de datos, similar al centro de gravedad en una balanza. Este valor representa un balance que toma en cuenta tanto la cantidad de datos como la magnitud de cada uno. En otras palabras, la media es la ubicación del “punto de equilibrio” de todos los datos distribuidos con sus respectivas magnitudes repartidas a lo largo de un segmento de línea. En este sentido, la media indica el punto en el que el “peso” total de los valores a la izquierda se compensa exactamente con el de los valores a la derecha, por lo que ofrece una referencia del comportamiento general del conjunto.\nEsto puede observarse en el siguiente gráfico, que muestra la distribución de la esperanza de vida en 2007. La línea roja indica la media, que equilibra visualmente la distribución. Si se calcula la suma de las diferencias individuales entre cada dato y la media (respetando el signo de cada diferencia), el resultado será siempre igual a cero.\n\n\n\n\n\n\n\n\n\nMediana\nLa mediana es otra medida de tendencia central, pero su definición es diferente a la media. No se basa en magnitudes acumuladas, sino en el orden de los datos. Corresponde al valor que divide al conjunto en dos mitades: el 50 % de los datos se encuentra por debajo de la mediana, y el otro 50 % por encima.\nPara calcular la mediana de la esperanza de vida en el conjunto de datos filtrado al año 2007, utilizamos la función median() en R, especificando la variable de interés (lifeExp):\n\nmedian(gapminder_2007$lifeExp)\n\n[1] 71.9355\n\n\nLa mediana de la esperanza de vida en 2007 se sitúa alrededor de 72 años.\nEl siguiente gráfico muestra la distribución de la esperanza de vida en 2007, con la mediana destacada como una línea azul:\n\n\n\n\n\n\n\n\n\nLa mediana es el punto medio que garantiza que la mitad de los datos está por encima y la otra mitad por debajo, sin tener en cuenta la magnitud de los valores individuales.\nMediana vs. media\nLa media, aunque es una medida ampliamente utilizada para representar el centro de un conjunto de datos, puede enfrentar ciertos problemas, especialmente en distribuciones que no son simétricas. Esto es debido a su sensibilidad a los valores extremos o outliers. En presencia de estos valores atípicos, la media puede desplazarse de forma significativa, lo que podría dar una impresión errónea del centro “típico” de los datos.\nLa mediana, al dividir los datos en dos mitades iguales, es una medida robusta, lo que significa que no se ve fácilmente afectada por valores extremos o atípicos que podrían distorsionar la media. No importa cuán grandes o pequeños sean los valores individuales, la mediana siempre asegura una división equitativa en términos de la cantidad de datos a cada lado, lo que la convierte en una opción confiable para resumir el punto medio de distribuciones no simétricas.\n\n\n\n\n\n\n\n\n\nEl portal Datatab tiene una representación gráfica bastante clara de ello:\n\n\n\nExtraído de: https://datatab.net/\n\n\nLa media busca el punto central de los datos considerando la magnitud de cada valor y las distancias entre ellos. A diferencia de la mediana, que divide los datos en dos grupos con igual número de observaciones, la media no garantiza que haya la misma cantidad de datos a cada lado, pero sí identifica un punto en el que los valores más grandes y más pequeños se compensan en conjunto. En este sentido, la media actúa como un punto medio que equilibra las distancias de todos los valores, tomando en cuenta tanto cuántos datos hay como cuán grandes o pequeños son en relación con el resto del conjunto.\nAunque la mediana es especialmente útil en la presencia de valores extremos (outliers) o distribuciones asimétricas, la media sigue siendo una herramienta fundamental debido a su capacidad para incorporar la influencia de todos los datos del conjunto. Esto la hace más sensible a cualquier cambio dentro de la distribución, lo que resulta valioso cuando se busca que la medida central refleje el impacto de todos los valores, incluso los más extremos o inusuales.\nSin embargo, la mediana también tiene sus propias limitaciones. Su cálculo solo considera la posición central del conjunto de datos y, en consecuencia, ignora la información sobre los valores individuales. Por ejemplo, si evaluamos la satisfacción ciudadana hacia los servicios públicos en una escala de 1 a 4 (donde 1 es “muy insatisfecho” y 4 es “muy satisfecho”) y encontramos que 600 de los 1000 encuestados seleccionaron 1, la mediana será 1. Esto indica una mayoría insatisfecha, pero no refleja la diversidad de opiniones del grupo, donde una parte considerable eligió valores como 2, 3 o 4. En contraste, la media, que podría ser 1.65 en este caso, captura mejor esa variabilidad al considerar todos los datos en el cálculo.\nAl final, la elección entre la media y la mediana depende del contexto y de las características del conjunto de datos. La media es ideal cuando queremos considerar todas las variaciones en los datos, pero puede ser afectada significativamente por valores extremos o distribuciones sesgadas (exploraremos esto más adelante). Por otro lado, la mediana es una opción más robusta que proporciona una visión clara del punto medio, aunque con la limitación de no utilizar toda la información disponible, lo que puede llevar a la pérdida de detalles importantes sobre la variabilidad o la influencia de valores extremos. Por ello, lo más informativo es reportar ambas.\nModa\nLa moda se define como el valor que ocurre con mayor frecuencia en un conjunto de datos. Es especialmente útil cuando se trabaja con datos categóricos o discretos, donde los valores no son numéricos o no tienen una relación de orden clara (por ejemplo, preferencias políticas, o nacionalidades). Hablaremos mas de ella en los resúmenes categóricos.\n\n\n4.1.2 Medidas de dispersión\nLas medidas de dispersión son estadísticas que describen la variabilidad o dispersión de un conjunto de datos. Mientras que las medidas de tendencia central, como la media y la mediana, nos indicaban un valor central que representa el “centro” de los datos, las medidas de dispersión nos muestran qué tan alejados o agrupados están los datos en relación con ese valor central. Es decir, permiten evaluar el grado de concentración o dispersión de los datos alrededor de la referencia representativa.\nRango\nEl rango es la diferencia entre el valor máximo y el valor mínimo de un conjunto de datos. Es la medida de dispersión más simple, y nos da una idea rápida de la amplitud de los datos. Sin embargo, el rango puede resultar engañoso cuando el conjunto de datos contiene valores atípicos, ya que estos pueden ampliar artificialmente la distancia entre el valor mínimo y el máximo, sin reflejar adecuadamente la dispersión general de los datos. Por lo que es una medida poco robusta.\nPor ejemplo, para calcular el rango de la esperanza de vida en el conjunto de datos filtrado al año 2007, simplemente restamos el valor mínimo del valor máximo:\n\nmax(gapminder_2007$lifeExp) - min(gapminder_2007$lifeExp)\n\n[1] 42.99\n\n\nO con la función range()\n\nrange(gapminder_2007$lifeExp)\n\n[1] 39.613 82.603\n\n\nEl rango es útil para entender la extensión de los datos, pero no proporciona información sobre cómo se distribuyen los valores entre el mínimo y el máximo.\nVarianza y desviación estándar\nLa varianza y la desviación estándar son medidas de dispersión que nos indican, en promedio, cuánto se desvían los datos del valor central, que en este caso es la media. Aunque estos conceptos pueden sonar un poco complicados al principio, los vamos a desglosar paso a paso.\nPrimero, recordemos que la media es el promedio de todos los valores de esperanza de vida en nuestro conjunto de datos:\n\nmean(gapminder_2007$lifeExp)\n\n[1] 67.00742\n\n\nCalculamos que la esperanza de vida media en 2007 es de aproximadamente 67 años. Este valor representa el “centro” alrededor del cual queremos ver cómo se distribuyen las esperanzas de vida de diferentes países.\nPara cada país en nuestro conjunto de datos, calculamos qué tan lejos está su esperanza de vida de la media de 67 años. Estas diferencias nos muestran si la esperanza de vida en un país es mayor o menor que el promedio mundial. Para visualizar esto, podemos usar un gráfico de puntos, donde cada punto representa un país, y la línea roja en el gráfico indica la media de 67 años. Esto nos permite ver qué países están por encima o por debajo de la media y cuán lejos están de este valor central.\n\ngapminder_2007 %&gt;% \n  ggplot(aes(x = 1:nrow(gapminder_2007), y = lifeExp)) +\n  geom_point(position = position_jitter(width = 0.2), color = '#2ecc71') +\n  geom_hline(yintercept = mean(gapminder_2007$lifeExp), color = 'red', linetype = \"dashed\", size = 1) +\n  labs(title = 'Variación de la esperanza de vida respecto a la media (2007)',\n       x = 'Observaciones (países)',\n       y = 'Esperanza de vida') +\n  theme_minimal() +\n  coord_flip()\n\n\n\n\n\n\n\n\nEstas diferencias son importantes porque nos permiten ver la variabilidad en los datos: un valor positivo indica que la esperanza de vida en ese país es mayor que la media, mientras que un valor negativo indica que es menor. Cuanto más grande sea la diferencia, mayor es la desviación de ese país con respecto al valor central, lo que nos da una primera idea de cuán dispersos están los datos alrededor de la media.\nSin embargo, si simplemente sumáramos estas diferencias, las positivas y las negativas se cancelarían entre sí, resultando en una suma de cero. ¿Por qué? Si recuerdas la definición de media, es precisamente lo que hace la media: equilibrar las diferencias por encima y por debajo del valor central. Este cero no nos proporcionaría ninguna información útil sobre la variabilidad de los datos. Para evitar este problema, elevamos cada diferencia al cuadrado, lo que convierte todas las diferencias en valores positivos y da más peso a las desviaciones más grandes.\n\n\n\nElaboración propia\n\n\nLuego, tomamos el promedio de todas esas diferencias al cuadrado y las dividimos el el número de datos, lo que nos da la varianza. La varianza mide cuánto se dispersan los datos en relación con la media. Se define como:\n\\[\n\\sigma^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{n}\n\\] Donde:\n- (\\(\\sigma^2\\)) es la varianza.\n- (\\(x_i\\)) representa cada valor individual.\n- (\\(\\mu\\)) es la media.\n- (\\(n\\)) es el número total de observaciones.\nLa varianza nos da una idea de cuánta variabilidad hay en las esperanzas de vida de los diferentes países en comparación con la media de 67 años. Podemos calcular la varianza en R fácilmente:\n\nvar(gapminder_2007$lifeExp)\n\n[1] 145.7578\n\n\nSin embargo, la varianza tiene un problema: no es tan fácil de interpretar directamente porque está en unidades “cuadradas”. Sabemos que una menor varianza indica menos variación y una mayor varianza indica más, pero al elevar al cuadrado las diferencias, hemos “inflado” estos valores. Para corregir esto, le sacamos la raíz cuadrada a la varianza, lo que nos da la desviación estándar. La desviación estándar nos dice, en promedio, cuánto se desvía la esperanza de vida de un país del valor central de 67 años, pero en las mismas unidades que estamos utilizando para medir la esperanza de vida (años).\n\\[\n\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{n}}\n\\]\nLa desviación estándar recibe su nombre porque es una medida “estandarizada” de cuánto se desvían los datos del valor central, la media. Al sacar la raíz cuadrada de la varianza, no solo hacemos que la medida sea más fácil de interpretar, sino que también estandarizamos la dispersión de los datos, permitiendo que podamos comparar la variabilidad entre diferentes conjuntos de datos, incluso si están en diferentes escalas o unidades. La desviación estándar nos proporciona una manera consistente de entender la variabilidad en cualquier situación, sean años, alturas, dinero, gramos o kilómetros:\nEn R, calculamos la desviación estándar usando sd()\n\nsd(gapminder_2007$lifeExp)\n\n[1] 12.07302\n\n\nLa desviación estándar junto con la media y la mediana son las medidas más útiles y utilizadas para variables numéricas en el análisis estadístico. Seguiremos viéndolas en contextos más complejos en el transcurso del libro.\n\n\n4.1.3 Medidas de posición\nLas medidas de posición son estadísticas que nos permiten describir dónde se sitúan los valores de un conjunto de datos en relación con los demás. Tenemos valores como los cuartiles, percentiles y deciles, dependiendo de cuantas partes dividimos los datos.\nCuartiles\nLos cuartiles dividen el conjunto de datos en cuatro grupos iguales. Los tres cuartiles son los siguientes:\n\n\n\nExtraído de: https://www.investopedia.com/terms/q/quartile.asp\n\n\n\nEl primer cuartil (Q1), que es el percentil 25, es el valor en el cual el 25% de los datos se encuentra por debajo.\nEl segundo cuartil (Q2) es la mediana, de la cual ya hemos hablado, y es el valor donde el 50% está por debajo de él y el 50% está por encima de él.\nEl último cuartil es el tercer cuartil (Q3), que es el valor donde el 75% de los datos se encuentra por debajo de él.\n\nEstas medidas ayudan a comprender la distribución de los datos en varias secciones y la forma en que se agrupan alrededor de la mediana.\nPara calcular los cuartiles de la esperanza de vida en el conjunto de datos filtrado al año 2007, podemos utilizar la función quantile() en R:\n\nquantile(gapminder_2007$lifeExp)\n\n      0%      25%      50%      75%     100% \n39.61300 57.16025 71.93550 76.41325 82.60300 \n\n\nPercentiles\nLos percentiles son una extensión de los cuartiles y dividen el conjunto de datos en 100 partes iguales. El percentil 25, por ejemplo, es equivalente al primer cuartil, y el percentil 50 es equivalente a la mediana. Los percentiles son útiles cuando queremos conocer la posición relativa de un valor en comparación con el resto de los datos.\nPodemos calcular cualquier percentil en R utilizando la función quantile(), especificando el porcentaje deseado. Por ejemplo, el percentil 90 de la esperanza de vida sería:\n\nquantile(gapminder_2007$lifeExp, 0.90)\n\n    90% \n79.8223 \n\n\nRango intercuartílico (IQR)\nEl rango intercuartílico es una medida de dispersión basada en los cuartiles. Representa la diferencia entre el tercer cuartil (Q3) y el primer cuartil (Q1), y nos indica la amplitud de la “caja” central de los datos, es decir, la mitad del conjunto de datos alrededor de la mediana. Esta medida es resistente a los valores extremos, ya que solo toma en cuenta el rango intermedio de los datos.\nPodemos calcular el rango intercuartílico en R utilizando la función IQR():\n\nIQR(gapminder_2007$lifeExp)\n\n[1] 19.253\n\n\nEl rango intercuartílico es especialmente útil cuando los datos contienen valores atípicos o extremos, ya que el IQR no se ve afectado por estos valores, a diferencia de la desviación estándar o el rango total. De esta forma, ayuda a tener una medida más robusta de la dispersión.\n\n\n4.1.4 Medidas de forma de la distribución\nLas medidas de forma de la distribución nos permiten describir la simetría y el apuntamiento de la distribución de los datos, lo que nos da información adicional sobre la estructura de los datos más allá de la tendencia central y la dispersión.\nAsimetría (Skewness)\nLa asimetría mide la simetría de la distribución de los datos. Pero, ¿a qué nos referimos con asimetría? La asimetría mide la falta de simetría en la distribución de los datos. Una distribución es simétrica cuando los valores se distribuyen de manera uniforme a ambos lados de la media.\n\n\n\nElaboración propia\n\n\nLa asimetría describe si la cola de la distribución se inclina más hacia la derecha o hacia la izquierda, lo cual nos indica qué tan equilibrada está la distribución respecto a su centro.\n\n\n\nExtraído de: https://www.biologyforlife.com/skew.html\n\n\n\nAsimetría positiva (o hacia la derecha): La cola de la distribución es más larga a la derecha. Esto significa que hay más valores pequeños concentrados cerca de la media y algunos pocos valores grandes que están mucho más alejados, extendiendo la cola derecha. En este escenario, la media suele ser mayor que la mediana, ya que los valores extremos tiran la media hacia la derecha.\nAsimetría negativa (o hacia la izquierda): La cola de la distribución es más larga a la izquierda. Esto significa que hay más valores grandes cerca de la media y unos pocos valores muy pequeños que extienden la cola hacia la izquierda. En este tipo de distribución, la mediana suele ser mayor que la media.\nAsimetría nula (distribución simétrica): Si la asimetría es cercana a cero, los valores están distribuidos de manera uniforme a ambos lados de la media. Un ejemplo clásico de esto es la distribución normal, donde la media y la mediana coinciden. Mas sobre la distribución normal en el siguiente capítulo.\n\nPodemos calcular la asimetría en R usando la función skewness() del paquete dlookr:\n\nskewness(gapminder_2007$lifeExp)\n\n[1] -0.6814798\n\n\nEl valor de asimetría (skewness) de lifeExp es -0.6814798, lo que indica que la distribución de la esperanza de vida en 2007 presenta una asimetría negativa.\nLo podemos representar con un gráfico de densidad:\n\ngapminder_2007 %&gt;% \n  ggplot(aes(x = lifeExp)) +\n  geom_density(color = 'darkgreen', linewidth = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCurtosis\nLa curtosis mide el apuntamiento de la distribución, es decir, qué tan concentrados o dispersos están los datos en torno a la media. Se utiliza para describir la forma de la distribución, en particular, la presencia de valores extremos en las colas. La curtosis estándar de una distribución normal es 3, lo que se considera “curtosis mesocúrtica”. Las distribuciones pueden tener mayor o menor curtosis, lo que sugiere diferencias en la concentración de los datos y en la presencia de valores extremos.\n\n\nCurtosis alta (leptocúrtica): En una distribución con alta curtosis, los datos están muy concentrados alrededor de la media, lo que genera un pico más alto y colas más largas. Esto sugiere que hay una mayor probabilidad de que los datos contengan valores extremos, tanto bajos como altos. Por ejemplo, en una distribución de ingresos, si observamos una alta curtosis, significa que hay muchas personas con ingresos muy similares, pero también existen algunos individuos con ingresos extremadamente altos o bajos.\nCurtosis baja (platicúrtica): En una distribución con baja curtosis, los datos están más dispersos y distribuidos de manera uniforme, lo que produce una distribución más plana y con colas cortas. Esto sugiere que hay menos valores extremos y que los datos están más uniformemente repartidos alrededor de la media. Un ejemplo podría ser una distribución de alturas en una población donde la mayoría de los individuos tienen estaturas similares, sin grandes desviaciones.\nCurtosis normal (mesocúrtica): La distribución normal tiene una curtosis de 3, lo que indica una forma estándar con una cantidad moderada de valores concentrados alrededor de la media y colas de longitud moderada. Este es el punto de referencia con el cual se comparan las distribuciones más apuntadas o más planas.\n\nPodemos calcular la curtosis en R usando también la función kurtosis() del paquete dlookr:\n\nkurtosis(gapminder_2007$lifeExp)\n\n[1] 2.157154\n\n\nEl valor de curtosis de lifeExp es 2.16 lo que indica que es platicúrtica, es decir, más plana que una distribución normal, con colas más cortas y menos valores extremos.\nAmbas medidas de forma, asimetría y curtosis, nos ayudan a entender mejor la forma de la distribución y a identificar posibles anomalías o valores extremos que no serían evidentes solo con las medidas de tendencia central y dispersión.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "statdesc.html#resúmenes-categóricos",
    "href": "statdesc.html#resúmenes-categóricos",
    "title": "4  Estadística descriptiva",
    "section": "4.2 Resúmenes categóricos",
    "text": "4.2 Resúmenes categóricos\nLas variables categóricas son aquellas que representan grupos o categorías en lugar de valores numéricos. A diferencia de las variables numéricas, que pueden ser descritas mediante medidas como la media o la desviación estándar, las variables categóricas no se prestan a una descripción numérica directa. En lugar de esto, necesitamos utilizar otras técnicas para resumir y entender estas variables.\nPara la demostración, modificaremos la base de datos para agregarle una variable categórica adicional:\n\ngapminder_mod = gapminder_2007 %&gt;%\n  mutate(nivel_esp = cut(lifeExp,\n                         breaks = c(0, 65, 80, Inf),\n                         labels = c(\"bajo\", \"medio\", \"alto\")))  %&gt;% \n  select(country, continent, lifeExp, nivel_esp)\n\nhead(gapminder_mod)\n\n# A tibble: 6 × 4\n  country     continent lifeExp nivel_esp\n  &lt;chr&gt;       &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;    \n1 Afghanistan Asia         43.8 bajo     \n2 Albania     Europe       76.4 medio    \n3 Algeria     Africa       72.3 medio    \n4 Angola      Africa       42.7 bajo     \n5 Argentina   Americas     75.3 medio    \n6 Australia   Oceania      81.2 alto     \n\n\nLa variable nivel_esp se ha creado para clasificar la esperanza de vida (lifeExp) en tres categorías:\n\nBajo: Esperanza de vida menor a 65 años.\nMedio: Esperanza de vida entre 65 y 80 años.\nAlto: Esperanza de vida mayor a 80 años.\n\nPara variables categóricas, utilizamos dos medidas clave: la frecuencia y la proporción.\n\nFrecuencia: Esta medida indica el número de veces que ocurre cada categoría en el conjunto de datos. Es útil para comprender cuántas observaciones caen en cada categoría específica.\nPor ejemplo, si estamos analizando las categorías de esperanza de vida en el conjunto de datos gapminder_mod, la frecuencia nos dirá cuántos países caen en cada una de las categorías de esperanza de vida (bajo, medio, alto).\nPara calcular las frecuencias en R, usamos la función table para contar el número de países en cada categoría de nivel_esp:\n\ntable(gapminder_mod$nivel_esp)\n\n\n bajo medio  alto \n   52    77    13 \n\n\nDe esta forma podemos observar que, bajo nuestros parámetros, 52 países tiene una baja esperanza de vida, 77 media y 13 alta.\nProporción: La proporción representa la frecuencia de cada categoría en relación con el total de observaciones. Se calcula dividiendo la frecuencia de una categoría por el número total de observaciones. La proporción es útil para entender la distribución relativa de las categorías en el conjunto de datos.\n\nSiguiendo el mismo ejemplo, la proporción nos indicará qué fracción del total de países cae en cada categoría de esperanza de vida. Usamos prop.table sobre la tabla de frecuencias (es posible aplicar un función dentro de otra):\n\nprop.table(table(gapminder_mod$nivel_esp))\n\n\n     bajo     medio      alto \n0.3661972 0.5422535 0.0915493 \n\n\nLo que hace esto es calcular la proporción de cada categoría dividiendo la frecuencia de cada categoría por el total de observaciones. El resultado es una tabla que muestra la proporción de países en cada categoría de nivel_esp. Podemos observar que el 36.6% países tiene una baja esperanza de vida, 54.2% media y 9.1% alta.\nEn el apartado de medidas de tendencia central, mencionamos brevemente la moda. Podemos retomarla en el contexto de las variables categóricas. A diferencia de otras medidas como la media o la mediana, que son más adecuadas para variables numéricas, la moda es especialmente útil en variables categóricas, ya que nos permite identificar cuál es la categoría que predomina en un grupo o conjunto de observaciones.\n\ntable(gapminder_mod$nivel_esp)\n\n\n bajo medio  alto \n   52    77    13 \n\n\nLa moda de la variable nivel_esp es medio, ya que es la categoría con mayor frecuencia (77 países). Esto indica que, en general, la mayoría de los países tienen una esperanza de vida entre 65 y 80 años en este conjunto de datos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "statdesc.html#gráficos-descriptivos",
    "href": "statdesc.html#gráficos-descriptivos",
    "title": "4  Estadística descriptiva",
    "section": "4.3 Gráficos descriptivos",
    "text": "4.3 Gráficos descriptivos\nLos gráficos descriptivos son herramientas esenciales al momento de realizar un análisis, ya que nos permiten visualizar y comprender de manera más intuitiva los patrones y distribuciones de nuestros datos, tanto numéricos como categóricos. A través de los gráficos, podemos identificar rápidamente tendencias, relaciones y posibles anomalías en los datos que podrían no ser evidentes solo con resúmenes numéricos.\n\n4.3.1 Gráficos para variables numéricas\nLos datos numéricos, por su naturaleza, permiten realizar operaciones matemáticas que nos ayudan a comprender su distribución. Esta capacidad también se refleja en los gráficos que utilizamos para visualizarlos. Los gráficos más comunes para datos numéricos incluyen histogramas, diagramas de caja (boxplots), gráficos de dispersión, entre otros. Cada uno de estos gráficos no solo representa visualmente los datos, sino que también captura aspectos fundamentales ya mencionados antes como la tendencia central y la dispersión.\nHistogramas:\nEl histograma es una representación gráfica de la distribución de los datos numéricos. Divide los datos en intervalos (o “bins”) y muestra la frecuencia de los datos que caen dentro de cada intervalo. Por ejemplo, para visualizar la distribución de la esperanza de vida en 2007, podemos utilizar un histograma:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = lifeExp)) +  \n  geom_histogram(\n    # Color de relleno del histograma \n    fill = '#2ecc71',  \n    # Color de los bordes de las barras\n    color = 'black',   \n    # Ancho de las barras del histograma\n    binwidth = 5) +  \n  labs(title = 'Distribución de la esperanza de vida (2007)',  \n       x = 'Esperanza de vida',  \n       y = 'Frecuencia') +  \n  theme_minimal()  \n\n\n\n\n\n\n\n\nUno de los argumentos más importantes al crear un histograma es binwidth, que controla el ancho de estos intervalos. En este ejemplo, binwidth = 5 agrupa las esperanzas de vida en intervalos de 5 años. Si aumentamos el binwidth, los intervalos serán más amplios, lo que puede suavizar la distribución pero también ocultar detalles. Por el contrario, un binwidth más pequeño creará más intervalos, mostrando más detalles pero haciendo el gráfico potencialmente más ruidoso y difícil de interpretar. El valor de binwidth puede cambiar drásticamente la apariencia del histograma:\nCon binwidth de 10:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = lifeExp)) +  \n  geom_histogram(\n    # Color de relleno del histograma \n    fill = '#2ecc71',  \n    # Color de los bordes de las barras\n    color = 'black',   \n    # Ancho de las barras del histograma\n    binwidth = 10) +  \n  labs(title = 'Distribución de la esperanza de vida (2007)',  \n       x = 'Esperanza de vida',  \n       y = 'Frecuencia') +  \n  theme_minimal()  \n\n\n\n\n\n\n\n\nCon binwidth de 2:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = lifeExp)) +\n  geom_histogram(fill = '#2ecc71', color = 'black', binwidth = 2) +\n  labs(title = 'Distribución de la esperanza de vida (2007)',\n       x = 'Esperanza de vida',\n       y = 'Frecuencia') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nDiagramas de caja (Boxplots):\nEl boxplot es útil para resumir la distribución de los datos mostrando los cuartiles, la mediana y los valores atípicos.\n\n\n\nExtraído de: https://www.kdnuggets.com/2019/11/understanding-boxplots.html\n\n\n\nLa caja en el boxplot representa el rango intercuartílico (IQR), la distancia entre el primer cuartil (Q1) y el tercer cuartil (Q3). Es decir, la caja contiene el 50% central de los datos.\nQ1 (Primer cuartil): Marca el 25% de los datos más bajos. Es el límite inferior de la caja.\nQ3 (Tercer cuartil): Marca el 25% de los datos más altos. Es el límite superior de la caja.\nLínea dentro de la Caja (Mediana): La línea horizontal dentro de la caja representa la mediana del conjunto de datos. La mediana es el valor que divide el conjunto de datos en dos partes iguales: el 50% de los datos está por debajo de la mediana y el otro 50% por encima. Si la mediana está más cerca de Q1 o de Q3, esto indica que la distribución de los datos es asimétrica (sesgada).\nBigotes (Whiskers): Los bigotes se extienden desde los bordes de la caja hasta los valores más lejanos que aún están dentro de 1.5 veces el rango intercuartílico (1.5 * IQR) desde Q1 y Q3. Los puntos más cercanos a Q1 y Q3 que están dentro de este rango son los extremos de los bigotes. Estos bigotes ayudan a visualizar la dispersión de los datos fuera de la caja.\nPuntos Atípicos (Outliers): Cualquier dato que se encuentra fuera del rango cubierto por los bigotes (más allá de 1.5 veces el IQR desde Q1 o Q3) se representa como un punto individual. Estos puntos se consideran outliers o valores atípicos. Los outliers pueden ser importantes para identificar observaciones inusuales que podrían requerir una atención especial o que podrían influir de manera significativa en los resultados del análisis.\n\nPodemos realizarlo para la variable a nivel mundial:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = lifeExp, y = 1)) +  \n  geom_boxplot(\n    # Color de relleno del boxplot \n    fill = '#2ecc71',  \n    # Color de los bordes del boxplot\n    color = 'black') +  \n  labs(title = 'Distribución de la esperanza de vida mundial (2007)',  \n       x = 'Continente',  \n       y = 'Esperanza de vida') +  \n  theme_minimal() \n\n\n\n\n\n\n\n\nO por cada continente definiendo el otro eje del gráfico:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = lifeExp, \n             y = continent)) +\n  geom_boxplot(fill = '#2ecc71', \n               color = 'black') +\n  labs(title = 'Distribución de la esperanza de vida por continente (2007)',\n       x = 'Continente',\n       y = 'Esperanza de vida') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nGráficos de dispersión (Scatterplots):\nLos gráficos de dispersión son ideales para visualizar la relación entre dos variables numéricas. Los argumentos como color, size y alpha y shape pueden ajustarse directamente dentro de geom_point() para aplicar efectos visuales fijos a todos los puntos, o bien dentro de aes() para mapear nuevas variables:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), \n             y = lifeExp)) +\n  geom_point(color = '#2ecc71') +\n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n       x = 'PIB per cápita',\n       y = 'Esperanza de vida') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAgregando size modificamos el tamaño:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), \n             y = lifeExp)) +\n  geom_point(\n    # Color de los puntos (verde claro)\n    color = '#2ecc71',  \n    # Tamaño de los puntos\n    size = 3) +  \n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n       x = 'PIB per cápita',\n       y = 'Esperanza de vida') +\n  theme_minimal() \n\n\n\n\n\n\n\n\nAgregando alpha modificamos la transparencia:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap),  \n             y = lifeExp)) +\n  geom_point(\n    # Color de los puntos (verde claro)\n    color = '#2ecc71',  \n    # Tamaño de los puntos\n    size = 3,  \n    # Transparencia de los puntos (valor entre 0 y 1)\n    alpha = 0.6) +  \n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',  \n       x = 'PIB per cápita',  \n       y = 'Esperanza de vida') +  \n  theme_minimal() \n\n\n\n\n\n\n\n\nPodemos modificar los puntos por otras formas geométrias con shape\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap),  \n             y = lifeExp)) +  \n  geom_point(\n    # Color de los puntos\n    color = '#2ecc71',  \n    # Tamaño de los puntos\n    size = 3,  \n    # Transparencia de los puntos (entre 0 y 1)\n    alpha = 0.6,  \n    # Forma de los puntos (17 es el símbolo de un triángulo)\n    shape = 17) +  \n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',  \n       x = 'PIB per cápita',  \n       y = 'Esperanza de vida') +  \n  theme_minimal()  \n\n\n\n\n\n\n\n\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point(color = '#2ecc71', \n             size = 3, \n             alpha = 0.6, \n             # Forma de los puntos (18 es un símbolo de diamante)\n             shape = 18) +\n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n       x = 'PIB per cápita',\n       y = 'Esperanza de vida') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCuando pasamos estos argumentos dentro de aes(), los vinculamos a una variable del conjunto de datos, lo que permite que los puntos reflejen atributos adicionales tanto numéricos como categóricos:\nPuedes usar color para asignar colores a los puntos según una variable como continente.\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), \n             y = lifeExp, \n             # Asigna un color diferente a cada continente\n             color = continent)) +\n  geom_point() +\n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n       x = 'PIB per cápita',\n       y = 'Esperanza de vida',\n       color = 'Continente') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEl tamaño de los puntos (size) puede reflejar una variable numérica, como la población.\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), \n             y = lifeExp, \n             # El tamaño de los puntos se asigna según la población\n             size = pop)) +\n  geom_point() +\n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n       x = 'PIB per cápita',\n       y = 'Esperanza de vida',\n       color = 'Continente') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nalpha controla la transparencia de los puntos, y puede estar vinculado a una variable numérica para indicar niveles de intensidad. Podemos usar una escala logarítmica para la población (pop)\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), \n             y = lifeExp, \n             # Se asigna la transparencia según el logaritmo de la población\n             alpha = log(pop))) +\n  geom_point() +\n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n       x = 'PIB per cápita',\n       y = 'Esperanza de vida',\n       color = 'Continente') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEn este caso, los puntos con mayor transparencia representan países con menor población.\nshape puede asignar diferentes formas a los puntos en función de una variable categórica.\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), \n             y = lifeExp, \n             shape = continent)) +\n  geom_point() +\n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n       x = 'PIB per cápita',\n       y = 'Esperanza de vida',\n       color = 'Continente') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEn este ejemplo, cada continente tiene una forma distinta en los puntos del gráfico.\nAhora podemos integrar todo ello en una sola gráfica. Recuerda: los argumentos en geom_point() son para todas las geometrías de la gráfica y los argumentos en aes() son para mapear la estética a una variable:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap),  \n             y = lifeExp,  \n             # Asigna colores según el continente\n             color = continent,  \n             # Asigna el tamaño de los puntos según la población\n             size = pop)) +  \n  # Establece la transparencia de los puntos (0.6\n  geom_point(alpha = 0.6,   \n  # Forma de los puntos (19 corresponde a círculos sólidos)          \n              shape = 19) +  \n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',  \n       x = 'PIB per cápita',  # Etiqueta del eje X\n       y = 'Esperanza de vida',  # Etiqueta del eje Y\n       color = 'Continente') +  # Leyenda para los colores, correspondiente al continente\n  scale_color_manual(values = c(\"Asia\" = \"#2471a3\", # Asigna color azul a Asia\n                               \"Europe\" = \"#229954\", # Asigna color verde a Europa\n                               \"Africa\" = \"#a93226\", # Asigna color rojo a África\n                               \"Americas\" = \"#884ea0\", # Asigna color morado a las Américas\n                               \"Oceania\" = \"#d4ac0d\")) + # Asigna color amarillo a Oceanía\n  # Establece el rango del tamaño de los puntos entre 1 y 20\n  scale_size_continuous(range = c(1, 20),  \n  # Define puntos de ruptura para los tamaños (1M, 100M, 1B)                      \n                        breaks = c(1e6, 1e8, 1e9),  \n  # Etiquetas de los tamaños (1 millón, 100 millones, 1 mil millones) \n                       labels = c(\"1M\",\"100M\", \"1B\")) +  \n  theme_minimal()  \n\n\n\n\n\n\n\n\nComo tercera variable podemos resaltar el continente\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), y = lifeExp, color = continent)) +\n    geom_point(alpha = 0.6, shape = 15, size = 3) +  # Mantener el tamaño fijo y más pequeño\n    labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n         x = 'PIB per cápita',\n         y = 'Esperanza de vida',\n         color = 'Continente') +\n    scale_color_manual(values = c(\"Asia\" = \"#2471a3\",    \n                                  \"Europe\" = \"#229954\", \n                                  \"Africa\" = \"#a93226\", \n                                  \"Americas\" = \"#884ea0\", \n                                  \"Oceania\" = \"#d4ac0d\")) + \n    theme_minimal()\n\n\n\n\n\n\n\n\nO podemos resaltar la población\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), y = lifeExp, size = pop)) +\n    geom_point(alpha = 0.6, color = '#229954') +\n    labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n         x = 'PIB per cápita',\n         y = 'Esperanza de vida',\n         color = 'Continente')  + \n  scale_size_continuous(range = c(1, 20),             \n                          breaks = c(1e6, 1e8, 1e9),  \n                          labels = c(\"1M\",\"100M\", \"1000M\")) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nPero ojo, a veces “menos es más”. No siempre más datos o más detalles en una gráfica la hacen mejor o más informativa. Si introducimos demasiada información, podemos sobrecargar la visualización y terminar con un gráfico que, en vez de ser claro, se vuelve confuso o poco útil. Hay que encontrar un equilibrio entre la estética y la información que queremos transmitir. Al final, es tu decisión cómo equilibrar estos elementos según lo que quieras resaltar.\n\n\n4.3.2 Gráficos para variables categóricas\nLas variables categóricas representan distintos grupos o categorías dentro de los datos. A diferencia de las variables numéricas, no se pueden realizar operaciones matemáticas directas con ellas. Sin embargo, son fundamentales para analizar patrones de distribución, frecuencia y relación entre grupos. En este caso, los gráficos para variables categóricas nos ayudan a identificar cómo se distribuyen los datos entre diferentes categorías y cómo estas se relacionan con otras variables.\nLos gráficos más comunes para visualizar variables categóricas son los gráficos de barras, los diagramas de torta y los diagramas de mosaico. Cada uno de estos gráficos cumple un propósito particular, pero todos nos ofrecen una representación visual clara de la frecuencia o proporción de cada categoría.\n\ngapminder_2007 = gapminder_2007 %&gt;%\n  mutate(nivel = cut(lifeExp, \n                                breaks = c(0, 50, 70, Inf), \n                                labels = c(\"Baja\", \"Media\", \"Alta\")))\n\nGráficos de barras:\nUn gráfico de barras es ideal para visualizar la distribución de categorías de una variable. Cada barra representa una categoría y su altura (o longitud) corresponde a la frecuencia o proporción de datos en dicha categoría. Es muy útil cuando se quiere comparar la frecuencia de varias categorías.\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = nivel)) +\n  geom_bar(fill = '#884ea0', color = 'black') +\n  labs(title = 'Distribución de países por esperanza de vida (2007)',\n       x = 'Nivel de Esperanza de Vida',\n       y = 'Cantidad de países') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEn este gráfico, el eje x representa las categorías de expectativa de vida, y el eje y muestra la cantidad de países en cada categoría. Si queremos destacar visualmente algunas categorías, podemos usar fill para asignar un color específico a cada una:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = nivel, fill = nivel)) +\n  geom_bar(color = 'black') +\n  labs(title = 'Distribución de países por esperanza de vida (2007)',\n       x = 'Nivel de Esperanza de Vida',\n       y = 'Cantidad de países',\n       fill = 'Niveles') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIncluso con facet_warp() podemos crear un gráfico de barras para cada continente\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = nivel, fill = nivel)) +\n  geom_bar(color = 'black') +\n  labs(title = 'Distribución de países por esperanza de vida (2007)',\n       x = '',\n       y = 'Cantidad de países',\n       fill = 'Niveles') +\n  theme_minimal() +\n  facet_wrap(~continent)\n\n\n\n\n\n\n\n\nGráfico de Barras Apiladas:\nUna variación del gráfico de barras tradicional pero introducimos la relación con una variable adicional. En este gráfico cada barra está dividida en segmentos que representan subcategorías de una variable adicional.\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = nivel, fill = continent)) +\n  geom_bar(position = 'stack', color = 'black') +\n  labs(title = 'Distribución de países por esperanza de vida y continente (2007)',\n       x = 'Nivel de Esperanza de Vida',\n       y = 'Cantidad de países',\n       fill = 'Continente') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEn este gráfico, el eje x representa los niveles de esperanza de vida, mientras que el eje y muestra la cantidad de países en cada categoría. El color de cada segmento dentro de una barra representa un continente, lo que visualizar la distribución de países en cada nivel de esperanza de vida.\nNormalmente, si queremos facilitar la comparación entre subcategorías, podemos usar position = 'fill' para normalizar las alturas de las barras y convertirlo en un gráfico de proporciones:\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = nivel, fill = continent)) +\n  geom_bar(position = 'fill', color = 'black') +\n  labs(title = 'Proporción de países por esperanza de vida y continente (2007)',\n       x = 'Nivel de Esperanza de Vida',\n       y = 'Proporción',\n       fill = 'Continente') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nDiagramas de torta (Pie charts):\nEl diagrama de torta es otra forma común de representar variables categóricas. Aunque se usa con frecuencia, es importante tener en cuenta que este gráfico puede no ser tan eficaz cuando hay muchas categorías o cuando las diferencias entre las proporciones son mínimas. El propósito es mostrar las proporciones relativas de cada categoría dentro del total. En caso usarlo es importarte saber que añadir etiquetas con los porcentajes en un diagrama de torta es necesario para mejorar la comprensión visual del gráfico.\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = \"\", fill = nivel)) +\n  geom_bar(width = 1, stat = \"count\") +\n  coord_polar(theta = \"y\") +\n  labs(title = 'Proporción de países por esperanza de vida (2007)',\n       fill = 'Expectativa de Vida') +\n  theme_void() + # Para eliminar las etiquetas de los ejes \n  # Añadimos texto con el cálculo de la proporción\n  geom_text(aes(label = \n                  scales::percent(after_stat(count)/sum(after_stat(count)))), \n            stat = 'count', \n            position = position_stack(vjust = 0.5), \n            color = 'white') # Añadir etiquetas con porcentajes\n\n\n\n\n\n\n\n\nDiagramas de flujo categórico:\nEste tipo de gráfico permite visualizar la relación entre dos variables categóricas mostrando cómo se distribuyen las categorías de una variable en función de otra. Podemos hacer uso el paquete ggalluvial (Brunson 2020).\n\nlibrary(ggalluvial)\n\n\ndf = gapminder_2007 %&gt;%\n  count(continent, nivel) %&gt;%\n  mutate(prop = n / sum(n))\n\nggplot(df,\n       aes(axis1 = continent, axis2 = nivel, y = prop)) +\n  geom_alluvium(aes(fill = nivel), width = 1/12, alpha = 0.7) +\n  geom_stratum(width = 1/6, color = \"grey30\") +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(title = \"Relación entre continente y esperanza de vida (2007)\",\n       y = \"Proporción\", x = NULL, fill = \"Nivel\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEn el gráfico, cada bloque rectangular representa una categoría de continente, mientras que las bandas que los conectan reflejan la distribución de los diferentes niveles de expectativa de vida (nivel) dentro de cada continente.\nEl grosor de cada banda es proporcional al peso relativo de cada categoría, lo que permite evaluar la composición y las diferencias entre grupos.\nExisten formas más detalladas de reportar tanto variables categóricas como numéricas. El proceso completo para explorar y entender los datos se denomina análisis exploratorio de datos o (EDA) por sus siglas en inglés (Exploratory Data Analysis).\n\n\n4.3.3 Prácticas a evitar al crear gráficos\nAsí como los gráficos de torta pueden resultar no se la mejor decisión al momento de comunicar nuestros datos o resultados, existen otros gráficos o estilo con los que se debería tener precaución. Por ejemplo:\nLos gráficos 3D pueden ser visualmente atractivos para algunos, pero a menudo dificultan la interpretación de los datos. Las distorsiones del espacio 3D y la perspectiva pueden hacer que las relaciones entre los puntos no sean claras, y la profundidad añadida puede confundir al espectador.\n\n\n\nExtraído de: https://peltiertech.com/excel-3d-charts-charts-with-no-value/\n\n\nLos colores similares o brillantes pueden dificultar la lectura, donde se que necesita contrastes claros entre categorías.\n\n\n\n\n\n\n\n\n\nLos gráficos sin títulos, etiquetas o leyendas son difíciles de interpretar. El público no sabe qué representan las variables o qué significa cada color o forma.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "statdesc.html#análisis-exploratorio-de-datos",
    "href": "statdesc.html#análisis-exploratorio-de-datos",
    "title": "4  Estadística descriptiva",
    "section": "4.4 Análisis exploratorio de datos",
    "text": "4.4 Análisis exploratorio de datos\nEl EDA (Exploratory Data Analysis) es un proceso que integra de forma sistemática todo lo que hemos realizado previamente, como el cálculo de medidas de tendencia central, dispersión, asimetría, curtosis y la visualización de datos a través de gráficos. Es una forma estructurada de explorar los datos para detectar patrones, anomalías y relaciones importantes entre variables, combinando análisis numérico y visual. Tenemos que realizar dos tipos de análisis: en univariado y el bivariado.\nVamos a recordar la estructura del dataset gapminder_2007\n\nstr(gapminder_2007)\n\ntibble [142 × 7] (S3: tbl_df/tbl/data.frame)\n $ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 4 1 1 2 5 4 3 3 4 ...\n $ year     : num [1:142] 2007 2007 2007 2007 2007 ...\n $ lifeExp  : num [1:142] 43.8 76.4 72.3 42.7 75.3 ...\n $ pop      : num [1:142] 31889923 3600523 33333216 12420476 40301927 ...\n $ gdpPercap: num [1:142] 975 5937 6223 4797 12779 ...\n $ nivel    : Factor w/ 3 levels \"Baja\",\"Media\",..: 1 3 3 1 3 3 3 3 2 3 ...\n\n\n\n4.4.1 Análisis univariado\nSeguiremos haciendo uso del paquete dlookr. Esta paquete nos ofrece una función muy práctica, describe(), que nos permite generar un resumen detallado de nuestras variables.\n\ndescribe(gapminder_2007)\n\n# A tibble: 4 × 26\n  described_variables     n    na   mean     sd se_mean    IQR skewness kurtosis\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 year                  142     0 2.01e3 0       0      0       NaN      NaN    \n2 lifeExp               142     0 6.70e1 1.21e1  1.01e0 1.93e1   -0.689   -0.830\n3 pop                   142     0 4.40e7 1.48e8  1.24e7 2.67e7    7.40    58.3  \n4 gdpPercap             142     0 1.17e4 1.29e4  1.08e3 1.64e4    1.22     0.350\n# ℹ 17 more variables: p00 &lt;dbl&gt;, p01 &lt;dbl&gt;, p05 &lt;dbl&gt;, p10 &lt;dbl&gt;, p20 &lt;dbl&gt;,\n#   p25 &lt;dbl&gt;, p30 &lt;dbl&gt;, p40 &lt;dbl&gt;, p50 &lt;dbl&gt;, p60 &lt;dbl&gt;, p70 &lt;dbl&gt;,\n#   p75 &lt;dbl&gt;, p80 &lt;dbl&gt;, p90 &lt;dbl&gt;, p95 &lt;dbl&gt;, p99 &lt;dbl&gt;, p100 &lt;dbl&gt;\n\n\nEsto genera una tabla con los siguientes valores:\n\nn: El número de observaciones no nulas (no faltantes) para cada variable.\nna: El número de valores faltantes (NA) en cada variable.\nmean: La media aritmética de cada variable numérica.\nsd: La desviación estándar, que mide la dispersión de los datos.\nse_mean: El error estándar de la media, que refleja la precisión de la media (ya lo veremos)\nIQR: El rango intercuartílico (Q3 - Q1), que mide la dispersión central.\nskewness: La asimetría de la distribución de la variable (positiva o negativa).\nkurtosis: La curtosis, que indica que tan pronunciada o plana es la distribución.\npercentiles (p00, p01, …, p100): Percentiles que indican el valor por debajo del cual cae un cierto porcentaje de las observaciones.\n\nNo cabe todo en la página del libro pero podemos visualizar todos los valores que calcula por nosotros y seleccionar lo que queremos.\n\ncolnames(describe(gapminder_2007))\n\n [1] \"described_variables\" \"n\"                   \"na\"                 \n [4] \"mean\"                \"sd\"                  \"se_mean\"            \n [7] \"IQR\"                 \"skewness\"            \"kurtosis\"           \n[10] \"p00\"                 \"p01\"                 \"p05\"                \n[13] \"p10\"                 \"p20\"                 \"p25\"                \n[16] \"p30\"                 \"p40\"                 \"p50\"                \n[19] \"p60\"                 \"p70\"                 \"p75\"                \n[22] \"p80\"                 \"p90\"                 \"p95\"                \n[25] \"p99\"                 \"p100\"               \n\n\nUtilizando la selección, podemos ser más específicos con lo que queremos mostrar con describe.\n\ndescribe(gapminder_2007)[c(\"described_variables\", \"n\", \"mean\", \"sd\", \"IQR\", \n                           \"skewness\", \"kurtosis\", \"p50\")]\n\n# A tibble: 4 × 8\n  described_variables     n       mean        sd    IQR skewness kurtosis    p50\n  &lt;chr&gt;               &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 year                  142     2007      0      0       NaN      NaN     2.01e3\n2 lifeExp               142       67.0    1.21e1 1.93e1   -0.689   -0.830 7.19e1\n3 pop                   142 44021220.     1.48e8 2.67e7    7.40    58.3   1.05e7\n4 gdpPercap             142    11680.     1.29e4 1.64e4    1.22     0.350 6.12e3\n\n\nPodemos especificar las variable que queremos describir introduciendo el nombre del dataset como primer argumento y las variables de interés luego.\n\ndescribe(gapminder_2007, lifeExp)[c(\"described_variables\", \"n\", \"mean\", \"sd\", \"IQR\", \n                           \"skewness\", \"kurtosis\", \"p50\")]\n\n# A tibble: 1 × 8\n  described_variables     n  mean    sd   IQR skewness kurtosis   p50\n  &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 lifeExp               142  67.0  12.1  19.3   -0.689   -0.830  71.9\n\n\nPuede ser que queramos que no tome en cuenta una variable, por ejemplo aquí el año no es muy útil para el análisis ya que esta filtrado al 2007. La podemos excluir con un signo -\n\ndescribe(gapminder_2007, -year)[c(\"described_variables\", \"n\", \"mean\", \"sd\", \"IQR\", \n                           \"skewness\", \"kurtosis\", \"p50\")]\n\n# A tibble: 3 × 8\n  described_variables     n       mean        sd    IQR skewness kurtosis    p50\n  &lt;chr&gt;               &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 lifeExp               142       67.0    1.21e1 1.93e1   -0.689   -0.830 7.19e1\n2 pop                   142 44021220.     1.48e8 2.67e7    7.40    58.3   1.05e7\n3 gdpPercap             142    11680.     1.29e4 1.64e4    1.22     0.350 6.12e3\n\n\nOtra forma alternativa es haciendo uso del paquete dplyr y select().\n\ngapminder %&gt;% \n  describe() %&gt;% \n  select(described_variables, mean, p25, p50, p75)\n\n# A tibble: 4 × 5\n  described_variables       mean       p25       p50        p75\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 year                    1980.     1966.     1980.      1993. \n2 lifeExp                   59.5      48.2      60.7       70.8\n3 pop                 29601212.  2793664   7023596.  19585222. \n4 gdpPercap               7215.     1202.     3532.      9325. \n\n\nPodemos hacer uso de las funciones de agrupamiento que nos proporciona dplyr para hacer una análisis por categoría. Por ejemplo, filtremos el conjunto de datos gapminder_2007 para excluir a Oceanía, ya que solo contiene dos países (Australia y Nueva Zelanda). Luego, agrupamos los datos por continente y utilizamos describe() para calcular estadísticas descriptivas para todas las variables numéricas excepto el año. Seleccionamos y mostramos la media, mediana y desviación estándar de cada variable\n\ngapminder_2007 %&gt;%\n  # Filtramos el continente \"Oceania\"\n  filter(continent != 'Oceania') %&gt;%\n  # Agrupamos los datos por continente\n  group_by(continent) %&gt;%\n  # Resumen estadístico excluyendo la columna \"year\"\n  describe(-year) %&gt;%\n  # Seleccionamos las columnas relevantes del resumen\n  select(continent, described_variables, mean, p50, sd)\n\n# A tibble: 12 × 5\n   continent described_variables        mean        p50           sd\n   &lt;fct&gt;     &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 Africa    gdpPercap                3089.      1452.       3618.  \n 2 Americas  gdpPercap               11003.      8948.       9713.  \n 3 Asia      gdpPercap               12473.      4471.      14155.  \n 4 Europe    gdpPercap               25054.     28054.      11800.  \n 5 Africa    lifeExp                    54.8       52.9         9.63\n 6 Americas  lifeExp                    73.6       72.9         4.44\n 7 Asia      lifeExp                    70.7       72.4         7.96\n 8 Europe    lifeExp                    77.6       78.6         2.98\n 9 Africa    pop                  17875763.  10093310.   24917726.  \n10 Americas  pop                  35954847.   9319622    68833781.  \n11 Asia      pop                 115513752.  24821286   289673399.  \n12 Europe    pop                  19536618.   9493598    23624744.  \n\n\nHasta ahora, hemos estado enfocados en variables numéricas, pero ¿qué pasa con las categóricas? Aquí es donde entra la función univar_category(), que nos permite describir variables categóricas, como el continente o el país:\n\nunivar_category(gapminder_2007)\n\n$continent\n# A tibble: 5 × 3\n  continent     n   rate\n  &lt;fct&gt;     &lt;int&gt;  &lt;dbl&gt;\n1 Africa       52 0.366 \n2 Americas     25 0.176 \n3 Asia         33 0.232 \n4 Europe       30 0.211 \n5 Oceania       2 0.0141\n\n$nivel\n# A tibble: 3 × 3\n  nivel     n  rate\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 Baja     19 0.134\n2 Media    40 0.282\n3 Alta     83 0.585\n\n\nFinalmente, podemos utilizar otra herramienta útil para un resumen general del dataset: skimr. Este paquete nos da una visión rápida de todas las variables, tanto numéricas como categóricas:\n\n#install.packages('skimr')\nlibrary(skimr)\n\n\ngapminder_2007 %&gt;%\n  select(-pop) %&gt;%  \n  skim() %&gt;%  # Generar el resumen con skim()\n  mutate(across(where(is.numeric), ~round(., 2))) \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n142\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncountry\n0\n1\n4\n24\n0\n142\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncontinent\n0\n1\nFALSE\n5\nAfr: 52, Asi: 33, Eur: 30, Ame: 25\n\n\nnivel\n0\n1\nFALSE\n3\nAlt: 83, Med: 40, Baj: 19\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2007.00\n0.00\n2007.00\n2007.00\n2007.00\n2007.00\n2007.00\n▁▁▇▁▁\n\n\nlifeExp\n0\n1\n67.01\n12.07\n39.61\n57.16\n71.94\n76.41\n82.60\n▂▃▃▆▇\n\n\ngdpPercap\n0\n1\n11680.07\n12859.94\n277.55\n1624.84\n6124.37\n18008.84\n49357.19\n▇▂▁▂▁\n\n\n\n\n\nAhora que hemos explorado cada variable por separado, llega el momento de analizar las relaciones entre ellas. Este es el análisis bivariado, que nos ayuda a entender cómo interactúan dos variables a la vez ¿La expectativa de vida está relacionada con el PIB per cápita? ¿Cómo varía esta relación según el continente? Estas son preguntas que contestaremos a continuación.\n\n\n4.4.2 Análisis bivariado\nEn los análisis multivariados, el objetivo principal es estudiar las relaciones entre variables para comprender patrones y tendencias en los datos. Se dice que existe una asociación entre dos variables cuando los cambios en una están relacionados con los cambios en la otra. El análisis de la relación entre dos variables se conoce como análisis bivariado. En principio, la asociación es simétrica, ya que si X está asociada con Y, entonces Y también lo está con X. Sin embargo, en muchos enfoques analíticos, se asigna un rol específico a cada variable para facilitar la interpretación. En este caso, se distingue entre la variable respuesta, que representa el fenómeno que se desea estudiar, y la variable explicativa, que ayuda a entender cómo este resultado varía en función de sus valores, ya sea a través de categorías (si es categórica) o a lo largo de un rango (si es numérica). En este capítulo veremos como métodos descriptivos como la correlación ambas variables se tratan de manera equivalente mientras que en modelos inferenciales como la regresión se asume direccionalidad (8.1.4).\n\n\n\nElaboración propia\n\n\nEl análisis se centra en cómo el resultado de la variable de respuesta es explicado por el valor de la variable explicativa (Agresti 2018). Si no hay asociación entre las variables, se considera que son independientes, lo que significa que los valores de una variable no influyen en los valores de la otra, y los resultados se distribuyen de manera uniforme entre las categorías.\nBivariado numérico\nPara ilustrar métodos útiles cuando ambas variables son cuantitativas, uno de los enfoques más comunes es el cálculo de la correlación, que describe qué tan fuerte es la asociación entre dos variables en términos de qué tan cerca los datos siguen una tendencia lineal. La correlación nos proporciona una medida de la fuerza y la dirección de la relación entre las variables.\nEl coeficiente de correlación de Pearson es el más comúnmente utilizado y su valor puede oscilar entre -1 y 1. Un valor de 1 indica una correlación perfectamente positiva, lo que significa que a medida que una variable aumenta, la otra también lo hace de manera proporcional. Un valor de -1 indica una correlación perfectamente negativa, donde un aumento en una variable está asociado con una disminución proporcional en la otra. Un valor de 0 sugiere que no hay una relación lineal discernible entre las dos variables.\n\n\n\nExtraído de: https://www.simplypsychology.org/correlation.html\n\n\nPuedes interactuar con la correlación en la Datáfora Interactiva: [Click Aquí]\nO usa el QR:\n\n\n\nDirígete a la pestaña Correlación\n\n\nDebes ser consciente que la correlación solo captura relaciones lineales. Si la relación entre dos variables es no lineal, la correlación podría no reflejarla adecuadamente.\nEl paquete dlookr también puede ayudarnos a realizar este análisis bivariado de manera sencilla y visual.\nSeleccionamos las variables de interés y eliminamos cualquier columna innecesaria (en este caso, el año).\n\ngapminder_2007 = gapminder_2007 %&gt;% \n  select(-year)\n\nCon la función correlate() de dlookr, podemos obtener una tabla con los coeficientes de correlación para todas las combinaciones de variables cuantitativas en el conjunto de datos.\n\ncorrelate(gapminder_2007)\n\n# A tibble: 6 × 3\n  var1      var2      coef_corr\n  &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt;\n1 pop       lifeExp      0.0476\n2 gdpPercap lifeExp      0.679 \n3 lifeExp   pop          0.0476\n4 gdpPercap pop         -0.0557\n5 lifeExp   gdpPercap    0.679 \n6 pop       gdpPercap   -0.0557\n\n\nCon esto tendremos una matriz de correlaciones que muestra qué tan fuerte es la relación entre cada par de variables cuantitativas. Los coeficientes de correlación varían entre -1 y 1. Recuerda,, valores cercanos a 1 indican una fuerte relación positiva, valores cercanos a -1 indican una fuerte relación negativa, y valores cercanos a 0 indican poca o ninguna relación lineal.\nPara interpretar mejor los resultados, puedes utilizar la función plot_correlate() que genera un gráfico visual de las correlaciones entre las variables.\n\nplot_correlate(gapminder_2007)\n\n\n\n\n\n\n\n\nEl gráfico muestra un mapa de calor donde el color y la intensidad indican la fuerza de la correlación.\nLa mejor forma de poder visualizar la correlación es con un gráfico de dispersión. Utilicemos la relación entre PBI per cápita y esperanza de vida como hicimos anteriormente. Generalmente el eje x contiene la variable explicativa y el eje y la variable respuesta.\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point(color = '#2ecc71', size = 3, alpha = 0.6) +\n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n       x = 'PIB per cápita',\n       y = 'Esperanza de vida') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCon la función geom_smooth, podemos agregar una línea de tendencia que visualiza la relación lineal entre las dos variables. Mostrando cómo sería una correlación perfecta \\(r = 1\\). La correlación entre estas dos variables nos dio 0.68 por lo que tiene sentido que la mayoría de las observaciones se aproximen a la recta.\n\ngapminder_2007 %&gt;%\n  ggplot(aes(x = log(gdpPercap), y = lifeExp)) +\n  geom_point(color = '#2ecc71', size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", \n              color = \"red\", \n              se = FALSE, \n              linetype = \"dashed\", \n              size = 1) +\n  labs(title = 'Relación entre PIB per cápita y esperanza de vida (2007)',\n       x = 'PIB per cápita',\n       y = 'Esperanza de vida') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEs común encontrar estudios o titulares que afirman que “X causa Y”, cuando en realidad lo único que muestran es que ambas variables están correlacionadas. Sin embargo, es fundamental comprender que la correlación, por sí sola, no implica causalidad. Por poner un ejemplo absurdo, imaginemos que analizamos los datos de una ciudad y notamos una correlación fuerte entre dos variables, a medida que aumenta la venta de helado, también se incrementan los accidentes en la playa. ¿Significa esto que comer más helado aumenta el riesgo de sufrir un accidente en la playa? Por supuesto que no. Lo que realmente ocurre es que hay una variable oculta (lurking variable) detrás de esta relación: el verano. Cuando las temperaturas suben, más personas compran helado y, al mismo tiempo, más gente acude a la playa, lo que naturalmente incrementa el número de accidentes. Sin considerar esta tercera variable, podríamos llegar a una conclusión errónea. Y aunque este ejemplo pueda parecer exagerado, este tipo de relaciones engañosas son muy comunes y se conocen como correlaciones espurias. De hecho, muchas de ellas han sido documentadas de manera entretenida en la página Spurious Correlations, donde se encuentran ejemplos llamativos, como la aparente relación entre el consumo de queso y las muertes por enredarse en sábanas.\nLa correlación es una herramienta muy útil para identificar asociaciones entre variables, lo que puede ser clave para formular hipótesis o detectar patrones en los datos. La correlación nos permite entender cómo dos variables se relacionan entre sí, lo que en sí mismo tiene un gran valor analítico. El problema surge cuando la correlación se interpreta como evidencia de causalidad sin una evaluación adecuada. Confundir correlación con causalidad puede tener consecuencias serias cuando se reportan resultados sin el debido análisis. Este tipo de errores pueden influir en decisiones políticas, económicas o de salud pública, generando interpretaciones equivocadas de la realidad y como científicos sociales, debemos ser cautelosos al interpretar los datos. En el capítulo 9, exploraremos este tema en mayor profundidad.\nBivariado categórico: Tablas de contingencia\nUna herramienta útil para reportar la relación entre variables categóricas son las tablas de contingencia. Estas tablas muestran el número de observaciones en las combinaciones de resultados posibles entre dos variables, lo que permite visualizar cómo los resultados de una variable de respuesta dependen de las categorías de una variable explicativa.\nPor ejemplo, si estamos interesados en entender cómo se distribuyen los niveles de expectativa de vida entre diferentes continentes, una tabla de contingencia puede ser extremadamente útil. Esta tabla nos permite ver cuántos países en cada continente se agrupan en cada nivel de expectativa de vida (bajo, medio, alto).\nPodemos usar count para contar observaciones por las variables especificadas:\n\ntabla_cont = gapminder_2007 %&gt;%\n  count(continent, nivel)\n\ntabla_cont\n\n# A tibble: 10 × 3\n   continent nivel     n\n   &lt;fct&gt;     &lt;fct&gt; &lt;int&gt;\n 1 Africa    Baja     18\n 2 Africa    Media    27\n 3 Africa    Alta      7\n 4 Americas  Media     3\n 5 Americas  Alta     22\n 6 Asia      Baja      1\n 7 Asia      Media    10\n 8 Asia      Alta     22\n 9 Europe    Alta     30\n10 Oceania   Alta      2\n\n\nPara obtener una tabla de contingencia con frecuencias absolutas y relativas, puedes usar la función table() que usamos anteriormente:\n\ntabla_contingencia = table(gapminder_2007$continent, gapminder_2007$nivel)\n\n\ntabla_contingencia\n\n          \n           Baja Media Alta\n  Africa     18    27    7\n  Americas    0     3   22\n  Asia        1    10   22\n  Europe      0     0   30\n  Oceania     0     0    2\n\n\nPara obtener proporciones, usa prop.table() dentro de la tabla generada con table()\n\ntabla_proporciones = prop.table(tabla_contingencia)  \n\ntabla_proporciones\n\n          \n                  Baja       Media        Alta\n  Africa   0.126760563 0.190140845 0.049295775\n  Americas 0.000000000 0.021126761 0.154929577\n  Asia     0.007042254 0.070422535 0.154929577\n  Europe   0.000000000 0.000000000 0.211267606\n  Oceania  0.000000000 0.000000000 0.014084507\n\n\nAl observar la tabla de proporciones, cada celda muestra la proporción de observaciones que corresponden a una combinación particular de las dos variables. Esto nos da una idea de cómo se distribuyen las categorías de una variable en relación con las de la otra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "statdesc.html#flor-analiza-la-confianza-en-instituciones",
    "href": "statdesc.html#flor-analiza-la-confianza-en-instituciones",
    "title": "4  Estadística descriptiva",
    "section": "4.5 Flor analiza la confianza en instituciones",
    "text": "4.5 Flor analiza la confianza en instituciones\nFlor está interesada en comprender cómo la ciudadanía percibe y confía en las instituciones peruanas. Para ello, decide profundizar en los datos de Latinobarómetro 2023. El Latinobarómetro es un estudio de opinión pública que se realiza en 18 países de América Latina, con el objetivo de medir las actitudes, percepciones y comportamientos de la población sobre diversos temas sociales, políticos y económicos.\nDado su interés en analizar la confianza en las instituciones y su relación con factores socioeconómicos y políticos, Flor selecciona las variables más relevantes del conjunto de datos. Antes de comenzar el análisis, es fundamental leer el libro de códigos del Latinobarómetro, ya que proporciona información detallada sobre cada variable y las escalas de medición utilizadas.\nLa confianza en las instituciones se mide utilizando una escala de 1 a 4, donde valores más bajos indican mayor confianza y valores más altos reflejan mayor desconfianza. Flor selecciona las siguientes variables que capturan el nivel de confianza expresado por los ciudadanos en distintas instituciones peruanas:\n\nP13STGBS.A: Confianza en el Ejército\n\nP13STGBS.B: Confianza en la Policía\n\nP13STGBS.C: Confianza en la Iglesia\n\nP13STGBS.D: Confianza en el Congreso\n\nP13STGBS.E: Confianza en el Gobierno\n\nP13STGBS.F: Confianza en el Poder Judicial\n\nP13STGBS.G: Confianza en los Partidos Políticos\n\nP13STGBS.H: Confianza en la Institución Electoral\n\nP13STGBS.I: Confianza en el Presidente\n\nCada una de estas variables refleja el nivel de confianza en una institución específica. Un valor de 1 significa “mucha confianza”, mientras que un valor de 4 representa “ninguna confianza”.\nAdemás de la confianza en las instituciones, Flor también incorpora variables que reflejan la percepción ciudadana sobre la economía y el sistema político. Para ello, selecciona las siguientes variables:\n\nP5STGBS: Situación económica actual, que evalúa cómo perciben los ciudadanos la economía del país en el momento de la encuesta.\n\nP6STGBS: Comparación de la economía con el año anterior, que indica si los encuestados consideran que la economía ha mejorado o empeorado en comparación con el año anterior.\n\nP11STGBS.A: Satisfacción con la democracia, que mide el nivel de satisfacción de los ciudadanos con el sistema democrático.\n\nP2ST: Percepción del progreso del país, que refleja si los encuestados consideran que el país está progresando, estancado o en retroceso.\n\nPor último, Flor incorpora variables sociodemográficas, que le permitirán analizar cómo la confianza en las instituciones varía según características personales de los encuestados. Considera que es relevante observar diferencias según género y edad. Para ello, selecciona:\n\nSEXO: Género del encuestado, codificado como 1 para hombres y 2 para mujeres.\n\nEDAD: Edad del encuestado en años.\n\nLo primero que hace Flor es cargar las librerías necesarias, que incluyen herramientas para manipulación de datos (dplyr), visualización (ggplot2), resumen (dlookr) e importación de datos (readr).\n\n# Carga paquetes necesarios\nlibrary(readr) # Importación\nlibrary(dplyr)    # Para manipulación de datos\nlibrary(ggplot2)  # Para visualización de datos\nlibrary(dlookr)   # Para EDA \n\nLuego, importa el conjunto de datos\n\n# Importa la base de datos \nlatb2023 = read_csv(\"latb2023.csv\")\n\nFlor selecciona las variables seleccionadas para analizar la confianza en instituciones.\n\n# Filtra datos de Perú y selecciona variables clave\nconfianza = latb2023 %&gt;%\n  # Filtra solo las respuestas de Perú (Código 604)\n  filter(IDENPA == 604) %&gt;%  \n         # Variables sociodemográficas\n  select(SEXO, EDAD,        \n         # Confianza en instituciones\n         P13STGBS.A:P13ST.I,\n         # Variables económicas\n         P5STGBS, P6STGBS,  \n        # Opinión sobre democracia y progreso del país\n         P11STGBS.A, P2ST)  \n\nComprueba el nuevo data.frame y sus variables\n\nglimpse(confianza)\n\nRows: 1,200\nColumns: 15\n$ SEXO       &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2,…\n$ EDAD       &lt;dbl&gt; 19, 20, 64, 69, 68, 54, 26, 30, 58, 30, 22, 60, 57, 62, 51,…\n$ P13STGBS.A &lt;dbl&gt; 1, 2, 3, 4, 3, 3, 3, 3, 3, 1, 4, 1, 3, 4, 3, 1, 2, 3, 2, 2,…\n$ P13STGBS.B &lt;dbl&gt; 1, 2, 4, 4, 3, 3, 4, 3, 3, 1, 4, 1, 2, 3, 3, 3, 4, 3, 3, 2,…\n$ P13ST.C    &lt;dbl&gt; 2, 2, 4, 4, 2, 3, 1, 1, 2, 3, 3, 1, 2, 1, 3, 3, 2, 4, 2, 3,…\n$ P13ST.D    &lt;dbl&gt; 3, 3, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 4,…\n$ P13ST.E    &lt;dbl&gt; 3, 3, 4, 4, 4, 4, 4, 3, 3, 3, 4, 1, 3, 3, 4, 2, 3, 3, 3, 4,…\n$ P13ST.F    &lt;dbl&gt; 3, 2, 3, 4, 4, 4, 4, 3, 2, 3, 4, 1, 3, 2, 4, 2, 3, 1, 3, 3,…\n$ P13ST.G    &lt;dbl&gt; 3, 3, 4, 4, 4, 4, 4, 3, 2, 3, 4, 3, 4, 3, 4, 4, 2, 3, 3, 4,…\n$ P13ST.H    &lt;dbl&gt; 3, 3, NA, 2, 3, 2, 4, 2, 2, 1, 4, 4, 3, 3, 4, 3, 2, 3, 2, 2…\n$ P13ST.I    &lt;dbl&gt; 3, 3, 4, 4, 4, 4, 4, 3, 2, 3, 4, 1, 4, 3, 4, 2, 3, 4, 3, 3,…\n$ P5STGBS    &lt;dbl&gt; 3, 3, 4, 3, 4, 5, 3, 3, 3, 2, 5, 3, 4, 3, 4, 4, 4, 4, 2, 4,…\n$ P6STGBS    &lt;dbl&gt; 4, 4, 5, 4, 5, 5, 4, 2, 5, 4, 5, 2, 4, 4, 3, 3, 5, 5, 2, 3,…\n$ P11STGBS.A &lt;dbl&gt; 3, 3, 4, 3, 4, 4, 4, 3, 3, 2, 4, 1, 4, 3, 3, 3, 3, 3, 2, 3,…\n$ P2ST       &lt;dbl&gt; 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 2,…\n\n\nEn total son 1200 observacioes y 15 variables las que está tomando en cuenta.\nFlor se da cuenta de que las variables tienen nombres poco intuitivos. Por lo tanto, decide renombrarlas con nombres más descriptivos, para ello puede hacer uso de rename.\nPrimero las de confianza\n\n# Renombra variables de confianza en instituciones\nconfianza = confianza %&gt;%\n  rename(\n    confianza_ejercito = P13STGBS.A,\n    confianza_policia = P13STGBS.B,\n    confianza_iglesia = P13ST.C,\n    confianza_congreso = P13ST.D,\n    confianza_gobierno = P13ST.E,\n    confianza_judicial = P13ST.F,\n    confianza_partidos = P13ST.G,\n    confianza_electoral = P13ST.H,\n    confianza_presidente = P13ST.I\n  )\n\nLuego la demás\n\n# Renombra variables de percepción económica\nconfianza = confianza %&gt;%\n  rename(\n    situacion_economica = P5STGBS,    \n    economia_vs_anterior = P6STGBS    \n  )\n\n# Renombra variables de percepción política\nconfianza = confianza %&gt;%\n  rename(\n    satisfaccion_democracia = P11STGBS.A,  \n    percepcion_progreso = P2ST             \n  )\n\n# Renombra variables sociodemográficas\nconfianza = confianza %&gt;%\n  rename(\n    sexo = SEXO,  \n    edad = EDAD     \n  )\n\nAntes de calcular estadísticas descriptivas, Flor elimina las filas con valores faltantes (NA)\n\n# Elimina valores NA\nconfianza = confianza %&gt;%\n  drop_na()\n\n\ndim(confianza)\n\n[1] 1147   15\n\n\nLe quedan 1147 observaciones.\nCon los datos limpios y estructurados, Flor inicia la exploración de las variables relacionadas con la confianza en las instituciones. Para ello, calcula estadísticas descriptivas que le permitan obtener un panorama general sobre la confianza que tienen los ciudadanos en cada institución.\nLas medidas que considera incluyen:\n\nMEDIA Y MEDIANA: Para conocer el nivel promedio de confianza y el valor central en la distribución.\n\nDESVIACIÓN ESTÁNDAR: Para entender cuánto varían las respuestas respecto a la media.\n\nASIMETRÍA (SKEWNESS): Para determinar si la distribución de las respuestas está sesgada hacia valores de mayor o menor confianza.\n\nCURTOSIS: Para identificar si las respuestas están concentradas en torno a la media o si hay una mayor dispersión en los datos.\n\n\n# Resumen estadístico de confianza en instituciones\nconfianza %&gt;% \n# Selecciona las variables de confianza  \n  select(starts_with(\"confianza_\")) %&gt;% \n  describe() %&gt;% \n  select(described_variables, mean, p50, sd, skewness, kurtosis) %&gt;% \n# Los ordena por orden de confianza\n    arrange(mean)\n\n# A tibble: 9 × 6\n  described_variables   mean   p50    sd skewness kurtosis\n  &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 confianza_iglesia     2.23     2 1.07     0.292  -1.19  \n2 confianza_ejercito    2.60     3 1.04    -0.123  -1.16  \n3 confianza_policia     2.88     3 0.930   -0.401  -0.754 \n4 confianza_electoral   2.96     3 0.885   -0.367  -0.801 \n5 confianza_judicial    3.28     3 0.848   -0.957   0.0600\n6 confianza_presidente  3.48     4 0.752   -1.28    0.770 \n7 confianza_gobierno    3.51     4 0.708   -1.37    1.34  \n8 confianza_partidos    3.57     4 0.689   -1.65    2.48  \n9 confianza_congreso    3.64     4 0.615   -1.80    3.27  \n\n\nFlor puede entender parcialmente el comportamiento de sus variables con estas mediciones. Encuentra que, por ejemplo, la Iglesia es la institución con mayor confianza, con una MEDIA de 2.23 y una MEDIANA de 2, lo que indica que la mayoría de los encuestados le otorgan una valoración positiva. Le sigue el Ejército, con una MEDIA de 2.60. En contraste, las instituciones con menor confianza son los Partidos Políticos y el Congreso, con MEDIAS de 3.57 y 3.64, y una MEDIANA de 4, lo que indica que la mayoría de los encuestados los percibe con altos niveles de desconfianza.\nSin embargo, la dispersión de las respuestas es mayor en la Iglesia, lo que se refleja en su DESVIACIÓN ESTÁNDAR de 1.07, sugiriendo opiniones más variadas sobre su nivel de confianza. En contraste, el Congreso y los Partidos Políticos presentan las valoraciones más homogéneas, con DESVIACIONES ESTÁNDAR de 0.615 y 0.689, respectivamente, lo que indica que la percepción de desconfianza es más uniforme.\nLa confianza en el Congreso y en los Partidos políticos presenta la mayor ASIMETRÍA NEGATIVA, con valores de -1.80 y -1.65 respectivamente, lo que sugiere una distribución sesgada hacia los valores más bajos de la escala. Esto implica que una proporción considerable de encuestados tiende a asignarles niveles reducidos de confianza. En cuanto a la CURTOSIS, los valores más altos se observan también en el Congreso (3.27) y en los Partidos políticos (2.48), lo cual indica una mayor concentración de respuestas en torno a los extremos de la escala, con una menor presencia de valoraciones intermedias.\nAhora Flor quiere observar con más detalle cómo se distribuyen los niveles de confianza en algunas de las instituciones que le llamaron más la atención. Hasta ahora, ha tratado la variable como numérica discreta, lo que le ha permitido calcular medidas como la MEDIA, la MEDIANA y la DESVIACIÓN ESTÁNDAR. Sin embargo, esta variable también es CATEGÓRICA ORDINAL, ya que representa niveles de confianza ordenados de mayor a menor. Flor utilizará GRÁFICOS DE BARRAS para visualizar la distribución de la confianza en cada institución.\nComo la confianza en la Iglesia\n\n# Histograma de confianza en la Iglesia\nggplot(confianza, aes(x = confianza_iglesia)) +\n  geom_bar(fill = \"#1F618D\", \n           color = \"black\") +\n  labs(title = \"Distribución de confianza en la Iglesia\", \n       x = \"Nivel de confianza\", \n       y = \"Frecuencia\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nO en el Congreso\n\n# Histograma de confianza en el Congreso\nggplot(confianza, aes(x = confianza_congreso)) +\n  geom_bar(fill = \"#1F618D\", \n           color = \"black\") +\n  labs(title = \"Distribución de confianza en el Gobierno\", \n       x = \"Nivel de confianza\", \n       y = \"Frecuencia\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nMás allá de conocer la distribución de la confianza en las instituciones, Flor se pregunta: ¿Existen factores que expliquen por qué algunas personas confían más en el Gobierno o en el Congreso? Para responder a esta pregunta, decide analizar si la satisfacción con la democracia influye en el nivel de confianza que las personas depositan en el Gobierno.\nPara abordar esta cuestión, Flor agrupa los datos de confianza en el Gobierno según el nivel de satisfacción con la democracia. El cual pregunta “En general, ¿diría usted que está muy satisfecho, más bien satisfecho, no muy satisfecho o nada satisfecho con el funcionamiento de la democracia en su país?”\nLas respuestas posibles están codificadas de la siguiente manera:\n\n1 = Muy satisfecho\n2 = Más bien satisfecho\n3 = No muy satisfecho\n4 = Nada satisfecho\n\nPor lo que Flor, calcula el promedio de confianza dentro de cada grupo. Esto le permite observar si aquellos que se sienten más satisfechos con la democracia tienden a confiar más en el Gobierno o, por el contrario, si el descontento democrático está asociado con una mayor desconfianza en esta institución.\n\nconfianza %&gt;%\n  # Agrupa por satisfacción con la democracia\n  group_by(satisfaccion_democracia) %&gt;%  \n  # Calcula la media de confianza en el Gobierno\n  reframe(media_confianza_gobierno = \n            mean(confianza_gobierno))  \n\n# A tibble: 4 × 2\n  satisfaccion_democracia media_confianza_gobierno\n                    &lt;dbl&gt;                    &lt;dbl&gt;\n1                       1                     2.85\n2                       2                     3.10\n3                       3                     3.40\n4                       4                     3.75\n\n\nFlor puede observar que, a medida que la satisfacción con la democracia aumenta, también lo hace la desconfianza en el Gobierno (recordemos que en la escala de confianza, un número mayor indica menor confianza).\nAquellos que expresan el nivel más bajo de satisfacción con la democracia (1) tienen un promedio de 2.85 en confianza en el Gobierno, lo que sugiere que su nivel de desconfianza es menor en comparación con otros grupos. Conforme la satisfacción con la democracia aumenta a nivel 2 y 3, el promedio de confianza en el Gobierno se incrementa a 3.10 y 3.40, respectivamente, lo que indica que la desconfianza es cada vez mayor. Además, quienes reportan estar más satisfechos con la democracia (nivel 4) presentan el nivel más alto de desconfianza en el Gobierno, con un promedio de 3.75.\nPara visualizar mejor esta relación, Flor crea un gráfico de barras donde cada barra representa el promedio de confianza en el Gobierno según el nivel de satisfacción con la democracia. Se asegura de mejorar la estética del gráfico eliminando la leyenda, ya que el eje X ya indica claramente las categorías, y utiliza una escala de colores para facilitar la interpretación.\n\nggplot(confianza, aes(x = satisfaccion_democracia, \n                      y = confianza_gobierno, \n                      # Usa colores diferentes para cada nivel de satisfacción\n                      fill = as.factor(satisfaccion_democracia))) +  \n  # Representa la media de confianza en el Gobierno como barras\n  geom_bar(stat = \"summary\", fun = \"mean\") +  \n  # Escala de satisfacción \n  scale_fill_manual(values = \n                      c(\"#2ECC71\", \"#ABEBC6\", \"#E67E22\", \"#C0392B\")) +  \n  labs(title = \"Confianza en el Gobierno según Satisfacción con la Democracia\", \n       x = \"Satisfacción con la Democracia\", \n       y = \"Confianza\") +\n  theme_minimal() +  \n  # Elimina la leyenda para evitar redundancia\n  theme(legend.position = \"none\")  \n\n\n\n\n\n\n\n\nAl examinar el gráfico, Flor nota que a medida que aumenta la satisfacción con la democracia, también lo hace la confianza en el Gobierno. Aquellos que se declaran muy satisfechos con la democracia presentan, en promedio, una mayor confianza en el Gobierno, mientras que quienes están menos satisfechos tienden a mostrar niveles más altos de desconfianza. Flor sugiere una asociación en la que la percepción sobre el funcionamiento democrático parece estar vinculada al nivel de confianza en el Gobierno.\nEste hallazgo la lleva a preguntarse si este patrón se repite con otras instituciones o si el Gobierno es un caso particular. Para ello, decide expandir su análisis. Flor también está interesada en explorar cómo la percepción económica influye en la confianza en el Congreso.\nAhora, se enfoca en la variable situacion_economica. Esta variable captura la evaluación de la ciudadanía sobre la situación económica del país y está codificada de la siguiente manera:\n\n1 = Muy buena\n2 = Buena\n3 = Ni buena ni mala\n4 = Mala\n5 = Muy mala\n\nCalcula la media de confianza en el Congreso según la percepción económica de los encuestados.\n\nconfianza %&gt;%\n  group_by(situacion_economica) %&gt;%\n  reframe(media_confianza_congreso = \n            mean(confianza_congreso))\n\n# A tibble: 5 × 2\n  situacion_economica media_confianza_congreso\n                &lt;dbl&gt;                    &lt;dbl&gt;\n1                   1                     3.09\n2                   2                     3.17\n3                   3                     3.55\n4                   4                     3.78\n5                   5                     3.79\n\n\nAl organizar los datos de esta manera, nota una tendencia similar: a medida que la percepción de la situación económica empeora, el promedio de confianza en el Congreso tiende a aumentar en valor, lo que indica una mayor desconfianza.\nPara visualizar mejor esta relación, genera un gráfico de barras apiladas donde la confianza en el Congreso se descompone según la percepción económica.\n\nggplot(confianza, aes(x = as.factor(confianza_congreso), \n                      fill = as.factor(situacion_economica))) +\n  # Usa \"fill\" para apilar las barras proporcionalmente\n  geom_bar(position = \"fill\") +  \n  scale_y_continuous(labels = \n                       # Mostrar en porcentaje\n                       scales::percent_format(accuracy = 1)) +  \n  scale_fill_manual(values = \n                       # Verde a rojo\n                       c(\"#2ECC71\", \"#ABEBC6\", \"#F1C40F\", \"#E67E22\", \"#C0392B\")) +  \n  labs(title = \"Confianza en el Congreso según Situación Económica\",\n       x = \"Confianza en el Congreso\",\n       y = \"Proporción\",\n       fill = \"Situación Económica\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLas personas con una mejor percepción de la economía tienen una distribución de confianza más baja (es decir, mayor confianza en el Congreso), mientras que aquellas que evalúan negativamente la economía muestran una mayor concentración en los niveles más altos de desconfianza.\n\n\n\nLa estadítica descriptiva nos permite entender el comportamiento de nuestras variables",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "statdesc.html#resumen-del-capítulo",
    "href": "statdesc.html#resumen-del-capítulo",
    "title": "4  Estadística descriptiva",
    "section": "4.6 Resumen del capítulo",
    "text": "4.6 Resumen del capítulo\nLa estadística descriptiva constituye el primer paso para entender los datos y determinar patrones relevantes antes de aplicar técnicas inferenciales o modelos predictivos. En R, este proceso se desarrolla sobre estructuras tabulares como data.frame o tibble, haciendo uso de herramientas del ecosistema tidyverse y funciones adicionales de paquetes como dlookr y skimr.\nEl análisis comienza con la identificación de los tipos de variables, ya sean numéricas (continuas o discretas) o categóricas (nominales, ordinales o binarias). Esta clasificación permite seleccionar las medidas adecuadas para describirlas. Para variables numéricas se utilizan medidas de tendencia central como media y mediana, medidas de dispersión como rango, desviación estándar y rango intercuartílico, así como medidas de forma como la asimetría y la curtosis. Se incorporan funciones como mean(), median(), sd(), IQR(), skewness() y kurtosis() para obtener estos estadísticos y representarlos gráficamente.\nLas variables categóricas se resumen a través de frecuencias absolutas (table()) y proporciones relativas (prop.table()), identificando la moda como la categoría más frecuente. La visualización se apoya en gráficos de barras, torta y mosaico para reflejar la distribución y la asociación entre categorías.\nEn este capítulo hemos introducido también el análisis exploratorio de datos (EDA), que articula estadísticas numéricas y visuales para comprender la estructura general del dataset. La función describe() del paquete dlookr permite obtener un resumen detallado para cada variable, incluyendo percentiles, asimetría y curtosis. Se extiende el análisis con skim() para descripciones simultáneas de variables numéricas y categóricas.\nEn el análisis univariado, se describe cada variable por separado. En el bivariado, se exploran asociaciones entre pares de variables. Para variables numéricas, se evalúa la correlación de Pearson, reportada con correlate() y visualizada mediante plot_correlate() o gráficos de dispersión con línea de tendencia (geom_smooth(method = \"lm\")). Es sumamente importante recordar que la correlación no implica causalidad. Para variables categóricas, se construyen tablas de contingencia que cuantifican la coocurrencia entre categorías y permiten evaluar relaciones mediante frecuencias cruzadas.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "statdesc.html#ejercicios",
    "href": "statdesc.html#ejercicios",
    "title": "4  Estadística descriptiva",
    "section": "4.7 Ejercicios",
    "text": "4.7 Ejercicios\n1.Flor analiza la confianza en el Gobierno en Perú. La variable confianza_gobierno está codificada en una escala de 1 a 4, donde valores menores indican mayor confianza.\n\nconfianza %&gt;% \n  select(confianza_gobierno) %&gt;% \n  describe() %&gt;% \n  select(described_variables, mean, p50, sd) \n\n# A tibble: 1 × 4\n  described_variables  mean   p50    sd\n  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 confianza_gobierno   3.51     4 0.708\n\n\n¿Cual es la media de la variable confianza_gobierno y cual es una correcta interpretación?\na) La media de la confianza en el Gobierno es 3.51, lo que indica que la percepción de confianza es alta, ya que los valores más altos en la escala representan mayor confianza y la mayoría de los encuestados se inclinó hacia ellos.\nb) La media de la confianza en el Gobierno es 4, lo que sugiere que las opiniones están divididas, pero con una tendencia hacia la confianza.\nc) La media de la confianza en el Gobierno es 4, lo que implica que la percepción de confianza en el Gobierno es neutral, ya que el valor está cerca del punto medio de la escala.\nd) La media de la confianza en el Gobierno es 3.51, lo que indica que la percepción de confianza es baja, ya que los valores más altos en la escala representan mayor desconfianza y la mayoría de los encuestados se inclinó hacia ellos.\n2.Un estudio sobre ingresos en una comunidad reporta los siguientes valores:\n\nMedia: $2,500\n\nMediana: $1,800\n\nDesviación estándar: $800\n\n¿Qué se puede inferir sobre la distribución de los ingresos?\na) La media es mayor que la mediana, lo que sugiere una distribución sesgada a la derecha con valores extremos altos.\n\nb) La mediana es mayor que la media, lo que indica una distribución sesgada a la izquierda.\n\nc) La desviación estándar baja indica que los ingresos son homogéneos.\n\nd) La media y la mediana similares sugieren que la distribución es simétrica.\n3.En una encuesta sobre el acceso a Internet, se encontró que el percentil 25 de horas semanales en línea es 5 horas, la mediana (percentil 50) es 12 horas, y el percentil 75 es 20 horas.\n¿Qué se puede inferir sobre el comportamiento de la población encuestada?\na) La mayoría de las personas usa Internet más de 20 horas a la semana.\n\nb) El 25% de la población usa Internet menos de 5 horas a la semana.\n\nc) El 50% de los encuestados usa Internet más de 20 horas a la semana.\n\nd) Todos los encuestados pasan entre 5 y 20 horas en línea.\n4.En un estudio sobre percepción de inseguridad en una ciudad, la asimetría de la variable nivel_de_inseguridad es de -1.7 y la curtosis es de 5.2.\n¿Cómo se interpreta esta información?\na) La distribución es simétrica y mesocúrtica.\n\nb) La distribución tiene sesgo negativo y es más apuntada que una normal.\n\nc) La distribución es sesgada positivamente y con colas más pesadas.\n\nd)La curtosis indica que la distribución es más plana que una normal.\n5.En un análisis de satisfacción con el sistema de salud, se reportaron los siguientes resultados en una escala de 1 a 10:\n\n\n\nNivel de ingresos\nPromedio de satisfacción\n\n\n\n\nBajo\n5.2\n\n\nMedio\n6.1\n\n\nAlto\n7.3\n\n\n\n¿Qué se puede concluir sobre la asociación entre ingresos y satisfacción con el sistema de salud?\na) Las personas con mayores ingresos tienden a reportar mayor satisfacción.\n\nb)El nivel de ingresos no tiene relación con la satisfacción en salud.\n\nc)Las personas con ingresos bajos son las más satisfechas con el sistema de salud.\n\nd)Todos los niveles de ingresos tienen la misma percepción sobre el sistema de salud.\n6.Flor quiere explorar la variable edad en el conjunto de datos de Latinobarómetro y obtener un resumen completo de sus estadísticas descriptivas.\n¿Cuál de las siguientes líneas de código en R generaría correctamente un EDA con dlookr sobre esta variable?\na)\ndescribe(confianza$edad)\nb)\ntable(confianza$edad)\nc)\nmedian(confianza$edad)\nd)\nhist(confianza$edad)\n7.En un estudio sobre la cantidad de libros leídos por año, se obtuvo:\n\nMedia: 8 libros\n\nDesviación estándar: 3 libros\n\nSi una persona reportó haber leído 14 libros, ¿cuántas desviaciones estándar está por encima de la media?\na) 1 desviación estándar\n\nb) 1.5 desviaciones estándar\n\nc) 2 desviaciones estándar\n\nd) 3 desviaciones estándar\n8.En un análisis de voto por género, se reportó la siguiente distribución entre tres candidatos:\n\n\n\nGénero\nCandidato A\nCandidato B\nCandidato C\n\n\n\n\nHombres\n45%\n30%\n25%\n\n\nMujeres\n35%\n50%\n15%\n\n\n\n¿Qué gráfico sería más adecuado para visualizar estos datos?\na) Histograma\n\nb) Gráfico de barras apiladas\n\nc) Gráfico de dispersión\n\nd) Boxplot\n9.En un estudio sobre la edad y el tiempo dedicado al uso de redes sociales, se obtuvo un coeficiente de correlación de -0.72.\n¿Qué se puede decir sobre la asociación entre edad y uso de redes sociales?\na) Las personas mayores tienden a pasar más tiempo en redes sociales.\n\nb) No hay asociación entre la edad y el uso de redes sociales.\n\nc) Las personas mayores tienden a pasar menos tiempo en redes sociales.\n\nd) La edad no es un factor que afecte el uso de redes sociales.\n10.Se analiza la correlación entre la participación política y otros factores en una comunidad:\n\n\n\n\n\n\n\n\n\nVariable\nNivel educativo\nIngreso mensual\nFrecuencia de consumo de noticias\n\n\n\n\nParticipación política\n0.65\n0.48\n0.72\n\n\n\n¿Qué se puede concluir de estos datos?\na) El nivel educativo no está asociado con la participación política.\n\nb) El consumo de noticias muestra la asociación más fuerte con la participación política.\n\nc) El ingreso mensual es la variable más fuertemente asociada con la participación política.\n\nd) No se puede determinar ninguna relación a partir de estos datos.\n11.Se ha recolectado información sobre la distribución del tiempo que las personas dedican a actividades recreativas semanales. Se desea visualizar la distribución de esta variable.\n¿Cuál de los siguientes gráficos sería el más adecuado para representar la distribución del tiempo dedicado a actividades recreativas?\na) Gráfico de dispersión\n\nb) Gráfico de barras\n\nc) Diagrama de mosaico\n\nd) Histograma\n12.Se está analizando la relación entre el nivel educativo y la preferencia por distintos tipos de medios de comunicación. Se tiene una variable categórica nivel_educativo (bajo, medio, alto) y una variable categórica medio_preferido (TV, radio, redes sociales, prensa escrita).\n¿Cuál de los siguientes gráficos es el más adecuado para visualizar la relación entre ambas variables?\na) Gráfico de dispersión\n\nb) Boxplot\n\nc) Diagrama de mosaico\n\nd) Histograma\n\n\n\n\nAgresti, Alan. 2018. Statistical methods for the social sciences. Fifth edition. Boston: Pearson.\n\n\nBrunson, Jason Cory. 2020. ggalluvial: Alluvial Plots in ’ggplot2’. https://CRAN.R-project.org/package=ggalluvial.\n\n\nRyu, Choonghyun. 2024. «dlookr: Tools for Data Diagnosis, Exploration, Transformation». https://CRAN.R-project.org/package=dlookr.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "5  Probabilidad",
    "section": "",
    "text": "5.1 Fundamentos de Probabilidad\nPara analizar datos con criterio estadístico y formular conclusiones que vayan más allá de lo observado, es fundamental comprender qué implica hablar de probabilidad. Aunque su origen precede al desarrollo formal de la estadística, la probabilidad se ha convertido en uno de los pilares fundamentales para interpretar fenómenos aleatorios, cuantificar la incertidumbre y construir modelos que permitan realizar inferencias válidas a partir de una muestra.\nEn su libro Probability, Statistics, and Data: A Fresh Approach Using R, Speegle y Clair (2021) proponen una definición precisa:\nEste enfoque es frecuentista, es decir: entiende la probabilidad como un valor que resume lo que se espera en el largo plazo si un experimento se repite muchas veces.\nDe ahí que presenten tres axiomas fundamentales que toda probabilidad válida debe cumplir:\nSin embargo, en un texto introductorio como este, lo más valioso es adoptar una forma intuitiva y operativa de pensar la probabilidad que sirva como base. En ese sentido, sugiero partamos de una noción algo más accesible: la probabilidad como proporción. Esta idea sugiere que hablar de la probabilidad de un evento es cuantificar cuán probable es su ocurrencia, comparada con todos los resultados posibles. Al lanzar una moneda, la probabilidad de obtener sello es de \\(\\frac{1}{2}\\), lo que equivale a 0.5 o al 50%. Esta proporción puede visualizarse como una fracción de veces que se espera que ocurra el evento a lo largo de múltiples repeticiones del experimento.\nPodemos visualizar la probabilidad de este evento como una barra que representa el resultado del la moneda que nos interesa, en relación con el total de resultados posibles.\nAmbas barras representan la proporción del total de posibles resultados. En términos gráficos, la altura de la barra muestra la probabilidad de que ocurra cada resultado.\nAquí es donde el concepto de área tiene relevancia. La probabilidad de cada número puede visualizarse como el área de la barra en un gráfico, en comparación con el área total que representan todos los posibles resultados. Para la moneda, la probabilidad de obtener un sello es \\(\\frac{1}{2}\\), lo que significa que la barra para el resultado sello ocupa la mitad del área total del gráfico.\nPero, ¿qué pasa cuando no estamos tratando con eventos discretos, como el lanzamiento de una modena, sino con variables que pueden tomar un rango infinito de valores? Por ejemplo, si quisiéramos saber la probabilidad de que la altura de una persona esté entre 160 cm y 170 cm, no podemos representar esta situación con barras, porque los valores posibles son infinitos (puede haber alturas de 160.1 cm, 160.2 cm, etc.).\nEn estos casos, la probabilidad la representamos con una curva. Al igual que con las barras, el área bajo la curva nos muestra la probabilidad, pero en este caso, la probabilidad se calcula para un rango de valores. La idea sigue siendo la misma: la probabilidad es el área, pero ahora estamos mirando el área bajo una curva en lugar de barras separadas.\nPor ejemplo, si estamos interesados en la probabilidad de que una altura esté entre 160 cm y 170 cm, esa probabilidad sería el área bajo la curva entre esos dos puntos. Mientras que en el caso de las barras sabíamos exactamente cuántos resultados posibles había, en el caso de la curva no tenemos un número fijo de resultados, sino un rango, y el área debajo de ese rango nos da la probabilidad de que el valor caiga ahí.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "prob.html#fundamentos-de-probabilidad",
    "href": "prob.html#fundamentos-de-probabilidad",
    "title": "5  Probabilidad",
    "section": "",
    "text": "“La probabilidad de un evento es un número entre cero y uno que describe la proporción de veces que se espera que dicho evento ocurra.”\n\n\n\n\nNo negatividad: Para cualquier evento \\(E\\), su probabilidad es mayor o igual que cero: \\(P(E) \\geq 0\\).\nTotalidad: La probabilidad del espacio muestral completo es uno: \\(P(S) = 1\\).\nAditividad: Si \\(A_1, A_2, \\dots\\) son eventos mutuamente excluyentes (es decir, no pueden ocurrir al mismo tiempo), entonces la probabilidad de su unión es la suma de sus probabilidades individuales:\n\\[P\\left( \\bigcup_{n=1}^{\\infty} A_n \\right) = \\sum_{n=1}^{\\infty} P(A_n)\\]\n\n\n\n\n\n\n\n\n\n\n\n5.1.1 La probabilidad como frecuencia relativa a largo plazo\nEn situaciones sencillas, el proceso es completamente conocido y la probabilidad puede estimarse de forma intuitiva. Imaginemos un escenario básico: un dado de seis caras numeradas del 1 al 6, y el objetivo es calcular la probabilidad de obtener un 3 al lanzarlo.\nSabemos que el dado tiene seis caras y que cada una tiene la misma probabilidad de aparecer, asumiendo que no está cargado. Si definimos el evento de interés como “obtener un 3”, entonces existe una única cara favorable entre seis posibles resultados.\nPor tanto, la probabilidad de obtener un 3 en un solo lanzamiento de dado se puede calcular como:\n\\[\nP(\\text{Sacar un 3}) = \\frac{\\text{Número de resultados favorables}}{\\text{Número total de resultados posibles}} = \\frac{1}{6}\n\\]\nEs decir\n\\[P(3) = \\frac{1}{6} \\approx 0.1667\\]\nEsto significa que, sin necesidad de hacer un solo lanzamiento, ya sabemos que la probabilidad de sacar un 3 es \\(\\frac{1}{6}\\) o aproximadamente 16.67%.\nLo importante aquí es entender que estamos trabajando con un sistema en el que todas las posibles opciones son igualmente probables y están perfectamente definidas. En el caso de un dado de seis caras, ya conocemos todos los posibles resultados y sabemos que no hay ninguna razón para que una cara sea más probable que otra. De la misma forma se puede aplicar a otros casos como:\n\nSi lanzas una moneda, hay dos posibles resultados (cara o cruz), por lo que la probabilidad de obtener “cara” es \\(\\frac{1}{2}\\).\nSi sacas una carta de una baraja estándar de 52 cartas, la probabilidad de obtener un as es \\(\\frac{4}{52}\\), porque hay 4 ases en total entre las 52 cartas posibles.\n\nEn algunos casos, calcular la probabilidad teórica puede ser bastante sencillo, siempre que sepamos con certeza cuáles son todos los resultados posibles y que estos tengan probabilidades conocidas o igual de probables.\nVolviendo al ejemplo de los dados, sabemos que tiene seis caras y que cada una de ellas tiene la misma probabilidad de salir. Esto significa que, en promedio, uno de cada seis lanzamientos del dado debería resultar en un 3. Esta es la expectativa teórica que calculamos partiendo del hecho de que conocemos el proceso fisico en su totalidad.\nSin embargo, en la vida real, y especialmente en áreas como las ciencias sociales, muchas veces no tenemos claro cuáles son todos los posibles resultados o no sabemos con precisión sus probabilidades. En estos casos, no podemos calcular directamente la probabilidad teórica porque nos falta información. Debemos encontrar la forma de aproximarnos a ella con la probabilidad experimental\nImagina que tienes dudas sobre si el dado es justo o está trucado. Tal vez sospechas que el dado está manipulado para que el 3 salga con más frecuencia que los otros números. En este caso, no puedes simplemente asumir que la probabilidad de sacar un 3 es \\(\\frac{1}{6}\\), porque no sabes con certeza si todas las caras tienen la misma probabilidad de aparecer. Aquí, la probabilidad teórica se vuelve difícil de calcular, porque no conoces las probabilidades exactas de cada cara. Podemos realizar un experimento y usar la probabilidad experimental para averiguar si el dado realmente está trucado o no.\nLa probabilidad experimental te permite aproximar lo que podría ser la probabilidad real observando los resultados de muchos lanzamientos del dado. Si lanzas el dado una sola vez y obtienes un 3, no puedes concluir nada. Pero, si lanzas el dado muchas veces, puedes empezar a contar cuántas veces sale cada número y aproximar la probabilidad de que salga un 3.\nPodemos empezar con algo sencillo: simular 10 lanzamientos de un dado y ver cuántas veces sale un 3. Para hacerlo, podemos utilizar la función sample, que nos permite generar valores aleatorios dentro de un rango. Usamos set.seed() para establecer un punto de partida fijo en la generación de números aleatorios, de no poner esto se generarían nuevos números aleatorios cada vez que corramos el código. Y finalmente usamos replace = TRUE para permitir que cada lanzamiento del dado sea independiente, permitiendo que cada número (1 a 6) pueda repetirse, tal como ocurre al lanzar un dado real en cada tirada.\n\n# Fijamos una semilla para asegurar que los resultados sean reproducibles\nset.seed(123)\n\n# Simulamos 10 lanzamientos de un dado\nlanzamientos_10 = sample(1:6, 10, replace = TRUE)\n\nlanzamientos_10\n\n [1] 3 6 3 2 2 6 3 5 4 6\n\n\nPodemos ver los resultados de los 10 lanzamientos. Por ejemplo, si 3 salió tres veces, podemos calcular la probabilidad experimental de obtener un 3.\n\n# Calculamos la probabilidad experimental de obtener un 3\nsum(lanzamientos_10 == 3) / 10\n\n[1] 0.3\n\n\nEsto nos da la proporción de veces que apareció el 3, que es nuestra probabilidad experimental para esta simulación con 10 lanzamientos.\nIncluso podemos graficar los resultados en un gráfico de barras usando ggplot2.\n\n# Convertimos los resultados a un data frame\ndf_10 = as.data.frame(table(lanzamientos_10))\n\nggplot(df_10, aes(x = lanzamientos_10, y = Freq)) +\n  geom_col(fill = \"darkgreen\") +\n  labs(x = \"Número del dado\", y = \"Frecuencia\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSi el dado no estuviese trucado, esperaríamos que todas las barras fueran más o menos del mismo tamaño, lo que significaría que todos los números del dado tienen la misma probabilidad de salir. Sin embargo, con solo 10 lanzamientos, es probable que algunas barras sean más altas que otras debido a la variabilidad que puede surgir con pocas muestras.\nAhora probemos lanzando el dado 100 veces\n\n# Simulamos 100 lanzamientos de un dado\nlanzamientos_100 = sample(1:6, 100, replace = TRUE)\n\n# Calculamos la probabilidad experimental de obtener un 3\nsum(lanzamientos_100 == 3) / 100\n\n[1] 0.15\n\n\nPodemos observar que la probabilidad experimental se empieza a cercar a la teórica. Vamos a graficar estos resultados:\n\n# Convertimos los resultados a un data frame\ndf_100 = as.data.frame(table(lanzamientos_100))\n\n# Graficamos los resultados\nggplot(df_100, aes(x = lanzamientos_100, y = Freq)) +\n  geom_col(fill = \"darkgreen\") +\n  labs(x = \"Número del dado\", y = \"Frecuencia\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCon 100 lanzamientos, los resultados son más consistentes. Aunque todavía puede haber algo de variación, la frecuencia de aparición de cada número tiende a ser más uniforme y la probabilidad experimental de obtener un 3 debería aproximarse más \\(0.1667\\), la probabilidad teórica.\nVamos a realizar 10,000 lanzamientos.\n\n# Simulamos 10,000 lanzamientos de un dado\nlanzamientos_10000 = sample(1:6, 10000, replace = TRUE)\n\n# Calculamos la probabilidad experimental de obtener un 3\nsum(lanzamientos_10000 == 3) / 10000\n\n[1] 0.1669\n\n\nEn este punto, la probabilidad experimental de obtener un 3 debería estar cerca de \\(\\frac{1}{6}\\) o aproximadamente \\(0.1667\\) y las barras más uniformes. Graficamos nuevamente los resultados:\n\n# Convertimos los resultados a un data frame\ndf_10000 = as.data.frame(table(lanzamientos_10000))\n\n# Graficamos los resultados\nggplot(df_10000, aes(x = lanzamientos_10000, y = Freq)) +\n  geom_col(fill = \"darkgreen\") +\n  labs(x = \"Número del dado\", y = \"Frecuencia\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCon 10,000 lanzamientos, la variabilidad aleatoria debería haberse reducido considerablemente, y las frecuencias de aparición de cada número deberían estar bastante equilibradas. La probabilidad experimental de obtener un 3 ahora debería acercarse mucho a la probabilidad teórica de \\(\\frac{1}{6}\\).\nAnalicemos los resultados:\n\nPocos lanzamientos (10): La probabilidad experimental puede variar bastante debido a que el número de lanzamientos es pequeño y el azar tiene un mayor impacto. En este caso, podríamos ver que algunos números aparecen con más frecuencia de lo esperado simplemente por casualidad.\n\n\n\n\n\n\n\n\n\n\n\nLanzamientos moderados (100): Con más lanzamientos, la probabilidad experimental comienza a estabilizarse y se acerca más a la probabilidad teórica, aunque todavía puede haber cierta variación.\n\n\n\n\n\n\n\n\n\n\n\nMuchos lanzamientos (10,000): Con un gran número de lanzamientos, la probabilidad experimental tiende a estabilizarse muy cerca de la probabilidad teórica de \\(\\frac{1}{6}\\).\n\n\n\n\n\n\n\n\n\n\nLa función principal de la probabilidad experimental es aproximar la probabilidad teórica cuando no estamos seguros de cuál es, o cuando no podemos asumir que todos los resultados son igualmente probables. En este ejemplo, como no estábamos seguros de si el dado estaba trucado o no, usamos los resultados de las simulaciones (los lanzamientos del dado) para aproximar la probabilidad real de obtener un 3.\nLa probabilidad experimental te da una estimación basada en la observación de lo que ocurre en el mundo real. Cuantas más veces repitas el experimento, más precisa será la aproximación de la probabilidad teórica verdadera. Este proceso se guía por la Ley de los Grandes Números, que establece que mientras más veces realizas un experimento, más cerca estarán los resultados observados de las probabilidades verdaderas.\n\n\n\nExtraído de: https://www.statology.org/law-of-large-numbers/\n\n\nEn otras palabras, la precisión de la estimación depende del tamaño de la muestra (el número de lanzamientos). A medida que el tamaño de la muestra aumenta, es decir, lanzamos el dado más veces, la probabilidad experimental se acerca más a la probabilidad teórica. Esto no solo mejora la precisión de nuestra estimación, sino que también aumenta nuestra confianza en que la probabilidad observada es cercana a la real. Un tamaño de muestra mayor reduce la variabilidad y nos permite hacer inferencias más seguras sobre la probabilidad verdadera, ya que la incertidumbre disminuye con más datos. Comprender esto es fundamental para aventurarnos al núcleo de los siguientes capítulos sobre la estadística inferencial, donde usamos una muestra para estimar parámetros desconocidos de una población más grande.\nPuedes interactuar con este concepto en la Datáfora Interactiva: [Click Aquí]\nO usa el QR:\n\n\n\nDirígete a la pestaña Ley de los grandes números",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "prob.html#distribuciones-de-probabilidad",
    "href": "prob.html#distribuciones-de-probabilidad",
    "title": "5  Probabilidad",
    "section": "5.2 Distribuciones de probabilidad",
    "text": "5.2 Distribuciones de probabilidad\nA medida que realizamos estos lanzamientos del dado y analizamos los resultados, hemos visto que los resultados pueden variar. Sin embargo, con suficientes lanzamientos, empezamos a notar ciertos patrones. En el caso de los dados, por ejemplo, la distribución teórica es uniforme, lo que significa que cada evento (cada cara del dado) tiene la misma probabilidad de ocurrir. Esto es lo que esperamos en un dado justo: que a lo largo de muchos lanzamientos, cada número salga aproximadamente la misma cantidad de veces. Lo mismo ocurre cuando lanzamos una moneda o sacamos una carta de un mazo: cada resultado tiene la misma probabilidad de ocurrencia. A este tipo de distribución la llamamos distribución uniforme, y básicamente indica que, a largo plazo, todos los eventos tienen la misma probabilidad de aparecer.\nPodemos visualizar una distribución uniforme en R con el siguiente código:\n\ndf_uniforme = data.frame(x = factor(1:6), \n                         y = rep(1/6, 6))\n\nggplot(df_uniforme, aes(x = x, y = y)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  labs(title = \"Distribución uniforme: lanzamiento de un dado\",\n       x = \"Resultados del dado\",\n       y = \"Probabilidad teórica\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEsta gráfica nos muestra cómo, en un dado justo, cada cara tiene una probabilidad del 16.67% de aparecer.\nSin embargo, no todos los fenómenos que observamos en la realidad presentan resultados con igual probabilidad de ocurrencia. De hecho, en muchos casos, ciertos eventos son más frecuentes o más esperados que otros. A estas formas de representar la incertidumbre alrededor de un fenómeno las llamamos: distribuciones de probabilidad. A través de estas distribuciones, la probabilidad se convierte en una herramienta para describir y comprender el comportamiento de fenómenos en contextos diversos, desde encuestas sociales hasta procesos naturales.\nDependiendo de lo que estemos midiendo, las distribuciones pueden adoptar distintas formas. En algunos casos, los valores tienden a agruparse en torno a un punto central (como en la distribución normal), mientras que en otros interesa contar la frecuencia de eventos dentro de intervalos (como ocurre con la distribución de Poisson) o calcular probabilidades de éxito en ensayos repetidos (como en la binomial).\n\n\n\nExtraído de: https://www.geeksforgeeks.org/probability-distribution/\n\n\nCada una de estas distribuciones posee propiedades que las hacen adecuadas para representar distintos tipos de fenómenos reales. Una diferencia clave que se refleja también en sus gráficos es si la variable aleatoria es discreta o continua.\n\nLas distribuciones discretas se aplican cuando los resultados posibles son finitos y contables, como en el caso de los lanzamientos de un dado.\nEn contraste, las distribuciones continuas modelan situaciones en las que los resultados pueden tomar cualquier valor dentro de un rango, como al medir la temperatura o el tiempo entre eventos.\n\nEn las distribuciones discretas, los resultados están representados por barras individuales que muestran la frecuencia de cada resultado específico, mientras que en las distribuciones continuas, los resultados se agrupan en rangos y se muestran como una curva. Esto se debe a que, en una distribución continua, los resultados pueden tomar infinitos valores dentro de un intervalo, y en lugar de asignar una probabilidad exacta a un solo valor, calculamos la probabilidad de que un valor caiga dentro de un rango determinado.\nEn las distribuciones discretas, la probabilidad de cada evento se puede calcular como la altura de la barra en su correspondiente gráfico. En el caso de las distribuciones continuas, la probabilidad de un rango de valores se calcula como el área bajo la curva entre esos valores.\nPuedes interactuar con algunas de ellas en la Datáfora Interactiva: [Click Aquí]\nO usa el QR:\n\n\n\nDirígete a la pestaña Distribuciones de probabilidad\n\n\n\n5.2.1 Distribución Binomial\nLa distribución binomial es una distribución discreta que modela el número de éxitos en una serie de experimentos independientes donde cada uno tiene dos posibles resultados (éxito o fracaso). Por ejemplo, podríamos estar interesados en saber cuántas veces sale “cara” al lanzar una moneda 10 veces. Esta distribución es útil para situaciones en las que repetimos un experimento varias veces bajo las mismas condiciones.\nLa fórmula para la probabilidad binomial es:\n\\[ P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k} \\]\nDonde:\n\n\\(n\\) es el número de intentos (lanzamientos, ensayos).\n\\(k\\) es el número de éxitos.\n\\(p\\) es la probabilidad de éxito en cada intento.\n\nAquí \\(\\binom{n}{k}\\) es el coeficiente binomial, que representa el número de formas en que se pueden elegir \\(k\\) éxitos entre \\(n\\) ensayos.\nEn R, podemos simular la distribución binomial de la siguiente manera:\n\n\n\n\n\n\n\n\n\nCada barra muestra la frecuencia con la que se obtuvo un número específico de éxitos (de 0 a 10) en 1,000 simulaciones de una distribución binomial con 10 intentos y probabilidad de éxito 0.5.\nPodemos observar cómo cambia la forma de la distribución en función de la probabilidad de éxito:\n\n\n\n\n\n\n\n\n\nPodemos ver claramente cómo la forma de la distribución cambia con la probabilidad de éxito \\(p\\). Con valores bajos de \\(p\\) (cercanos a 0.1), la mayor parte de los ensayos tienen pocos éxitos, lo que genera una distribución sesgada hacia la izquierda. A medida que \\(p\\) aumenta, la distribución se vuelve más simétrica, y con valores altos (cercanos a 0.9), la distribución se concentra en el extremo de mayores éxitos.\nLa distribución binomial es ideal para modelar el número de éxitos en experimentos como encuestas, pruebas de productos o cualquier proceso donde haya dos posibles resultados (sí/no, éxito/fracaso).\n\n\n5.2.2 Distribución de Poisson\nLa distribución de Poisson es una distribución discreta que modela el número de eventos que ocurren en un intervalo de tiempo o espacio dado, bajo el supuesto de que estos eventos ocurren de manera independiente y con una tasa constante. Un ejemplo clásico de su uso es el número de llamadas que recibe una central telefónica en una hora.\nLa fórmula para la probabilidad de Poisson es:\n\\[\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\]\nDonde:\n\n\\(\\lambda\\) es la tasa promedio de ocurrencia de eventos en un intervalo (número promedio de eventos por unidad de tiempo).\n\\(k\\) es el número de eventos que queremos calcular la probabilidad de que ocurran.\n\nEn R, podemos simular la distribución de Poisson de la siguiente manera:\n\n\n\n\n\n\n\n\n\nCada barra representa la frecuencia de ocurrencia de un número específico de eventos (cuántas veces ocurrió) en 1,000 simulaciones de una distribución de Poisson con un valor promedio de lambda = 3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos realizar la comparación de la forma de la distribución de Poisson según diferentes valores de \\(\\lambda\\)\n\n\n\n\n\n\n\n\n\nPodemos ver cómo la distribución de Poisson cambia según el valor de \\(\\lambda\\). Para valores pequeños de \\(\\lambda\\), la mayor parte de los eventos ocurren en el rango de pocos sucesos (por ejemplo, entre 0 y 2). A medida que \\(\\lambda\\) aumenta, la distribución se desplaza hacia valores mayores, reflejando que ocurren más eventos en ese intervalo.\n\n\n5.2.3 Distribución Normal\nLa distribución normal, también conocida como la “curva de campana”, es una distribución continua que describe cómo se distribuyen los datos alrededor de un valor central (la media). Se caracteriza por ser simétrica alrededor de la media, con valores más probables cercanos a la media, y menos probables a medida que nos alejamos de ella. Es una de las distribuciones más importantes en estadística.\nLa fórmula para la distribución normal es:\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n\\]\nTenemos dos valores que modifican el comportamiento de la distribución:\n\n\\(\\mu\\) es la media de la distribución, donde se agrupan los datos.\n\\(\\sigma\\) es la desviación estándar, que describe la dispersión de los valores con respecto a la media.\n\nHay varias situaciones que suelen describirse bajo una distribución normal, debido a que muchos fenómenos naturales y procesos en la vida cotidiana tienden a agruparse alrededor de un valor promedio, con menos casos en los extremos.\n\nAltura de personas en una población: En una muestra grande de personas, la altura suele distribuirse normalmente. La mayoría de las personas estarán cerca de la altura promedio, con pocas personas siendo extremadamente altas o bajas.\nPuntajes de exámenes: En pruebas como exámenes de admisión, los puntajes suelen seguir una distribución normal, con la mayoría de los estudiantes obteniendo puntajes cercanos al promedio y menos estudiantes con puntajes muy altos o muy bajos.\nPeso de frutas y verduras: En un lote grande de frutas o verduras (por ejemplo, manzanas), el peso de cada pieza tiende a distribuirse normalmente debido a las variaciones naturales en el crecimiento de los alimentos.\nPeso de productos producidos en masa: En una fábrica, el peso de productos como latas de refresco o paquetes de alimentos tiende a seguir una distribución normal debido a pequeñas variaciones en el proceso de producción.\nTemperatura corporal: La temperatura corporal humana (aproximadamente 37°C) en una población sana sigue una distribución aproximadamente normal, con la mayoría de los individuos cercanos a la media y menos en los valores extremos.\n\nPodemos representarla de forma gráfica:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparación de la forma de la distribución normal según diferentes medias y desviaciones estándar:\n\n\n\n\n\n\n\n\n\nA medida que observamos las tres distribuciones normales, podemos notar cómo la media y la desviación estándar afectan su forma. La media determina el centro de la curva, es decir, el valor alrededor del cual se agrupan los datos. En los gráficos, vemos cómo la media se desplaza de 0 a 10, moviendo el pico de la distribución hacia la derecha.\nPor otro lado, la desviación estándar controla la dispersión de los datos: una desviación estándar baja (SD = 1) concentra los valores cerca de la media, mientras que una desviación estándar mayor (SD = 5) extiende la curva, creando una distribución más ancha y plana. A pesar de estos cambios, todas las distribuciones permanecen simétricas, característica fundamental de la distribución normal.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "prob.html#la-distribución-normal-en-detalle",
    "href": "prob.html#la-distribución-normal-en-detalle",
    "title": "5  Probabilidad",
    "section": "5.3 La distribución normal en detalle",
    "text": "5.3 La distribución normal en detalle\nSi nos vamos a embarcar en la inferencia, sin lugar a duda la distribución más importante es la distribución normal y por ello merece su propio apartado en donde describamos sus características.\nSu forma es simétrica y tiene apariencia de campana. Esta distribución se define por dos parámetros clave: la media (\\(\\mu\\)) y la desviación estándar (\\(\\sigma\\)). La media es el valor central donde la mayor parte de los datos se agrupan, mientras que la desviación estándar mide qué tan dispersos están los datos alrededor de la media.\nLa media (\\(\\mu\\)) determina la posición de la curva en el eje horizontal. Si la media cambia, la curva simplemente se desplaza hacia la derecha o izquierda, pero su forma permanece igual. Por otro lado, la desviación estándar (\\(\\sigma\\)) afecta la forma de la curva. Una desviación estándar mayor produce una curva más ancha y baja, lo que indica que los datos están más dispersos. En cambio, una desviación estándar menor hace que la curva sea más estrecha y alta, lo que refleja que los datos están más concentrados alrededor de la media.\nUna propiedad fundamental de la distribución normal es que, sin importar los valores específicos de la media y la desviación estándar, las probabilidades de encontrar datos dentro de ciertos rangos de desviaciones estándar desde la media son siempre las mismas. Lo denominamos la regla empírica:\nAproximadamente el 68% de los datos se encuentran dentro de 1 desviación estándar de la media (\\(\\mu \\pm 2 \\sigma\\)) el 95% dentro de 2 desviaciones estándar (\\(\\mu \\pm 2 \\sigma\\)) 99.7% dentro de 3 desviaciones estándar (\\(\\mu \\pm 3 \\sigma\\)).\nLo característico de la regla empírica es que se cumple consistentemente en cualquier distribución normal, sin importar los valores específicos de la media o la desviación estándar. Ya sea que la media sea 5 o 50, o que la dispersión sea mayor o menor, la proporción de datos que se concentra dentro de 1, 2 o 3 desviaciones estándar alrededor de la media se mantiene aproximadamente constante. Aproximadamente:\n\n68% de los datos estarán dentro de 1 desviación estándar de la media.\n95% dentro de 2 desviaciones estándar.\n99.7% dentro de 3 desviaciones estándar.\n\n\n\n\n\n\n\n\n\n\nPuedes interactuar con la distribución normal en la Datáfora Interactiva: [Click Aquí]\nO usa el QR:\n\n\n\nDirígete a la pestaña Distribución normal",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "prob.html#funciones-para-distribuciones-de-probabilidad",
    "href": "prob.html#funciones-para-distribuciones-de-probabilidad",
    "title": "5  Probabilidad",
    "section": "5.4 Funciones para distribuciones de probabilidad",
    "text": "5.4 Funciones para distribuciones de probabilidad\nCuando realizamos análisis estadístico, es común trabajar con estas distribuciones de probabilidad. Al fin y al cabo, estas distribuciones buscan modelar fenómenos reales, como la altura de las personas, el número de clientes que llegan a un restaurante o el número de éxitos en una serie de intentos. Las funciones dxxx, pxxx, qxxx y rxxx en R permiten explorar estas distribuciones, cada una respondiendo a preguntas estadísticas fundamentales:\n\ndxxx: Calcula la densidad de probabilidad o probabilidad exacta de un valor.\npxxx: Calcula la probabilidad acumulada, es decir, la probabilidad de observar valores menores o iguales a un valor dado.\nqxxx: Encuentra el valor (cuantil) asociado a una probabilidad acumulada.\nrxxx: Genera números aleatorios que siguen una distribución específica.\n\n\n\n\nExtraído de: https://diggingdeeperwithstats.wordpress.com/2021/05/21/visual-guide-to-pnorm-dnorm-qnorm-and-rnorm-functions-in-r/\n\n\nNo te preocupes si esto resulta particularmente complejo, lo es. Pero interna entender y poner en practica los siguiente ejemplos.\nComo ya sabes, la distribución normal describe fenómenos continuos simétricos. En la vida real, puede modelar características como la altura de las personas.\nImaginemos que las altura de los hombres adultos en una región siguen una distribución normal con media de 175 cm y desviación estándar de 10 cm.\n\n# Parámetros\nmean = 175\nsd = 10\n\n\n¿Qué tan común es encontrar a alguien que mide exactamente 180 cm? (dnorm) La densidad de probabilidad nos dice cuán probable es observar una altura cercana a 180 cm.\n\n\nx = 180  # Altura específica\n\n# Densidad de probabilidad\ndnorm(x, mean = mean, sd = sd)\n\n[1] 0.03520653\n\n\nEl valor es 0.035 lo que indica que es menos probable encontrar esta altura comparada con valores más cercanos a la media (175 cm).\n\n\n\n\n\n\n\n\n\n\n¿Qué porcentaje de hombres miden menos de 180 cm? (pnorm)\n\n\n# Probabilidad acumulada\npnorm(x, mean = mean, sd = sd)\n\n[1] 0.6914625\n\n\nLa probabilidad acumulada hasta \\(x = 180\\) es, aproximadamente, 69.15%. Esto significa que aproximadamente el 69.15% de los hombres tienen una altura menor o igual a 180 cm.\n\n\n\n\n\n\n\n\n\nEste porcentaje se calcula sumando todas las probabilidades desde el extremo más bajo de la distribución (por ejemplo, 140 cm o menos) hasta los 180 cm. Es importante entender que esto no es una probabilidad puntual como la que obtendríamos con dnorm, sino una acumulación de probabilidades. El gráfico muestra cómo las probabilidades se acumulan a medida que aumentamos la altura. En el eje horizontal tenemos los valores de altura (en cm), y en el eje vertical, las probabilidades acumuladas. A medida que las alturas aumentan, la probabilidad acumulada también crece, lo que refleja que estamos “acumulando” más población bajo ese umbral. En 180 cm, el punto destacado en el gráfico, la probabilidad acumulada es aproximadamente 69.15%, lo que significa que el 69.15% de las personas mide 180 cm o menos.\n\n¿Qué altura corresponde al percentil 90? (qnorm)\n\n\n# Percentil 90\nprobabilidad = 0.90\nqnorm(probabilidad, mean = mean, sd = sd)\n\n[1] 187.8155\n\n\nEl percentil 90 corresponde a una altura, por ejemplo, de 187.81 cm. Esto significa que el 90% de los hombres en esta muestra mide menos de 187.81 cm, y solo el 10% mide más.\n\n\n\n\n\n\n\n\n\n\n¿Cómo generar una muestra simulada de 100 alturas? (rnorm)\n\n\n# Generar alturas aleatorias\nn = 100\nalturas &lt;- rnorm(n, mean = mean, sd = sd)\nhead(alturas)  # Muestra las primeras 6 alturas simuladas\n\n[1] 183.1663 181.8145 189.9282 160.4299 170.1698 186.0007\n\n\nEl conjunto generado contiene alturas simuladas de una muestra de 100 personas, todas siguiendo una distribución normal con media de 175 cm y desviación estándar de 10 cm.\nAunque nos estamos centrando en la distribución normal, las funciones dxxx, pxxx, qxxx y rxxx son igualmente aplicables a otras distribuciones importantes como la binomial (dbinom, pbinom, qbinom, y rbinom) o la poisson (dpois, ppois, qpois, y rpois).\nPara sintetizar:\n\n\n\n\n\n\n\n\n\nFunción\nDescripción\nPregunta que Responde\nArgumentos en R\n\n\n\n\ndnorm\nCalcula la densidad de probabilidad en un valor dado.\n¿Qué tan común es observar este valor específico?\ndnorm(x, mean, sd)x: valormean: mediasd: desviación estándar\n\n\npnorm\nCalcula la probabilidad acumulada hasta un valor específico.\n¿Qué porcentaje de la población está por debajo de este valor?\npnorm(q, mean, sd)q: valor cuantilmean: mediasd: desviación estándar\n\n\nqnorm\nDevuelve el valor asociado a una probabilidad acumulada dada.\n¿Qué valor corresponde a este percentil o probabilidad acumulada?\nqnorm(p, mean, sd)p: probabilidad acumuladamean: mediasd: desviación estándar\n\n\nrnorm\nGenera números aleatorios que siguen una distribución normal con parámetros definidos.\n¿Cómo puedo simular datos que sigan esta distribución?\nrnorm(n, mean, sd)n: número de valoresmean: mediasd: desviación estándar",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "prob.html#probabilidades-y-valor-z",
    "href": "prob.html#probabilidades-y-valor-z",
    "title": "5  Probabilidad",
    "section": "5.5 Probabilidades y valor z",
    "text": "5.5 Probabilidades y valor z\nEn una distribución normal, la probabilidad de que un valor caiga en un cierto rango está representada por el área bajo la curva. Por ejemplo, la probabilidad de que un valor esté por debajo de la media es del 50%, ya que la curva normal es simétrica alrededor de la media.\n\n\n\n\n\n\n\n\n\nAhora bien, en estadística, muchas veces necesitamos medir cuán lejos está un valor de la media en términos de desviaciones estándar. A esto lo llamamos valor z. El valor z nos indica cuántas desviaciones estándar por encima o por debajo de la media se encuentra un dato específico en la distribución normal sin importar la media o la disviación estandar que tengan.\nAsí, el valor z es simplemente una forma de estandarizar los datos en una distribución normal. Nos dice cuántas desviaciones estándar (\\(\\sigma\\)) está un valor (\\(x\\)) por encima o por debajo de la media (\\(\\mu\\)). La fórmula es:\n\\[z = \\frac{x - \\mu}{\\sigma}\\]\n\nSi el valor z es positivo, significa que el valor está por encima de la media.\nSi el valor z es negativo, el valor está por debajo de la media.\n\nPor ejemplo, si un valor tiene un z = 1, significa que está una desviación estándar por encima de la media; si tiene un z = -2, está dos desviaciones estándar por debajo de la media.\nRecordemos que las áreas bajo la curva normal corresponden a probabilidades. Por ejemplo:\n\nEl área total bajo la curva es igual a 1 (o el 100%).\nEl área a la izquierda de (z = 0) (la media) es 0.5, lo que significa que el 50% de los datos están por debajo de la media.\n\nCuando queremos encontrar el valor z para una probabilidad dada, estamos básicamente preguntando: “¿Cuántas desviaciones estándar debo alejarme de la media para que el área acumulada bajo la curva normal sea igual a una probabilidad específica?”\nPongamos un ejemplo. Supongamos que queremos encontrar el valor z que corresponde a una probabilidad específica. Queremos encontrar cuántas desviaciones estándar necesitamos movernos desde la media para que el 95% de los datos queden a la izquierda de ese valor. Usamos la función qnorm() en R, que toma como argumento una probabilidad y devuelve el valor z correspondiente:\n\nqnorm(0.95)\n\n[1] 1.644854\n\n\nEl resultado será un valor z positivo (aproximadamente 1.645), que indica que el 95% de los datos están por debajo de aproximadamente 1.645 desviaciones estándar por encima de la media.\n\n\n\n\n\n\n\n\n\nAhora, supongamos que queremos el valor z para una probabilidad del 5%, es decir, queremos saber cuántas desviaciones estándar hay entre la media y un punto que deja el 5% de los datos a la izquierda (en la cola izquierda de la distribución). Para esto usamos:\n\nqnorm(0.05)\n\n[1] -1.644854\n\n\nEste valor z será negativo (aproximadamente -1.645), lo que indica que este punto está a 1.645 desviaciones estándar por debajo de la media.\n\n\n\n\n\n\n\n\n\nRecordemos que la curva normal es símetrica. Entocnes, si queremos encontrar los valores z que dejan un 95% de los datos en el centro de la distribución, dejamos un 2.5% en cada cola (5% dividido entre 2). Calculamos ambos extremos:\n\nqnorm(0.025)  # Para el límite inferior\n\n[1] -1.959964\n\nqnorm(0.975)  # Para el límite superior\n\n[1] 1.959964\n\n\nLos resultados serán aproximadamente -1.96 y 1.96, lo que significa que el 95% de los datos se encuentran entre -1.96 y 1.96 desviaciones estándar de la media.\n\n\n\n\n\n\n\n\n\nEl valor \\(z\\) es una medida que nos dice cuántas desviaciones estándar está un dato de la media en una distribución normal. Para encontrar un valor z que corresponda a una probabilidad específica (o área bajo la curva). Usando esta lógica también podemos dar un valor e identificar la probabilidad de encontrarnos un mayor o menor valor.\nImagina que estamos trabajando con las alturas de personas y sabemos que las alturas siguen una distribución normal con una media (\\(\\mu\\)) de 170 cm y una desviación estándar (\\(\\sigma\\)) de 10 cm.\n\n\n\n\n\n\n\n\n\nEn este caso, queremos saber la probabilidad de que una persona mida menos de 180 cm. Lo que necesitamos es calcular el área bajo la curva normal desde la izquierda hasta 180 cm.\nPrimero, podemos calcular el valor z, que indica cuántas desviaciones estándar está esa altura de la media: \\[z = \\frac{x - \\mu}{\\sigma}\\]\nDonde:\n\n\\(x = 180\\) cm,\n\\(\\mu = 170\\) cm,\n\\(\\sigma = 10\\) cm.\n\nEntonces:\n\\[z = \\frac{180 - 170}{10}\\] \\[z = 1\\]\n\n\n\n\n\n\n\n\n\nAl ser \\(z = 1\\). En R, puedes hacer todo el cálculo con la función pnorm() , que directamente te da la probabilidad acumulada para un valor \\(x\\).\n\npnorm(1)\n\n[1] 0.8413447\n\n\nTambién podemos especificar la media y las desviación estándar y el valor deseado, dándonos el mismo resultado.\n\npnorm(180, mean=170, sd=10)\n\n[1] 0.8413447\n\n\nEs decir, una probabilidad del 84.13%\n\n\n\n\n\n\n\n\n\nSi ahora queremos saber la probabilidad de que alguien mida más de 190 cm, hacemos lo siguiente:\n\n1 - pnorm(190, mean=170, sd=10)\n\n[1] 0.02275013\n\n\nAquí restamos de 1 porque pnorm() nos da la probabilidad de estar por debajo de 190 cm, pero queremos saber la probabilidad de estar por encima.\n\n\n\n\n\n\n\n\n\nPara calcular la probabilidad de que alguien esté dentro de un rango, en este caso entre 160 cm y 180 cm, necesitamos calcular la probabilidad acumulada hasta 180 cm y restarle la probabilidad acumulada hasta 160 cm:\n\npnorm(180, mean=170, sd=10) - pnorm(160, mean=170, sd=10)\n\n[1] 0.6826895\n\n\nEsto nos dará la probabilidad de que una persona mida entre 160 cm y 180 cm.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "prob.html#distribución-muestral",
    "href": "prob.html#distribución-muestral",
    "title": "5  Probabilidad",
    "section": "5.6 Distribución muestral",
    "text": "5.6 Distribución muestral\nPerfecto, ahora que ya entendemos las bases de la probabilidad y la distribución normal, es momento de ponernos manos a la obra y ver como podemos integrarlo en la inferencia.\nRecuerdas que vimos en el capítulo anterior que cuando extraemos una muestra de una población, calculamos resúmenes numéricos como la media, la desviación estándar o la proporción, que nos dan información sobre el comportamiento de nuestras variables. Sin embargo, como nuestras muestras representan solo una fracción de la población, es natural que las métricas obtenidas de diferentes muestras varíen. Debemos ser capaces de poder representar esa variación, para ello tenemos la distribución muestral: se refiere a la distribución de un estadístico (por ejemplo, la media o la proporción) calculado a partir de múltiples muestras extraídas de la misma población.\n\nEl propósito de una distribución muestral es comprender cómo varía un estadístico (como la media) cuando repetimos el proceso de muestreo múltiples veces. En estadística, pocas veces tenemos acceso a toda la población, por lo que, en la práctica, trabajamos con muestras para hacer inferencias sobre esa población completa. Sin embargo, al tomar solo una muestra, no tenemos la certeza de que su media sea exactamente la media de la población, ya que siempre existirá cierta variación entre diferentes muestras. Aquí es donde la Ley de los Grandes Números nos recuerda que, a medida que aumentamos el número de muestras, la media muestral tenderá a acercarse a la media poblacional.\nLa distribución muestral de la media nos permite entender y cuantificar esta variabilidad al analizar cómo se distribuyen las medias de muchas muestras extraídas de la misma población. Al observar cómo se comportan las medias de múltiples muestras, podemos obtener una visión más precisa y confiable de la media poblacional.\nEmpecemos simulando una recolección de datos a partir de una muestra inicial de la población y calculando su media:\nImaginemos que hemos recolectado datos de una primera muestra de 120 personas, obteniendo sus alturas en centímetros. \n\nhead(muestra1)\n\n  observación  alturas\n1           1 168.9787\n2           2 160.7019\n3           3 158.7101\n4           4 150.2094\n5           5 174.0059\n6           6 171.2548\n\n\n\nmuestra1 %&gt;% \nggplot(aes(x = alturas)) +\n  geom_histogram(color = \"black\", fill = \"lightblue\") +\n  labs(title = \"Muestra 1\", x = \"Altura (cm)\", y = \"Densidad\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nPodemos extraer la media de esta muestra:\n\nmean(muestra1$alturas)\n\n[1] 170.7429\n\n\nPodemos recolectar otros 120 datos\nImaginemos que hemos recolectado datos de una primera muestra de 120 personas, obteniendo sus alturas en centímetros.\n\nhead(muestra2)\n\n  observación  alturas\n1           1 136.5332\n2           2 174.1863\n3           3 155.8392\n4           4 176.0232\n5           5 155.6674\n6           6 165.3986\n\n\n\nmuestra2 %&gt;% \nggplot(aes(x = alturas)) +\n  geom_histogram(color = \"black\", fill = \"lightblue\") +\n  labs(title = \"Muestra 2\", x = \"Altura (cm)\", y = \"Densidad\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nPodemos extraer la media de esta muestra:\n\nmean(muestra2$alturas)\n\n[1] 169.9448\n\n\nPodemos repetir este proceso de muestreo muchas veces para obtener múltiples muestras y así calcular la media de cada una. Esto nos permitirá observar cómo varía la media muestral de muestra en muestra y construir la distribución muestral de la media.\nEn este ejemplo, he generado 50 muestras de 120 personas.\n\nmedias_muestrales\n\n [1] 170.1544 169.7380 170.8635 170.2356 170.0972 168.8613 170.4026 171.4058\n [9] 170.5223 169.9030 169.7423 169.8366 171.3686 170.6401 169.9608 170.8007\n[17] 170.6227 168.1252 168.7881 170.4628 170.0155 170.7018 168.4767 170.6914\n[25] 170.7898 169.2718 169.7811 169.9039 170.5217 170.1457 169.6884 168.1856\n[33] 171.6336 169.2822 169.7794 169.8715 170.4480 169.5733 170.2067 169.4459\n[41] 169.6532 169.0059 170.2424 170.2214 169.6113 170.8416 169.7628 171.1970\n[49] 170.1087 171.0459\n\n\nFinalmente, lo podemos graficar.\n\n\n\n\n\n\n\n\n\nEste gráfico muestra la distribución muestral de la media, es decir, cómo se distribuyen las medias de las 50 muestras alrededor de la media de la población (170 cm). Es importante no confundir este histograma con uno que representa la distribución de la variable original (las alturas individuales). En este caso, el histograma no representa las alturas de cada persona, sino las medias de diferentes muestras de 120 personas cada una.\n¿Notas algo particular? Aunque aun son muy pocas las muestras tomadas, puedes observar que la concentración de estas medias en torno a un valor central donde, aunque cada muestra puede variar, las medias tienden a agruparse alrededor de la media poblacional.\nAhora con 1000 muestras.\n\n\n\n\n\n\n\n\n\n¿No te recuerda a una distribución normal?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "prob.html#teorema-del-límite-central",
    "href": "prob.html#teorema-del-límite-central",
    "title": "5  Probabilidad",
    "section": "5.7 Teorema del límite central",
    "text": "5.7 Teorema del límite central\nExacto, lo que estamos observando es un fenómeno descrito por el Teorema del Límite Central (TLC). Este teorema afirma que, si tomamos suficientes muestras grandes de una población, la distribución de sus medias tenderá a ser aproximadamente normal, independientemente de la forma de la distribución original de los datos.\nAquí es donde el TLC es especialmente valioso. En la práctica, los datos de una población pueden tener cualquier forma: podrían ser altamente asimétricos, como los ingresos de una población, donde la mayoría de las personas tiene ingresos bajos y unos pocos tienen ingresos muy altos. También podríamos encontrarnos con distribuciones con múltiples picos o incluso distribuciones muy sesgadas en una dirección. Sin embargo, el TLC nos asegura que, aunque la forma original de los datos sea irregular o no normal, la distribución de las medias de muchas muestras grandes se aproximará a una distribución normal.\nPero te preguntarás, ¿por qué hemos hecho tantas simulaciones de muestras? ¿No es que, en la práctica, solo tenemos acceso a una única muestra? La razón es que este ejercicio con múltiples muestras nos permite visualizar y comprender cómo se comportaría la media muestral si pudiéramos repetir el proceso de muestreo muchas veces, aunque en la realidad usualmente solo tengamos una muestra.\nEl Teorema del Límite Central nos da la seguridad de que, con una muestra suficientemente grande, la media de esa única muestra se aproximará razonablemente bien a la media de la población, sin necesidad de repetir el muestreo varias veces. Esta teoría es la base de muchas herramientas estadísticas que usamos para hacer inferencias sobre las características de la población (parámetros) a partir de la información de las muestras (estadísticos). En esencia, nos evita tener que recolectar múltiples muestras en la práctica, porque sabemos (gracias al TLC) que el promedio de nuestra muestra es un estimador confiable de la media poblacional.\nAsí, aunque en este ejercicio hicimos simulaciones para observar el proceso y entender cómo se construye la distribución muestral, el Teorema del Límite Central nos permite inferir el comportamiento de la población completa usando solo una muestra.\n\n\n\nExtraído de: https://www.allmath.com/central-limit-theorem-calculator.php\n\n\nEste conocimiento abre la puerta a una serie de herramientas estadísticas que se basan en la suposición de que la distribución de nuestras estimaciones (como la media muestral) tiende a una forma normal cuando el tamaño de la muestra es suficientemente grande.\nGracias al Teorema del Límite Central, podemos aplicar métodos de inferencia estadística, como los intervalos de confianza y las pruebas de hipótesis, para hacer estimaciones sobre parámetros poblacionales. Estos métodos se basan en la aproximación de la distribución muestral a una distribución normal cuando el tamaño de la muestra es suficientemente grande. Esto nos permite determinar intervalos de confianza y regiones de rechazo en pruebas de hipótesis (más sobre esto en el próximo capítulo).\nSabemos que la distribución normal está definida por dos parámetros fundamentales: la media y la desviación estándar, representadas como \\(\\mu\\) y \\(\\sigma\\), respectivamente. La media \\(\\mu\\) indica el centro de la distribución, mientras que la desviación estándar \\(\\sigma\\) mide la dispersión de los datos alrededor de esa media. Cuando realizamos múltiples simulaciones de muestras y calculamos sus estadísticas (como la media muestral \\(\\bar{x}\\)), observamos que estas estadísticas siguen una distribución muestral.\nAquí tienes una versión mejorada y más clara del párrafo, con un tono más explicativo y fluido:\nEn este contexto, es importante comprender la diferencia entre la media de la población (\\(\\mu\\)) y la media muestral (\\(\\bar{x}\\)). La media poblacional \\(\\mu\\) es un valor fijo y desconocido que queremos estimar, mientras que la media muestral \\(\\bar{x}\\) es el promedio de los datos en una muestra y puede variar de una muestra a otra. Sin embargo, gracias al Teorema del Límite Central, sabemos que si tomamos muchas muestras y calculamos sus medias, estas medias muestrales (\\(\\bar{x}\\)) seguirán una distribución en torno a \\(\\mu\\). En otras palabras, aunque cada muestra pueda producir una media ligeramente diferente, en promedio, la media muestral estará muy cerca de la media poblacional.\nPero, ¿qué tan cerca está realmente \\(\\bar{x}\\) de \\(\\mu\\) en cada caso? Para responder a eso usamos el concepto de error estándar. El error estándar no es más que la desviación estándar de la distribución de \\(\\bar{x}\\), y nos dice qué tan dispersas están las medias muestrales alrededor de la media poblacional. Se calcula así:\n\\[\n\\text{Error estándar} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nsi conocemos la desviación estándar de la población (\\(\\sigma\\)).\nSin embargo, en la mayoría de los casos no conocemos \\(\\sigma\\), ya que es un parámetro poblacional desconocido. En estos casos, usamos la desviación estándar de la muestra (\\(s\\)) como una aproximación de \\(\\sigma\\). Al hacer esto, estamos introduciendo una pequeña incertidumbre adicional, porque ahora nuestra estimación depende de dos valores estimados: la media muestral (\\(\\bar{x}\\)) y la desviación estándar muestral (\\(s\\)). En este caso, el error estándar se calcula como:\n\\[\n\\text{Error estándar} = \\frac{s}{\\sqrt{n}}\n\\]\nPor tanto, el error estándar nos ayuda a entender qué tanta variabilidad hay en las medias muestrales y, por lo tanto, qué tan confiable es nuestra estimación de \\(\\mu\\) usando \\(\\bar{x}\\). Un error estándar más pequeño significa que nuestras medias muestrales están más agrupadas alrededor de \\(\\mu\\), lo que nos da mayor confianza en nuestra estimación.\n\n\n\n\n\n\n\n\n\nEn los próximos capítulos, exploraremos cómo aplicar estas herramientas estadísticas en diferentes contextos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "prob.html#flor-y-el-uso-de-las-redes-sociales",
    "href": "prob.html#flor-y-el-uso-de-las-redes-sociales",
    "title": "5  Probabilidad",
    "section": "5.8 Flor y el uso de las redes sociales",
    "text": "5.8 Flor y el uso de las redes sociales\nFlor está interesada en analizar cuántas horas a la semana dedica un peruano adulto al uso de redes sociales. Para llevar a cabo este análisis, utilizó los resultados de una encuesta aplicada a 2000 personas, cuyos datos se encuentran en un archivo llamado internet.csv.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(psych)\n\nPrimero, Flor carga todas las librerías necesarias para su análisis e importa el archivo en R. Verifica las primeras filas para asegurarse de que los datos están organizados correctamente:\n\ninternet = read_csv(\"internet.csv\")\nhead(internet)\n\n# A tibble: 6 × 1\n  tiempo\n   &lt;dbl&gt;\n1     18\n2     28\n3     13\n4     21\n5     32\n6     23\n\n\nCada fila del archivo representa el tiempo (en horas por semana) que una persona dedica a redes sociales. Las primeras respuestas observadas fueron: 18, 28, 13, 21, 32 y 23 horas.\nLuego, procede a calcular estadísticas descriptivas que resumen los datos:\n\ndescribe(internet$tiempo)\n\n   vars    n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 2000 21.75 5.84     21   21.23 5.93  11  53    42 1.01     1.66 0.13\n\n\nLos resultados muestran lo siguiente:\n- Promedio (\\(\\bar{x}\\)): ( 21.75 ) horas/semana. En promedio, un adulto peruano dedica 21.75 horas semanales a redes sociales.\n- Desviación estándar (\\(s\\)): ( 5.84 ) horas. Esto indica cuánto varían las respuestas en torno al promedio.\n- Mediana: (21) horas. Esto significa que la mitad de los encuestados dedica menos de 21 horas a las redes sociales, y la otra mitad más.\n- Mínimo y máximo: Los valores oscilan entre (11 y (53) horas, con un rango de ( 42 ) horas.\n- Asimetría positiva: La asimetría (\\(skew = 1.01\\)) indica que algunos encuestados pasan muchas más horas que el promedio en redes sociales.\nFlor utiliza un histograma para comprender mejor la distribución de las horas dedicadas a redes sociales:\n\nggplot(internet, aes(x = tiempo)) +\n  geom_histogram(bins = 30, fill=\"skyblue\", color=\"black\") +\n  labs(\n    title = \"Distribución de horas/semana en el uso de redes sociales\",\n    x     = \"Horas a la semana\",\n    y     = \"Frecuencia\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEl histograma revela que la distribución es asimétrica hacia la derecha, con más personas concentradas en valores cercanos al promedio, pero con algunos valores altos que extienden la cola hacia la derecha.\nPara enfatizar esta asimetría, Flor añade líneas que marcan el promedio y la mediana:\n\nggplot(internet, aes(x = tiempo)) +\n  geom_histogram(bins=30, fill=\"skyblue\", color=\"black\") +\n  geom_vline(xintercept = mean(internet$tiempo), \n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = median(internet$tiempo), \n             color = \"purple\", \n             linetype = \"dashed\", \n             size = 1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLa línea roja (promedio) está ligeramente por encima de la línea púrpura (mediana), confirmando la asimetría positiva.\nFlor sabe que, según el Teorema del Límite Central, si se toman muchas muestras de un tamaño lo suficientemente alto (\\(n = 2000\\)) con respecto a la población, las medias de esas muestras seguirán una distribución aproximadamente normal, incluso si los datos originales (en este caso, las horas semanales dedicadas a redes sociales) no tienen una distribución normal. Es decir, aunque las respuestas individuales puedan variar y estar distribuidas de forma asimétrica, las medias de múltiples muestras tenderán a concentrarse cerca de un valor central.\nPara entender mejor esta idea y calcular cuánta variación esperaríamos en estas medias si repitiera el estudio, Flor necesita calcular el error estándar. El error estándar es la medida que nos dice cuánto podría variar la media de una muestra debido a la aleatoriedad de los datos. Se calcula utilizando la fórmula:\n\\[\n\\text{Error estándar (EE)} = \\frac{s}{\\sqrt{n}}\n\\]\nDonde:\n\n\\(s = 5.84\\) es la desviación estándar.\n\\(n = 2000\\) es el tamaño de la muestra.\n\nEn R, Flor utiliza:\n\nee = sd(internet$tiempo) / sqrt(length(internet$tiempo))\nee\n\n[1] 0.1306164\n\n\nEl error estándar resultante es 0.13 horas. Esto significa que si Flor repitiera esta encuesta muchas veces, las medias de cada muestra variarían en promedio \\(0.13\\) horas alrededor del valor verdadero.\nPara ilustrar el concepto del Teorema del Límite Central, Flor simula la distribución de las medias muestrales. En R, utiliza la función rnorm para generar 10,000 medias muestrales basadas en: - \\(\\text{mean} = 21.75\\), el promedio muestral. - \\(\\text{sd} = 0.13\\), el error estándar de la media.\n\ndist_muestral = data.frame(medias = \n  rnorm(\n    n     = 10000, \n    mean  = mean(internet$tiempo), \n    sd    = ee\n  )\n)\n\nPara ello, utiliza rnorm, en el cual: - n = 10000: Genera 10,000 valores simulados. - mean = 21.75: La media esperada. - sd = 0.13: El error estándar, que representa la variación esperada de las medias muestrales.\nFlor luego visualiza esta distribución de medias muestrales:\n\ndist_muestral %&gt;% \n  ggplot(aes(x = medias)) + \n  geom_histogram(bins=30, fill=\"lightgreen\", color=\"black\") +\n  labs(\n    title = \"Distribución muestral (aprox.) de la media\",\n    x     = \"Media (horas/sem)\",\n    y     = \"Frecuencia\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEl histograma resultante muestra una distribución normal centrada en \\(21.75\\) horas con una dispersión reducida (\\(0.13\\) horas).\n\n\n\nLa probabilidad representa la base fundamental de la estadística inferenical",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "prob.html#resumen-del-capítulo",
    "href": "prob.html#resumen-del-capítulo",
    "title": "5  Probabilidad",
    "section": "5.9 Resumen del capítulo",
    "text": "5.9 Resumen del capítulo\nLa inferencia estadística permite ir más allá del análisis descriptivo y realizar generalizaciones sobre una población a partir de una muestra. Esto se logra aplicando la probabilidad para modelar la incertidumbre en los datos y predecir comportamientos futuros. La probabilidad puede entenderse como una proporción que mide la frecuencia esperada de un evento en relación con el total de posibles resultados. Por ejemplo, en un lanzamiento de moneda, la probabilidad de obtener cara es \\(\\frac{1}{2}\\), lo que equivale al 50%.\nLa probabilidad también se puede interpretar en términos de frecuencia relativa a largo plazo, lo que significa que la estimación de la probabilidad mejora conforme se repite un experimento muchas veces. En un dado justo, cada cara tiene la misma probabilidad de salir, pero si se lanza un número reducido de veces, la frecuencia observada puede diferir de la probabilidad teórica. Sin embargo, a medida que el número de lanzamientos aumenta, la frecuencia relativa de cada resultado se aproxima a su probabilidad teórica. Este comportamiento se conoce como la Ley de los Grandes Números y fundamenta la validez de los cálculos probabilísticos en situaciones reales.\nLas distribuciones de probabilidad describen cómo se comportan los resultados de un experimento aleatorio. En el caso de una distribución uniforme, cada resultado tiene la misma probabilidad de ocurrir, como en un dado no trucado. Sin embargo, en muchos casos los eventos no tienen probabilidades iguales, lo que lleva a distribuciones más complejas como la binomial, que modela el número de éxitos en una serie de ensayos independientes con dos posibles resultados (éxito o fracaso), y la distribución de Poisson, que describe la frecuencia con la que ocurren eventos en un intervalo de tiempo o espacio.\nEntre todas las distribuciones, la normal tiene una importancia fundamental. Su forma de campana es simétrica y se caracteriza por dos parámetros: la media (\\(\\mu\\)), que indica el centro de la distribución, y la desviación estándar (\\(\\sigma\\)), que mide la dispersión de los valores alrededor de la media. La regla empírica establece que aproximadamente el 68% de los valores se encuentran dentro de una desviación estándar de la media, el 95% dentro de dos desviaciones y el 99.7% dentro de tres.\nExisten funciones específicas en R para trabajar con distribuciones de probabilidad. Las funciones dxxx, pxxx, qxxx y rxxx permiten, respectivamente, calcular densidades de probabilidad, probabilidades acumuladas, valores críticos y generar datos aleatorios de una distribución dada. Por ejemplo, en la distribución normal, pnorm(x, mean, sd) permite calcular la probabilidad de obtener un valor menor o igual a \\(x\\), mientras que qnorm(p, mean, sd) encuentra el valor que corresponde a un percentil específico dentro de la distribución.\nEl valor z es una medida estandarizada que permite comparar valores de distintas distribuciones. Se calcula como la diferencia entre un valor y la media, dividida por la desviación estándar:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nUn valor z positivo indica que el dato está por encima de la media, mientras que un valor negativo indica que está por debajo. Gracias a la simetría de la distribución normal, se pueden usar tablas de valores z o funciones en R para calcular probabilidades acumuladas y valores críticos, lo que resulta útil en pruebas de hipótesis y análisis inferenciales.\nCuando se trabaja con muestras extraídas de una población, la distribución muestral describe la variabilidad de un estadístico como la media. Aunque diferentes muestras pueden producir medias distintas, estas siguen un patrón predecible, lo que permite hacer inferencias sobre la población. Aquí entra en juego el Teorema del Límite Central, que establece que, independientemente de la forma de la distribución original, la distribución de las medias muestrales tiende a una distribución normal cuando el tamaño de la muestra es suficientemente grande. Este principio es crucial porque justifica el uso de métodos estadísticos basados en la normalidad, incluso cuando los datos individuales no siguen una distribución normal.\nEl error estándar permite cuantificar la variación de la distribución muestral y es fundamental para la consutrucción de herrmaientas inferenciales como la estimación y el contraste de hipó. Se calcula como la desviación estándar dividida por la raíz del tamaño de la muestra:\n\\[\n\\text{Error estándar} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nEste valor indica la variabilidad esperada en la media muestral si se tomaran múltiples muestras de la misma población. Un error estándar pequeño sugiere que la media muestral es un buen estimador de la media poblacional. En la práctica, cuando la desviación estándar de la población es desconocida, se usa la desviación estándar de la muestra (\\(s\\)) como una aproximación.\nEn estudios aplicados, la inferencia estadística se basa en la distribución muestral para estimar parámetros poblacionales y hacer afirmaciones sobre una población a partir de una muestra. Gracias al Teorema del Límite Central, se puede asumir que la distribución de las medias muestrales es aproximadamente normal, lo que permite calcular intervalos de confianza y realizar pruebas de hipótesis con fundamentos matemáticos sólidos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "prob.html#ejercicios",
    "href": "prob.html#ejercicios",
    "title": "5  Probabilidad",
    "section": "5.10 Ejercicios",
    "text": "5.10 Ejercicios\n1. Se lanza un dado justo de seis caras. Queremos calcular la probabilidad de obtener un número mayor a 4 en un solo lanzamiento. ¿Cuál es la probabilidad de este evento?\n\n\\(\\frac{4}{6}\\) (0.6667)\n\n\\(\\frac{2}{6}\\) (0.3333)\n\n\\(\\frac{1}{6}\\) (0.1667)\n\n\\(\\frac{3}{6}\\) (0.5000)\n\n2. En una población donde las alturas de los adultos siguen una distribución normal con media \\(\\mu = 170\\text{ cm}\\) y desviación estándar \\(\\sigma = 10\\text{ cm}\\), ¿entre qué valores se encuentra aproximadamente el 68% de las alturas según la Regla Empírica?\n\n150 cm y 190 cm\n165 cm y 175 cm\n160 cm y 180 cm\n\n155 cm y 185 cm\n\n3. Si una distribución normal tiene media \\(\\mu = 150\\) y desviación estándar \\(\\sigma = 20\\), ¿qué porcentaje de los datos se encuentra por debajo de \\(x = 90\\)? Usa la Regla Empírica y considera la acumulación de probabilidades a ambos lados.\n\n0.3%\n\n0.13%\n\n2.5%\n\n5%\n\n4. Las alturas de los hombres adultos en una región siguen una distribución normal con media \\(\\mu = 175\\text{ cm}\\) y desviación estándar \\(\\sigma = 10\\text{ cm}\\). ¿Cuál es la probabilidad de que un hombre seleccionado al azar mida menos de 165 cm?\n\n0.1587 (15.87%)\n\n0.3413 (34.13%)\n\n0.5000 (50.00%)\n\n0.8413 (84.13%)\n\n5. En una distribución normal con media \\(\\mu = 175\\text{ cm}\\) y desviación estándar \\(\\sigma = 10\\text{ cm}\\), ¿qué altura corresponde al percentil 95, es decir, el valor por debajo del cual se encuentra el 95% de la población?\n\n185.25 cm\n\n190.45 cm\n\n195.30 cm\n\n200.75 cm\n\n6. Flor realizó una encuesta a 2000 personas para medir el tiempo promedio que pasan en redes sociales. El promedio fue de 21.75 horas por semana, con una desviación estándar muestral de \\(s = 5.84\\) horas. ¿Cuál es el error estándar de la media en esta encuesta? Recuerda que el error estándar de la media se calcula como \\(\\frac{s}{\\sqrt{n}}\\).\n\n0.10 horas\n\n0.30 horas\n0.25 horas\n\n0.13 horas\n\n7. Se simulan 1000 muestras de tamaño \\(n = 30\\), provenientes de una población con media \\(\\mu = 50\\) y desviación estándar \\(\\sigma = 10\\). Según el Teorema del Límite Central, ¿cómo se comportará la distribución de las medias muestrales?\n\nTendrá una media de 50 y una desviación estándar de 10.\n\nTendrá una media de 50 y una desviación estándar de \\(\\frac{10}{\\sqrt{30}}\\).\n\nTendrá una media de 50 y una desviación estándar de \\(\\sqrt{30}\\).\n\nNo se puede determinar sin más información.\n\n8. Si las puntuaciones de un examen de matemáticas siguen una distribución normal con media \\(\\mu = 75\\) y desviación estándar \\(\\sigma = 8\\), ¿qué porcentaje de estudiantes obtienen una puntuación entre 67 y 83 puntos?\n\n34.13%\n\n68.26%\n\n84.13%\n\n95.44%\n\n9. Dos estudiantes realizan pruebas en diferentes universidades. En la universidad de Ana, la media de los puntajes es 500 con una desviación estándar de 100, mientras que en la universidad de Juan, la media es 75 con una desviación estándar de 10. Si Ana obtiene 650 y Juan obtiene 85, ¿quién tuvo un desempeño relativamente mejor en su universidad? Recuerda usar la puntuación estandarizada: \\(z = \\frac{x - \\mu}{\\sigma}\\).\n\nAna\n\nJuan\n\nAmbos tuvieron el mismo desempeño relativo\n\nNo se puede determinar sin más información\n\n10. ¿Cuál de las siguientes afirmaciones describe mejor el principio de la Ley de los Grandes Números?\n\nA medida que aumenta el número de repeticiones de un experimento aleatorio, la frecuencia relativa de un evento se aproxima a su probabilidad teórica.\n\nSi un evento ocurre varias veces seguidas, su probabilidad de ocurrencia en el siguiente intento disminuye.\n\nUna muestra grande siempre sigue una distribución normal, sin importar la distribución original.\n\nLa probabilidad de un evento se mantiene constante sin importar cuántas veces se repita el experimento.\n\n11. El número de autos que llegan a una estación de servicio en una hora sigue una distribución de Poisson con una media de 5 autos por hora. ¿Cuál es la probabilidad de que en la próxima hora lleguen exactamente 3 autos?\n\n0.1404\n\n0.2650\n\n0.6159\n\n0.8571\n\n12.¿Cuál de los siguientes ejemplos se modela mejor con una distribución discreta?\n\nLa cantidad de estudiantes que llegan tarde a clase en un día.\n\nLa temperatura promedio diaria en una ciudad.\n\nLa cantidad de lluvia caída en un mes en milímetros.\n\nLa distancia recorrida por un atleta en una maratón.\n\n13. Un semáforo permanece en luz verde durante 40 segundos, luego cambia a rojo por otros 40 segundos. Si una persona llega a un semáforo en un momento aleatorio del ciclo, ¿cuál es la probabilidad de que deba esperar?\n\n0.25\n\n0.50\n\n0.75\n\n1.00\n\n14. ¿Cuál de los siguientes eventos se modela mejor con una distribución binomial?\n\nEl tiempo de espera en una fila del banco.\nLa cantidad de clientes que llegan a una tienda en una hora.\n\nLa variación en la velocidad del viento en un día.\n\nEl número de respuestas correctas en un examen de opción múltiple con 10 preguntas.\n\n15. Si un estudiante obtiene una calificación de 85 en un examen donde la media es 75 y la desviación estándar es 5, ¿cuál es su puntaje en valor z?\n\n1.00\n\n2.00\n\n-1.00\n\n-2.00\n\n\n\n\n\nSpeegle, Deborah, y Benjamin Clair. 2021. Probability, Statistics, and Data: A Fresh Approach Using R. Boca Raton, FL: CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "inf.html",
    "href": "inf.html",
    "title": "6  Estadística inferencial",
    "section": "",
    "text": "6.1 Estimación\nLa estimación es el proceso mediante el cual intentamos aproximarnos al verdadero valor de un parámetro poblacional, como la media o la proporción, utilizando los estadísticos de una muestra. Dado que rara vez es posible observar a toda la población, la estimación nos permite hacer inferencias sobre sus características con base en una parte representativa de ella, la muestra. Gracias al Teorema del Límite Central, sabemos que, aunque los estadísticos obtenidos en distintas muestras pueden variar, con una muestra suficientemente grande, estos tienden a aproximarse al parámetro poblacional, siguiendo una distribución normal. Esta propiedad es fundamental en la estadística inferencial, ya que nos permite no solo obtener una estimación del parámetro desconocido, sino también medir la incertidumbre asociada a dicha estimación.\nPara cuantificar esta incertidumbre, utilizamos el intervalo de confianza, que nos proporciona un rango de valores dentro del cual esperamos que se encuentre el verdadero parámetro poblacional con un cierto nivel de certeza. Para ello, construimos una distribución normal alrededor del estadístico observado. Esta distribución refleja cómo variarían nuestras estimaciones si repitiéramos el muestreo múltiples veces, permitiéndonos delimitar un rango dentro del cual se encuentra el valor real del parámetro.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estadística inferencial</span>"
    ]
  },
  {
    "objectID": "inf.html#estimación",
    "href": "inf.html#estimación",
    "title": "6  Estadística inferencial",
    "section": "",
    "text": "6.1.1 Estimación y la regla empírica\nLa estimación es el proceso de construir un intervalo que, a partir de un estadístico calculado en una muestra, nos permite aproximarnos al valor del parámetro poblacional desconocido. Gracias al Teorema del Límite Central (TLC), sabemos que, bajo ciertas condiciones, la distribución de este estadístico tenderá a seguir una distribución normal.\nEntonces, utilizando las características de la distribución normal podemos construir intervalos de confianza. En particular, la regla empírica (68-95-99.7) nos dice que aproximadamente el 68%, 95% y 99.7% de los valores de una distribución normal se encuentran dentro de uno, dos o tres desviaciones estándar de la media, respectivamente (5.3). Utilizando esta información, podemos calcular intervalos que reflejen con un grado de confianza el rango en el que es probable que se encuentre el parámetro poblacional.\nHabíamos visto que, en la distribución muestral, la desviación estándar se convierte en el error estándar, por lo que debemos calcularlo primero. Su fórmula varía según el tipo de variable y el estadístico que estemos analizando.\nPara una media muestral, el error estándar se calcula dividiendo la desviación estándar de la población (\\(\\sigma\\)) entre la raíz cuadrada del tamaño de la muestra (\\(n\\)):\n\\[ \\text{Error estándar} = \\frac{\\sigma}{\\sqrt{n}}\\] donde:\n\n(\\(\\sigma\\)) es la desviación estándar de la población,\n(\\(n\\)) es el tamaño de la muestra.\n\nRecuerda que en la mayoría de los casos no conocemos \\(\\sigma\\), ya que es un parámetro poblacional desconocido, entonces: ¿cómo podemos conocer la desviación estándar de la población si, en realidad, solo tenemos una muestra? Usando la desviación estándar de la muestra (\\(s\\)) como una aproximación de \\(\\sigma\\).\n\\[\n\\text{Error estándar} = \\frac{s}{\\sqrt{n}}\n\\]\nPara una proporción, el error estándar se calcula distinto, ya que aquí estamos midiendo la variabilidad de una proporción (como el porcentaje de personas que apoyan un partido político, el porcentaje de personas con una característica particular, etc.). La fórmula del error estándar para una proporción (\\(\\hat{p}\\)) es:\n\\[\\text{Error estándar} = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\\]\ndonde:\n\n\\(\\hat{p}\\) es la proporción observada en la muestra,\n\\(n\\) es el tamaño de la muestra.\n\nEn ambos casos, el error estándar nos indica cuánto esperaríamos que varíe el estadístico muestral (ya sea la media o la proporción de una muestra) respecto al valor real del parámetro poblacional.\nObserva como el tamaño de la muestra (\\(n\\)) aparece en el denominador de ambas fórmula. Esto significa que a medida que la muestra crece, el error estándar disminuye. En términos intuitivos, cuanto más grande sea la muestra, más información tendremos sobre la población, lo que reduce la variabilidad entre las estimaciones obtenidas a partir de diferentes muestras.\n\n\n6.1.2 Nivel de Confianza y Alfa\nCuando construimos un intervalo de confianza, elegimos un nivel de confianza para indicar cuánta certeza queremos tener de que el intervalo contiene el verdadero valor del parámetro poblacional. Por ejemplo, si elegimos un nivel de confianza del 95%, estamos diciendo que, en el 95% de los casos, el intervalo construido incluirá el valor real de la población si repitiéramos el muestreo muchas veces. El nivel de confianza representa la probabilidad de que el intervalo contenga el verdadero valor.\nAl momento de seccionarlo tenemos que ser consciente que existe una relación inversa entre precisión y certeza en los intervalos de confianza. Cuanto mayor sea el nivel de confianza (y, por tanto, nuestra certeza de que el intervalo contiene el verdadero valor), más amplio será el intervalo y menor será la precisión. Esto se debe a que un nivel de confianza alto implica cubrir una mayor porción de la distribución, extendiendo el rango del intervalo para asegurar que se incluya el valor poblacional. Por el contrario, cuando optamos por un nivel de confianza menor (como el 90%), el intervalo se vuelve más estrecho, aumentando la precisión pero con menor certeza de que el intervalo incluya el verdadero valor poblacional.\nEsta relación inversa nos obliga a elegir un nivel de confianza que equilibre nuestras necesidades de precisión y certeza según el contexto del análisis. Comúnmente usamos niveles de confianza de:\n90% (nos da un poco menos de certeza, con intervalos más estrechos),\n95% (el más utilizado, balancea certeza y ancho del intervalo),\n99% (mayor certeza, pero con un intervalo más amplio).\nEste nivel de confianza se relaciona directamente con alfa (\\(\\alpha\\)), que es el complemento del nivel de confianza. \\(\\alpha\\) representa la probabilidad de error o el riesgo de equivocación que estamos dispuestos a aceptar (ver Tipos de error)\n\\[ \\alpha = 1 - \\text{Nivel de Confianza} \\]\nPor ejemplo:\n\nSi tenemos un nivel de confianza del 95%, entonces \\(\\alpha = 0.05\\).\nSi nuestro nivel de confianza es del 90%, entonces \\(\\alpha = 0.10\\).\nSi optamos por un nivel de confianza del 99%, entonces \\(\\alpha = 0.01\\).\n\nEn el caso de un intervalo de confianza del 95%, \\(\\alpha = 0.05\\), lo que significa que hay un 5% de probabilidad de que el intervalo calculado no contenga el verdadero valor poblacional. Este margen de error del 5% normalmente se reparte en ambos extremos de la distribución (2.5% a la izquierda y 2.5% a la derecha) pero en algunas pruebas también puede distribuirse a un solo lado.\nPara construir un intervalo de confianza al estimar una proporción, debemos determinar cuántos errores estándar debemos desplazarnos a la izquierda y a la derecha de la media de la distribución muestral para capturar un área de la distribución que corresponda al nivel de confianza seleccionado. ¿Recuerdas el valor z en la distribución normal? (8.5). La distribución de las proporciones muestrales sigue aproximadamente una distribución normal, lo que nos permite utilizar los valores críticos \\(z\\) para definir los límites del intervalo.\nEl valor \\(z\\) representa cuántos errores estándar debemos alejarnos de la media a ambos lados para capturar la fracción deseada de la distribución. Es decir, nos permite establecer un rango dentro del cual se encuentra la verdadera proporción poblacional. Este valor crítico depende del nivel de confianza elegido, ya que cuanto mayor sea el nivel de confianza, más lejos debemos extender el intervalo para asegurarnos de que incluya el parámetro poblacional en la mayoría de los casos.\nDependiendo del nivel de confianza elegido, los valores críticos \\(z\\) más comunes son:\n\n90% de confianza: \\(z = 1.645\\)\n\n95% de confianza: \\(z = 1.96\\)\n\n99% de confianza: \\(z = 2.576\\)\n\nEstos valores provienen de la distribución normal estándar, donde el área bajo la curva entre \\(-z\\) y \\(+z\\) corresponde al nivel de confianza seleccionado. Cuanto mayor sea el nivel de confianza, mayor será el valor crítico \\(z\\), lo que significa que el intervalo de confianza será más amplio para abarcar una mayor proporción de la distribución.\nCuando en lugar de una proporción estamos realizando inferencias sobre la media poblacional, la situación cambia ligeramente. Si conociéramos la desviación estándar de la población, podríamos seguir utilizando la distribución normal de la población para construir el intervalo de confianza. Sin embargo,recuerda que en la mayoría de los casos, no contamos con ella, por lo que debemos estimarla a partir de la muestra (5.7).\nEsta estimación introduce incertidumbre adicional, lo que nos obliga a usar la distribución t de Student en lugar de la normal. La distribución t es similar a la normal, pero tiene colas más gruesas, lo que refleja la mayor variabilidad esperada cuando trabajamos con la desviación estándar muestral (\\(s\\)) en lugar de la poblacional (\\(\\sigma\\)).\nLa diferencia entre la distribución t y la normal es más notable en muestras pequeñas. A medida que el tamaño de la muestra aumenta, la estimación de la desviación estándar poblacional mejora, y la distribución t se aproxima cada vez más a la normal. Generalmente, cuando el tamaño muestral es mayor a 30 observaciones, la diferencia entre ambas distribuciones es mínima.\nLa forma de la distribución t está determinada por un parámetro llamado grados de libertad (\\(df\\), degrees of freedom), que refleja cuánta información útil tenemos en los datos para estimar la variabilidad. Los grados de libertad indican cuántos valores en la muestra pueden variar libremente antes de que el resto esté completamente determinado. Por ejemplo, si tenemos una muestra de \\(n\\) datos solo \\(n - 1\\) valores pueden tomar cualquier valor antes de que el último quede determinado automáticamente. Entonces, los grados de libertad se calculan como \\(df = n - 1\\).\nCuando el número de grados de libertad es bajo, la distribución t tiene colas más gruesas que la normal, reflejando la mayor incertidumbre en la estimación de la variabilidad. A medida que los grados de libertad aumentan, la distribución t se estrecha y se parece más a la normal.\n\n\n\nExtraído de: https://www.scribbr.com/statistics/t-distribution/\n\n\n\n\n6.1.3 Construcción del Intervalo de Confianza\nCon el valor \\(z\\) o \\(t\\) correspondiente al nivel de confianza deseado, el intervalo de confianza para la media poblacional \\(\\mu\\) se construye alrededor de la media muestral \\(\\bar{x}\\) y el error estándar (\\(\\text{EE}\\)):\n\\[ \\text{Intervalo de Confianza} = \\bar{x} \\pm z \\times \\text{EE} \\]\nó\n\\[ \\text{Intervalo de Confianza} = \\bar{x} \\pm t \\times \\text{EE} \\]\ndonde:\n\n\\(\\bar{x}\\) es la media de la muestra,\n\\(z\\) o \\(t\\) es el valor correspondiente al nivel de confianza deseado de la distribución normal (\\(z\\)) o t de Student(\\(t\\)),\n\\(\\text{EE}\\) es el error estándar.\n\nEste intervalo nos da un rango dentro del cual esperamos que se encuentre la verdadera media poblacional con el nivel de confianza seleccionado. Por ejemplo, para una distribución normal.\n\n\n\n\n\n\n\n\n\nEn el caso de la distribución t, recuerda que su forma cambia en función de los grados de libertad (\\(df\\)). A medida que los grados de libertad aumentan, la distribución se va estrechando y sus colas se hacen menos pronunciadas, acercándose cada vez más a la normal estándar. Veamos cómo varía la distribución t para un nivel de confianza del 95% en diferentes valores de \\(df\\).\n\n\n\n\n\n\n\n\n\nCuando el número de grados de libertad (\\(df\\)) es pequeño, la estimación de la desviación estándar poblacional a partir de la muestra es menos precisa, lo que introduce una mayor variabilidad en la distribución de la media muestral. Para compensar esta incertidumbre, la distribución t tiene colas más gruesas y valores críticos (\\(t\\)) más grandes, lo que significa que, para un mismo nivel de confianza, los intervalos de confianza deben ser más amplios. A medida que el tamaño de la muestra aumenta, la estimación de la desviación estándar poblacional se vuelve más precisa, reduciendo la variabilidad en la distribución muestral y haciendo que la distribución t se asemeje cada vez más a la normal estándar. Esto a larga hace que los valores críticos \\(t\\) se acerquen a los valores \\(z\\) de la distribución normal al punto en el que la diferencia es mínima.\nPor ejemplo, calculemos los valores críticos \\(z\\) y \\(t\\) para un nivel de confianza del 99%, comparando la distribución normal con la distribución t cuando los grados de libertad son \\(df = 300\\).\nCuando calculamos los valores críticos para un nivel de confianza del 99%, normlamente dividimos la probabilidad restante (\\(\\alpha\\)) entre dos, ya que los intervalos de confianza van a ambos lados. Dado que el nivel de confianza es del 99%, la probabilidad total en las colas de la distribución es:\n\\[\n\\alpha = 1 - 0.99 = 0.01\n\\]\nComo el intervalo es simétrico, la cola izquierda contiene \\(\\alpha/2 = 0.005\\) y la cola derecha también \\(alpha/2 = 0.005\\).\n\n\n\nElaboración propia\n\n\nPor ello, para asegurarnos de que el intervalo de confianza abarque el 99% central de la distribución simétrica, debemos establecer un 0.5% en cada cola.\nPor lo tanto, el valor crítico (\\(z\\)) de la distribución normal se obtiene usando qnorm()\n\nqnorm(0.005)\n\n[1] -2.575829\n\n\nMientras que el valor crítico (\\(t\\)) para df = 300 se obtiene usando qt() definiendo df\n\nqt(0.005, df = 300)\n\n[1] -2.592316\n\n\nComo puedes ver, la diferencia con la distribución normal es mínima cuando los \\(df\\) son mayores a 30.\n\n\n6.1.4 Intervalo de Confianza para una Media\nFlor está estudiando la altura promedio de un grupo de personas y solo tiene acceso a una muestra de 350 individuos. Quiere calcular un intervalo de confianza para estimar la altura promedio de la población a partir de esta muestra.\n\nmuestra = read_csv('alturas.csv')\n\n\nglimpse(muestra)\n\nRows: 350\nColumns: 1\n$ alturas &lt;dbl&gt; 164.3952, 167.6982, 185.5871, 170.7051, 171.2929, 187.1506, 17…\n\n\n\nmuestra %&gt;% \n  ggplot(aes(x = alturas)) +\n  geom_histogram(binwidth = 2, \n                 fill = \"skyblue\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  labs(title = \"Histograma de las alturas de la muestra\",\n       x = \"Altura (cm)\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nDescribe la variable altura de nuestra muestra\nTamaño de la muestra\n\nlength(muestra$alturas)\n\n[1] 350\n\n\n\\(n = 350\\)\nMedia de la muestra\n\nmean(muestra$alturas)\n\n[1] 170.2981\n\n\n\\(\\bar x \\approx  170\\)\nDesviación estándar de la muestra\n\nsd(muestra$alturas)\n\n[1] 9.569994\n\n\n\\(s = 9.57\\)\nDefine un nivel de confianza: Nivel de confianza = 95%\nPaso 1: Determinar el valor t\nPara un nivel de confianza del 95% (el 5% restante de divide en ambos lados). Por lo que el valor \\(t\\) para cada lado será en el área que represente el 2.5%. En vez de usar qnorm, usa qt ya que hace uso de las distribución \\(t\\), especificando los grados de libertad.\nDefine los grados de libertad (\\(n - 1\\))\n\ngrad_lib = length(muestra$alturas) - 1\ngrad_lib\n\n[1] 349\n\n\n\nqt(0.025, df = grad_lib)\n\n[1] -1.966785\n\nqt(0.975, df = grad_lib)\n\n[1] 1.966785\n\n\nEl valor \\(t\\) correspondiente es aproximadamente 1.96 a cada lado. Este valor indica cuántos errores estándar abarcan el 95% de la distribución normal alrededor de la media, que es lo que necesitamos para construir el intervalo.\nPaso 2: Calcular el Error Estándar\nEl error estándar (\\(EE\\)) es la desviación estándar de la districuión muestral. Por ello nos indica cuánto esperaríamos que varíe la media muestral con respecto a la media poblacional. Es importante observar que el tamaño de la muestra (\\(n\\)) se encuentra en el denominador de la fracción. Esto nos indica que, cuanto mayor sea el tamaño de la muestra, menor será el error estándar, lo cual hace que el intervalo de confianza sea más preciso.\nLa fórmula para calcular el error estándar de la media es:\n\\[ \\text{EE} = \\frac{s}{\\sqrt{n}}\\] donde:\n\ns es la desviación estándar de la muestra (9.57cm), y\nn es el tamaño de la muestra (350).\n\nSustituye los valores a la formula del cálculo del EE:\n\\[\\text{EE} = \\frac{9.57}{\\sqrt{350}} = \\frac{9.57}{18.71} \\approx 0.51\\]\nPaso 3: Calcular el Intervalo de Confianza\nCon el error estándar (\\(EE\\)) y el valor \\(t\\), es posible calcular el intervalo de confianza para la media poblacional. La fórmula general para el intervalo de confianza se expresa como:\n\\[\\text{IC} = \\bar{x} \\pm t \\times \\text{EE}\\]\nEl valor t indica cuántas desviaciones estándar (error estándar \\(EE\\)) debemos alejarnos de la media muestral (\\(\\bar{x}\\)) hacia ambos lados para cubrir una proporción específica de la distribución, determinada por el nivel de confianza deseado. El término \\(t \\times EE\\) se denomina margen de error (ME) y determina la amplitud del intervalo.\nSustituyendo los valores:\n\\[\\text{IC} = 170.29 \\pm 1.96 \\times 0.51\\] Entonces, el intervalo de confianza es:\nSi la media es 170.29, el intervalo de confianza se expresaría así:\n\\[\\text{IC} = 170.29 \\pm 0.99 = [169.30, 171.28]\\]\nes decir:\n\\[\\text{IC} = 169.30 \\text{ hasta } 171.28\\]\nEsto indica que, con un nivel de confianza del 95%, estimamos que la media poblacional se encuentra entre 169.30 y 171.28.\nInterpretación del resultado\nCon un nivel de confianza del 95%, Flor estima que la altura promedio de la población está entre 169.30 cm y 171.28 cm.\nLa función t.test() en R realiza automáticamente el cálculo del intervalo de confianza para la media de una muestra, ajustándose según el tamaño de la muestra y asumiendo la distribución t de Student\nInternamente, t.test() sigue los siguientes pasos para calcular el intervalo de confianza:\n1. Calcula la media de la muestra y la desviación estándar muestral.\n2. Determina el error estándar, que mide la variabilidad esperada de la media muestral con respecto a la media poblacional.\n3. Ajusta el valor \\(t\\) de acuerdo con el nivel de confianza especificado. En este caso, para un nivel de confianza del 95%.\n4. Calcula el intervalo de confianza sumando y restando el margen de error (valor crítico multiplicado por el error estándar) a la media muestral.\n\nprueba_t = t.test(muestra, conf.level = 0.95)\n\nprueba_t$conf.int\n\n[1] 169.2921 171.3042\nattr(,\"conf.level\")\n[1] 0.95\n\n\nGráficamente podemos visualizarlo así:\n\n\n\n\n\n\n\n\n\n\n\n6.1.5 Intervalo de Confianza para una Proporción\nAhora supongamos que está estudiando la intención de voto en una población y ha recolectado una muestra de 350 personas. De estas, 210 han manifestado que votarían por el partido A. Quiere calcular un intervalo de confianza para estimar la proporción de la población que votaría por el partido A.\n\napoyo = read_csv('apoyo.csv')\n\n\nglimpse(apoyo)\n\nRows: 350\nColumns: 2\n$ id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ apoyo &lt;chr&gt; \"A\", \"A\", \"A\", \"Otro\", \"A\", \"Otro\", \"Otro\", \"Otro\", \"Otro\", \"A\",…\n\n\n\napoyo %&gt;% \n  ggplot(aes(x = apoyo, fill = apoyo)) +\n  geom_bar(color = \"black\") +\n  labs(title = \"Distribución de la intención de voto en la muestra\",\n       x = \"Opción de voto\",\n       y = \"Frecuencia\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nDescribe la variable apoyo en su muestra.\nTamaño de la muestra:\n\nlength(apoyo$apoyo)\n\n[1] 350\n\n\n\\(n = 350\\)\nProporción de la muestra (porcentaje que votaría por el partido A):\n\nprop_muestra = mean(apoyo$apoyo == \"A\")\nprop_muestra\n\n[1] 0.6\n\n\n\\(\\hat{p} = 0.6\\)\nDefine un nivel de confianza: Nivel de confianza = 95%\nPaso 1: Determinar el Valor Z\nAl estimar una proporción no hay problema con que use la distribución normal por lo que el valor crítico es \\(z\\) y lo puede calcular haciendo uso de qnorm.\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\nPara un nivel de confianza del 95%, el valor \\(z\\) correspondiente es aproximadamente 1.96. Este valor indica cuántos errores estándar (z) abarcan el 95% de la distribución normal alrededor de la proporción muestral.\nPaso 2: Calcular el Error Estándar\nEl error estándar (EE) nos indica cuánto esperaríamos que varíe la proporción muestral con respecto a la proporción poblacional. De la misma forma, cuanto mayor sea el tamaño de la muestra, menor será el error estándar, lo cual hace que el intervalo de confianza sea más preciso.\nLa fórmula para calcular el error estándar de una proporción es:\n\\[ \\text{EE} = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\]\ndonde:\n\n\\(\\hat{p}\\) es la proporción de la muestra (0.6),\n\\(n\\) es el tamaño de la muestra (350).\n\nSustituimos los valores en la fórmula para el cálculo del EE:\n\\[\\text{EE} = \\sqrt{\\frac{0.6 \\times (1 - 0.6)}{350}} = \\sqrt{\\frac{0.6 \\times 0.4}{350}} \\approx 0.0252\\]\nPaso 3: Calcular el Intervalo de Confianza\nCon el error estándar y el valor \\(z\\), ahora puede calcular el intervalo de confianza para la proporción poblacional. La fórmula general para el intervalo de confianza es:\n\\[\\text{IC} = \\hat{p} \\pm z \\times \\text{EE}\\]\nSustituyendo los valores:\n\\[\\text{IC} = 0.6 \\pm 1.96 \\times 0.0252\\]\nCalculando el margen de error:\n\\[1.96 \\times 0.0252 \\approx 0.0494\\]\nEntonces, el intervalo de confianza es:\n\\[\\text{IC} = 0.6 \\pm 0.0494 = [0.5506, 0.6494]\\]\nes decir:\n\\[\\text{IC} = 0.5506 \\text{ hasta } 0.6494\\]\nEsto indica que, con un nivel de confianza del 95%, estimamos que la proporción de la población que votaría por el partido A se encuentra entre 55.06% y 64.94%.\nInterpretación del Resultado\nCon un nivel de confianza del 95%, Flor estima que entre el 55.06% y el 64.94% de la población votaría por el partido A.\nPodemos calcular este intervalo de confianza en R directamente usando prop.test(), una función que facilita los cálculos de intervalos de confianza para proporciones.\n\nprueba_prop = prop.test(210, 350, conf.level = 0.95)\n\nprueba_prop$conf.int\n\n[1] 0.5464154 0.6513532\nattr(,\"conf.level\")\n[1] 0.95\n\n\nGráficamente:\n\n\n\n\n\n\n\n\n\nPuedes interactuar con el conceptos del intervalo en confianza en la Datáfora Interactiva: [Click Aquí]\nO usa el QR:\n\n\n\nDirígete a la pestaña Intervalos de confianza",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estadística inferencial</span>"
    ]
  },
  {
    "objectID": "inf.html#contraste-de-hipótesis",
    "href": "inf.html#contraste-de-hipótesis",
    "title": "6  Estadística inferencial",
    "section": "6.2 Contraste de hipótesis",
    "text": "6.2 Contraste de hipótesis\nEl contraste de hipótesis es una herramienta estadística que nos permite decidir si un resultado observado en una muestra nos da suficiente evidencia para apoyar una afirmación sobre una población. Existen varias pruebas pero se caracterizan por ser todas un proceso organizado que nos ayuda a diferenciar entre resultados que podrían ocurrir solo por variación aleatoria y aquellos que son lo suficientemente inusuales como para indicar un efecto real.\nAunque existen diversas pruebas, todas siguen un proceso estructurado y sistemático diseñado para diferenciar entre resultados que podrían ocurrir únicamente por variación aleatoria y aquellos que son estadísticamente significativos, es decir, lo suficientemente inusuales como para sugerir la presencia de un patrón o efecto real en la población.\n\n6.2.1 Pasos del Contraste de Hipótesis\nI. Planteamiento de Hipótesis:\nComenzamos formulando dos hipótesis opuestas:\n\nLa hipótesis nula (\\(H_0\\)) representa la idea de “no hay efecto” o “no hay diferencia”. Es nuestra afirmación de partida y sugiere que cualquier variación que veamos es simplemente producto de la variación aleatoria inherente a nuestra muestra.\n\nLa hipótesis alternativa (\\(H_1\\)) representa la afirmación que queremos evaluar. Indica que sí hay un efecto o diferencia en la población.\n\nLa hipótesis nula nos da un punto de referencia neutral contra el cual comparamos nuestros resultados para evaluar si son suficientemente distintos como para sugerir algo real.\nDependiendo de la dirección en la que buscamos evidencia contra \\(H_0\\) el análisis puede ser:\n\nPrueba bilateral (de dos colas): Se usa cuando queremos evaluar si hay una diferencia sin importar la dirección. Por ejemplo, si estudiamos el impacto de una nueva política pública en la tasa de empleo, \\(H_1\\) indicaría que la política cambia la tasa de empleo, ya sea aumentándola o reduciéndola. En este caso, consideramos ambos extremos de la distribución para determinar si el efecto observado es lo suficientemente inusual como para rechazar \\(H_0\\).\nPrueba unilateral (de una cola): Se usa cuando tenemos una expectativa clara sobre la dirección del efecto. Por ejemplo, si evaluamos si un programa de capacitación mejora las habilidades laborales, la hipótesis alternativa indicaría que el programa aumenta el nivel de habilidades (y no simplemente que lo cambia en cualquier dirección). Aquí, solo nos interesa una de las colas de la distribución.\n\nII. Establecimiento de la Distribución Nula\nIncluso si \\(H_0\\) es cierta, los resultados de una muestra pueden fluctuar debido a la variación aleatoria. Sin embargo, estas fluctuaciones siguen un patrón: la mayoría de los valores observados estarán cerca de la media de la distribución nula, y conforme nos alejamos de esta media, los valores se vuelven menos probables. Esto significa que, en cualquier prueba de hipótesis, necesitamos un criterio que nos ayude a determinar cuándo un resultado es lo suficientemente diferente de lo esperado bajo \\(H_0\\) como para considerarlo significativo. . Para hacer esta comparación, necesitamos una distribución de referencia que nos muestre cómo se comportaría nuestro estadístico (como la media o la proporción) si la hipótesis nula fuera verdadera.\nAl igual que podemos construir una distribución de posibles valores para el estadístico observado, podemos construir una distribución para la hipótesis nula. Recordando el Teorema del Límite Central (TLC), sabemos que, si tomamos muchas muestras de una población y calculamos su estadístico (como la media), los resultados tienden a seguir una distribución aproximadamente normal alrededor del valor supuesto por la hipótesis nula. Esta distribución para la hipótesis nula nos indica qué tan probable es obtener diferentes valores del estadístico bajo la suposición de que no hay efecto o diferencia real.\n\n\n\n\n\n\n\n\n\nIII. Nivel de confianza y definición de la zonas criticas:\nLa esencia de un contraste de hipótesis radica en evaluar qué tan diferente o extremo debe ser un valor observado para poder afirmar que no es probable si la hipótesis nula (\\(H_0\\)) fuera cierta, incluso considerando la variabilidad natural que ocurre por azar. En otras palabras, el contraste de hipótesis busca determinar si el resultado obtenido se desvía tanto de lo esperado bajo \\(H_0\\) que podemos justificar su rechazo. Este proceso nos permite distinguir entre variaciones que pueden explicarse simplemente por la aleatoriedad inherente al muestreo y aquellas que son lo suficientemente inusuales como para sugerir que existe un efecto real o una diferencia significativa.\nPara evaluar si un resultado es lo suficientemente extremo como para considerarlo evidencia en contra de \\(H_0\\), necesitamos definir un umbral que nos ayude a tomar esta decisión. Este umbral se establece a través del nivel de confianza y el nivel de significancia (\\(\\alpha\\)).\nSabiendo que:\n\\[ \\text{Nivel de confianza} = 1 - \\alpha \\]\nEl nivel de significancia \\(\\alpha\\) nos dice cuánto estamos dispuestos a aceptar el riesgo de cometer un Error Tipo I (rechazar \\(H_0\\) cuando en realidad es cierta). Si elegimos \\(\\alpha = 0.05\\), estamos definiendo que solo el 5% de los valores más extremos de la distribución nula serán considerados “demasiado improbables” bajo \\(H_0\\).\nPor ejemplo, si elegimos un nivel de confianza del 95%, significa que queremos estar seguros en un 95% de que la conclusión que tomemos será correcta y aceptamos un 5% de riesgo de cometer un Error Tipo I (rechazar \\(H_0\\) cuando en realidad es cierta). Esto implica que el nivel de significancia en este caso es:\n\\[ \\alpha = 0.05 \\]\nEl valor de \\(\\alpha\\) nos ayuda a definir las zonas críticas de la distribución nula, que son las regiones donde un resultado se considerará lo suficientemente improbable como para rechazar \\(H_0\\). La ubicación exacta de estas zonas depende del tipo de prueba que estemos realizando:\nSi realizamos una prueba bilateral (de dos colas), el nivel de significancia se divide en dos extremos de la distribución nula, dejando \\(\\frac{\\alpha}{2}\\) en cada cola. Esto significa que rechazamos \\(H_0\\) si el valor observado es demasiado grande o demasiado pequeño en comparación con lo que esperaríamos si \\(H_0\\) fuera cierta.\n\n\n\n\n\n\n\n\n\nSi realizamos una prueba unilateral (de una cola), todo el nivel de significancia \\(\\alpha\\) se coloca en una sola cola de la distribución, dependiendo de si estamos evaluando un efecto mayor o menor. En este caso, rechazamos \\(H_0\\) solo si el resultado observado es significativamente mayor o significativamente menor de lo esperado bajo \\(H_0\\).\n\n\n\n\n\n\n\n\n\nPara determinar qué tan extremo debe ser un resultado para considerarlo estadísticamente significativo, usamos el valor crítico (\\(z\\) crítico o \\(t\\) crítico) que determina la distancia en errores estándar de la media de la distribución.\nPor ejemplo, en una prueba basada en la distribución normal estándar, el \\(z\\) crítico para \\(\\alpha = 0.05\\) en una prueba bilateral es el 2.5% más extremo a cada lado, por lo que aproximadamente \\(\\pm1.96\\)\n\nqnorm(0.975)\n\n[1] 1.959964\n\nqnorm(0.025)\n\n[1] -1.959964\n\n\n\n\n\n\n\n\n\n\n\nEsto que significa que cualquier estadístico que caiga fuera de este rango se considera lo suficientemente improbable bajo \\(H_0\\) como para rechazarla. Además, recuerda que si la muestra es pequeña o la varianza es desconocida, usamos la distribución \\(t\\) de Student, en cuyo caso el \\(t\\) crítico dependerá del tamaño muestral y los grados de libertad.\nIV. Comparación del Valor Observado con la Distribución Nula\nYa que escogimos \\(\\alpha = 0.05\\), si el valor observado cae en el 5% más extremo de la distribución nula (es decir, en las áreas menos probables), lo consideramos lo suficientemente inusual como para que no pueda explicarse únicamente por la variabilidad aleatoria bajo la hipótesis nula (\\(H_0\\)). En este caso, rechazamos \\(H_0\\) y favorecemos la hipótesis alternativa (\\(H_1\\)).\nSi el valor observado está dentro del 95% de la distribución nula, significa que se encuentra dentro del rango esperado por variación natural. No es lo suficientemente inusual, por lo que no rechazamos \\(H_0\\). En cambio, si el valor observado cae en el 5% más extremo de la distribución, consideramos que es lo suficientemente improbable bajo \\(H_0\\), por lo que tenemos suficiente evidencia para rechazarla en favor de \\(H_1\\).\nUtilizamos el valor \\(t\\) y el valor \\(z\\) para evaluar cuán lejos está un valor observado de lo que esperaríamos bajo la hipótesis nula.\n\nPara pruebas sobre una media: \\[\nt = \\frac{\\bar{x} - \\mu_0}{\\text{Error estándar}}, \\quad \\text{donde } \\mu_0 \\text{ es la media bajo } H_0.\n\\]\nPara pruebas sobre una proporción: \\[\nz = \\frac{\\hat{p} - p_0}{\\text{Error estándar}}, \\quad \\text{donde } p_0 \\text{ es la proporción bajo } H_0.\n\\] No olvidar que los valores \\(z\\) y \\(t\\) representan distancia (en errores estándar) con respecto al centro de la distribución.\n\nV. Decisión Basada en el Nivel de Significancia (\\(\\alpha\\)) y el p-valor:\nEl contraste de hipótesis, en esencia, usa una distribución nula para comparar los resultados observados con lo que esperaríamos bajo la hipótesis nula. Esto permite hacer una evaluación objetiva, decidiendo si el resultado es suficientemente extremo como para ser significativo o si podría explicarse simplemente por la variabilidad natural en los datos. Por lo que utilizamos nuestro estadístico observado y evaluamos si se encuentra en la zona crítica expresada en valores \\(t\\) o \\(z\\).\n\\[\n  \\text{Región de rechazo: } |t| &gt; t_{\\text{crítico}} \\quad \\text{o} \\quad |z| &gt; z_{\\text{crítico}}\n\\]\nEl valor \\(t\\) o \\(z\\) observado puede estar tanto a la izquierda como a la derecha, por ello su valor absoluto.\nHay que ser cautelosos ya que con este proceso es que no estamos diciendo que \\(H_0\\) sea absolutamente falsa, sino que el resultado observado en nuestra muestra es lo suficientemente improbable como para justificar dudar de ella y considerar que \\(H_1\\) es más plausible.\nOtra forma de evaluar si el resultado es suficientemente extremo es mediante el p-valor, que representa la probabilidad de obtener un estadístico de prueba igual o más extremo que el observado, bajo la suposición de que \\(H_0\\) es verdadera.\nAmbos enfoques (comparar el estadístico con el valor crítico o utilizar el p-valor) son dos caras de la misma moneda y llevan a la misma conclusión. Mientras que la comparación con el valor crítico permite establecer de manera directa si un resultado cae dentro de la región de rechazo, el p-valor ofrece una medida de cuán improbable es el resultado bajo \\(H_0\\).\nPara tomar una decisión utilizando el p-valor:\n\nSi \\(p \\leq \\alpha\\) → Se rechaza \\(H_0\\) ya que el resultado es demasiado improbable bajo la hipótesis nula.\n\nSi \\(p &gt; \\alpha\\) → No se rechaza \\(H_0\\) ya que el resultado no es lo suficientemente extremo como para concluir que hay una diferencia significativa.\n\nEste enfoque es especialmente útil porque las pruebas estadísticas en R reportan automáticamente el p-valor.\n\n\n6.2.2 Contraste de Hipótesis para una Media\nFlor está investigando si la altura promedio de una población es diferente de 170 cm. Para ello ha recolectado una muestra de 350 individuos y pretende realizar una prueba de hipótesis.\nPlanteamiento de Hipótesis:\n\nHipótesis nula (\\(H_0\\)): La altura promedio de la población es igual a 170 cm. \\[ H_0: \\mu = 170 \\]\nHipótesis alternativa (\\(H_1\\)): La altura promedio de la población es diferente de 170 cm. \\[ H_1: \\mu \\neq 170 \\]\n\n\nmuestra = read_csv('alturas.csv')\n\n\nglimpse(muestra)\n\nRows: 350\nColumns: 1\n$ alturas &lt;dbl&gt; 164.3952, 167.6982, 185.5871, 170.7051, 171.2929, 187.1506, 17…\n\n\nDescribe la variable altura de nuestra muestra.\nTamaño de la muestra:\n\nlength(muestra$alturas)\n\n[1] 350\n\n\n\\(n = 350\\)\nMedia de la muestra:\n\nmean(muestra$alturas)\n\n[1] 170.2981\n\n\n\\(\\bar{x} = 170.3\\)\nDesviación estándar de la muestra:\n\nsd(muestra$alturas)\n\n[1] 9.569994\n\n\n\\(s = 9.57\\)\nDefine un nivel de significancia: \\(\\alpha = 0.05\\)\nRealización de la Prueba en R\nPara realizar el contraste de hipótesis, utilizamos t.test() en R especificando el valor de la media poblacional bajo la hipótesis nula (mu = 170). El argumento alternative en la función t.test() en R permite definir el tipo de prueba de hipótesis que se realizará. Específicamente, determina si la prueba será bilateral (de dos colas) o unilateral (de una cola).\n\n# Prueba t para una media con hipótesis de que la media poblacional es 170\nprueba_t = t.test(muestra$alturas, \n                  mu = 170, \n                  conf.level = 0.95,\n                  alternative = \"two.sided\")\n\nprueba_t\n\n\n    One Sample t-test\n\ndata:  muestra$alturas\nt = 0.58284, df = 349, p-value = 0.5604\nalternative hypothesis: true mean is not equal to 170\n95 percent confidence interval:\n 169.2921 171.3042\nsample estimates:\nmean of x \n 170.2981 \n\n\nPuedes llamar exclusivamente al p-valor:\n\nprueba_t$p.value\n\n[1] 0.5603749\n\n\nEl p-valor es 0.56\nInterpretación del Resultado\nSi el p-valor es menor que \\(\\alpha = 0.05\\), rechazamos la hipótesis nula y concluye que la altura promedio de la población es significativamente diferente de 170 cm. Si el p-valor es mayor que 0.05, no tendría suficiente evidencia para rechazar la hipótesis nula, y por lo tanto, no podría afirmar que la altura promedio sea diferente de 170 cm.\nGráficamente:\n\n\n\n\n\n\n\n\n\nEn este caso, la gráfica no muestra el intervalo de confianza de una estimación sino la distribución nula, que representa cómo se distribuirían las medias muestrales si la hipótesis nula fuera cierta. La línea negra en el centro marca el valor hipotético de la media \\(\\mu_0 = 170\\), mientras que la línea verde indica la media muestral observada \\(\\bar{x} = 170.3\\).\nLas regiones sombreadas en rojo representan las áreas de rechazo, es decir, los valores extremos donde, si la media muestral cayera dentro de estos rangos, consideraríamos que hay suficiente evidencia para rechazar \\(H_0\\). En nuestro ejemplo, los valores de corte para el rechazo están aproximadamente en 168.99 y 171.01 cm.\nEl p-valor obtenido es 0.5604, lo que indica que la media muestral observada no es lo suficientemente extrema para rechazar la hipótesis nula. En otras palabras, la diferencia entre la media muestral (\\(\\bar{x} = 170.3\\)) y la media hipotética (\\(\\mu_0 = 170\\)) no es suficientmente significativa para que Flor pueda afirmar que la media poblacional sea diferente de 170 cm.\n\n\n6.2.3 Contraste de Hipótesis para una Proporción\nAhora, supongamos que está investigando si la proporción de personas que votarían por el partido A en la población es diferente de 50%. Realizamos una encuesta con 350 personas, de las cuales 210 manifestaron que votarían por el partido A.\nPlanteamiento de Hipótesis:\n\nHipótesis nula (\\(H_0\\)): La proporción de personas que votan por el partido A es igual a 60%. \\[ H_0: p = 0.6 \\]\nHipótesis alternativa (\\(H_1\\)): La proporción de personas que votan por el partido A es diferente de 60%. \\[ H_1: p \\neq 0.6 \\]\n\n\napoyo = read_csv('apoyo.csv')\n\n\nglimpse(apoyo)\n\nRows: 350\nColumns: 2\n$ id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ apoyo &lt;chr&gt; \"A\", \"A\", \"A\", \"Otro\", \"A\", \"Otro\", \"Otro\", \"Otro\", \"Otro\", \"A\",…\n\n\nDescribe la variable intencion en su muestra.\nTamaño de la muestra:\n\nlength(apoyo$apoyo)\n\n[1] 350\n\n\n\\(n = 350\\)\n(cambiar por 0.62)\nProporción de la muestra (porcentaje que votaría por el partido A):\n\nprop_muestra = mean(apoyo$apoyo == \"A\")\nprop_muestra\n\n[1] 0.6\n\n\n\\(\\hat{p} = 0.6\\)\nDefinimos un nivel de significancia: \\(\\alpha = 0.05\\)\nRealización de la Prueba en R\nPara realizar el contraste de hipótesis para una proporción, usamos prop.test() en R especificando la hipótesis nula de que la proporción poblacional es 0.6.\n\nprueba_prop = prop.test(210, \n                        350, \n                        p = 0.5, \n                        conf.level = 0.95)\n\nprueba_prop\n\n\n    1-sample proportions test with continuity correction\n\ndata:  210 out of 350, null probability 0.5\nX-squared = 13.603, df = 1, p-value = 0.0002258\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5464154 0.6513532\nsample estimates:\n  p \n0.6 \n\n\n\nprueba_prop$p.value\n\n[1] 0.0002258415\n\n\nEl p-valor es 0.0002258415\nInterpretación del Resultado\nSi el p-valor es menor que \\(\\alpha = 0.05\\), rechazaría la hipótesis nula y llegaría a la conclusión que la proporción de personas que votarían por el partido A es significativamente diferente de 60%. Si el p-valor es mayor que 0.05, no tendría suficiente evidencia para rechazar la hipótesis nula, por lo que no podría afirmar que la proporción de votantes por el partido A difiera de 60%.\nGráficamente:\n\n\n\n\n\n\n\n\n\nDado que la proporción muestral observada 0.6 está claramente fuera de esta región crítica, el p-valor es extremadamente pequeño (0.000226), lo que indica que es altamente improbable obtener una proporción muestral tan distante de 0.5 solo por azar.\n\n\n6.2.4 Tipos de error\nDebemos ser conscientes de que al realizar una prueba estadística estamos tomando una decisión. Al generar una zona de rechazo, estamos marcando un límite que determina si los resultados son o no significativos. Esta decisión es dicotómica (si/no) y, por tanto, tiene sus limitaciones. Para empezar, muchas veces la realidad no es tan simple, y otras veces la intrínseca variabilidad aleatoria de nuestra muestra puede llevarnos a cometer errores al momento de tomar la decisión.\nEstos errores siempre están presentes y no podemos evadirlos. De hecho, la presencia de uno está inversamente relacionada con la presencia del otro. Es decir, debido a la variabilidad del muestreo, la decisión que tomemos siempre esta expuesta a determinada probabilidad de cometer un error. Existen cuatro escenarios posibles, dos en los que la decisión que se toma es correcta y dos en los que se comete un error. Empecemos con un ejemplo clásico primero y luego lo llevamos a nuestro campo.\nError Tipo I y Error Tipo II\nImagina que estamos ante un juicio sobre la culpabilidad de una persona. En este juicio:\n\nNuestra (\\(H_0\\)) es la opción conservadora: el acusado es inocente.\n\nNuestra (\\(H_1\\)) es la que propone un cambio y sobre la cual buscamos evidencia: el acusado es culpable.\n\nAhora, piensa en las cuatro posibles situaciones en las que se puede dar su veredicto:\n\n\n\n\n\n\n\n\nDecisión tomada\nEl acusado es realmente inocente (\\(H_0\\) es verdadera)\nEl acusado es realmente culpable (\\(H_1\\) es verdadera)\n\n\n\n\nDeclararlo culpable (rechazar \\(H_0\\))\nError Tipo I (\\(\\alpha\\)): Se condena a un inocente.\nCorrecto positivo: Se condena a un culpable. También llamado Poder (\\(1 - \\beta\\)).\n\n\nDeclararlo inocente (no rechazar \\(H_0\\))\nCorrecto negativo: Se absuelve a un inocente.\nError Tipo II (\\(\\beta\\)): Se absuelve a un culpable.\n\n\n\nPara desglosar la tabla. En el veredicto pueden suceder cuatro situaciones distintas, dependiendo de la decisión tomada y la realidad sobre la culpabilidad del acusado. Estos escenarios tienen su equivalente en estadística cuando realizamos una prueba de hipótesis.\n\nError Tipo I (\\(\\alpha\\)): Se condena a un inocente\nOcurre cuando se rechaza la hipótesis nula (\\(H_0\\)) siendo esta verdadera. En el juicio, esto significa que el acusado es realmente inocente, pero la decisión tomada es declararlo culpable. En términos estadísticos, es un falso positivo, es decir, se detecta “culpabilidad” cuando en realidad no la hay.\nError Tipo II (\\(\\beta\\)): Se absuelve a un culpable\nOcurre cuando no se rechaza la hipótesis nula (\\(H_0\\)) siendo esta falsa. En el juicio, esto significa que el acusado es culpable, pero la decisión tomada es declararlo inocente. En términos estadísticos, es un falso negativo, lo que significa que no se detecta la culpabilidad cuando realmente existe.\nDecisión correcta: Se condena a un culpable\nOcurre cuando se rechaza la hipótesis nula (\\(H_0\\)) y esta es falsa. En el juicio, esto significa que el acusado es culpable y la decisión tomada es declararlo culpable. En términos estadísticos, corresponde a un verdadero positivo y su probabilidad se conoce como poder de la prueba (\\(1 - \\beta\\)). El poder de la prueba representa la capacidad de la prueba estadística para rechazar correctamente la hipótesis nula cuando realmente es falsa. Un mayor poder implica una menor probabilidad de cometer un error tipo II.\nDecisión correcta: Se absuelve a un inocente\nOcurre cuando no se rechaza la hipótesis nula (\\(H_0\\)) y esta es verdadera. En el juicio, esto significa que el acusado es realmente inocente y la decisión tomada es declararlo inocente. En términos estadísticos, corresponde a un verdadero negativo, lo que implica que no se comete un error.\n\nExiste una relación directa entre los tipos de error y el poder de la prueba. Si reducimos la probabilidad de cometer un error tipo I (\\(\\alpha\\)), generalmente aumentamos la probabilidad de cometer un error tipo II (\\(\\beta\\)), y viceversa. Recuerda que somos nosotros quienes elegimos el nivel de confianza que queremos tener y que \\(1 - \\text{Nivel de confianza} = \\alpha\\). Por lo que el nivel de confianza que escojamos también determina el nivel de error tipo I (\\(\\alpha\\)) que estamos dispuestos a aceptar.\nPodemos interpretarlo intuitivamente con nuestro ejemplo del juicio: establecer un umbral más estricto (mayor nivel de confianza) significa que seremos más exigentes con la evidencia antes de declarar culpable a alguien. Esto reduce la posibilidad de condenar a un inocente (error tipo I), pero también hace más probable que dejemos libres a algunos culpables (error tipo II). Por otro lado, si relajamos el criterio y aceptamos condenar con menos evidencia, disminuimos la probabilidad de absolver a culpables (error tipo II), pero corremos el riesgo de castigar a más personas inocentes (error tipo I).\nVayamos ahora con otro ejemplo. Imagina que en un país se está evaluando la viabilidad de aprobar un referéndum. La ley establece que para que una propuesta sea aprobada, al menos el 60% de los votantes deben estar a favor. En este contexto, se realiza una encuesta para estimar el nivel de apoyo y determinar si hay suficiente respaldo antes de llevar el referéndum a votación. Aquí planteamos la prueba de hipótesis:\n\nHipótesis nula (\\(H_0\\)): El apoyo a la propuesta es del 60% o menos (\\(p \\leq 0.60\\)).\n\nHipótesis alternativa (\\(H_1\\)): El apoyo a la propuesta es mayor al 60% (\\(p &gt; 0.60\\)).\n\nDado que queremos demostrar que el apoyo supera el 60%, realizamos una prueba a la derecha. Si los resultados de la encuesta proporcionan suficiente evidencia, rechazamos \\(H_0\\) y concluimos que la propuesta tiene el respaldo necesario para ser aprobada en el referéndum.\nEste caso sigue la misma lógica que el juicio: debemos decidir entre rechazar o no la hipótesis nula, sabiendo que cualquiera de las dos decisiones puede llevarnos a cometer un error.\n\nError Tipo I (\\(\\alpha\\)): Concluir que la propuesta tiene más del 60% de apoyo cuando en realidad no lo tiene (un falso positivo).\n\nError Tipo II (\\(\\beta\\)): No detectar que la propuesta tiene suficiente apoyo cuando en realidad sí lo tiene (un falso negativo).\n\nRecuerda que al establecer un nivel de confianza, también definimos \\(\\alpha\\) como \\(1 - \\text{Nivel de confianza}\\). En el contraste de hipótesis, \\(\\alpha\\) representa el área de rechazo, que puede ubicarse en un solo extremo si la prueba es unilateral (a la derecha o a la izquierda) o en ambos extremos si la prueba es bilateral (simétrica).\nEs importante entender que \\(\\alpha\\) no solo define el área de rechazo, sino que también representa el nivel de error tipo I que estamos dispuestos a aceptar. Es decir, la probabilidad de rechazar incorrectamente la hipótesis nula cuando en realidad es verdadera. Cuanto menor sea \\(\\alpha\\), mayor será el nivel de confianza, lo que implica ser más cautelosos antes de rechazar \\(H_0\\).\nIncluso si la hipótesis nula fuese cierta, la variación aleatoria haría que los valores observados fluctúen alrededor de \\(H_0\\). Es decir, no todos los valores serán exactamente del 60%, pero la mayoría se acercará a este valor, ya sea ligeramente por debajo o por encima, formando la distribución nula que conocemos, aproximadamente normal.\nEn nuestro caso, al realizar una prueba a la derecha, el área de rechazo se encuentra en la cola derecha de la distribución. Esto significa que, si nuestra proporción estimada cae dentro de esta región, rechazamos \\(H_0\\) y asumimos que el apoyo supera el umbral del 60%. Sin embargo, esta decisión podría ser errónea si en realidad \\(H_0\\) es verdadera, y en ese caso estaríamos cometiendo un error tipo I.\nPor tanto, cuanto menor sea \\(\\alpha\\), menor será la zona de rechazo, lo que nos hace más cautelosos antes de rechazar \\(H_0\\). Sin embargo, esta mayor precaución tiene un costo: aumenta la probabilidad de cometer un error tipo II (\\(\\beta\\)), es decir, no detectar que el apoyo realmente supera el 60% cuando en realidad lo hace.\n\n\n\n\n\n\n\n\n\nPero esto también significa que seremos más exigentes con la evidencia, lo que aumenta la posibilidad de no detectar un cambio real (es decir, aumenta la probabilidad de cometer un error tipo II). Si en lugar de asumir que \\(H_0\\) es cierta, definiéramos la distribución bajo \\(H_1\\), veríamos que hay una nueva curva desplazada a la derecha, correspondiente a la verdadera proporción en caso de que el apoyo realmente supere el 60%.\nSupongamos que, en realidad, la hipótesis alternativa \\(H_1\\) es verdadera, es decir, que el verdadero apoyo a la propuesta sí supera el 60%. Si esto es cierto, entonces la proporción muestral que observamos no debería seguir la distribución bajo \\(H_0\\), sino una nueva distribución desplazada hacia la derecha, que representa los verdaderos valores posibles del apoyo. Esta nueva distribución es la que correspondería a \\(H_1\\), y nos muestra qué valores esperaríamos si efectivamente el apoyo ha aumentado. En este caso, la media de la muestra ya no estaría centrada en el 60%, sino en un valor mayor.\n\n\n\n\n\n\n\n\n\nObserva la línea roja en el gráfico. No olvidemos que esta línea marca el valor crítico, que define la región de rechazo de \\(H_0\\). En la distribución azul (\\(H_0\\)), cualquier valor a la derecha de esta línea nos llevaba a rechazar la hipótesis nula, y la zona roja que ves representa \\(\\alpha\\) (error tipo I), es decir, los casos en los que rechazamos incorrectamente \\(H_0\\) cuando en realidad era verdadera.\nSin embargo, cuando consideramos la distribución verde (\\(H_1\\)), lo que está a la izquierda de esta línea se vuelve importante. ¿Por qué? Porque ahora sabemos que \\(H_1\\) es la verdadera distribución, y todos los valores que caen a la izquierda del valor crítico nos llevan a no rechazar \\(H_0\\) cuando en realidad deberíamos hacerlo. Esta zona sombreada en verde en la distribución de \\(H_1\\) es el error tipo II (\\(\\beta\\)), que representa la probabilidad de no detectar el verdadero aumento en el apoyo cuando en realidad existe.\nEs aquí donde vemos la relación inversa entre \\(\\alpha\\) y \\(\\beta\\):\n\nSi movemos la línea de decisión hacia la derecha (disminuyendo \\(\\alpha\\)), la zona roja de error tipo I se reduce, pero la zona verde de error tipo II crece, lo que significa que seremos más propensos a no detectar un aumento real en el apoyo.\n\nSi movemos la línea hacia la izquierda (aumentando \\(\\alpha\\)), seremos más propensos a detectar un cambio cuando lo hay, pero corremos más riesgo de cometer errores tipo I.\n\nAhora, el poder de la prueba se define como \\(1 - \\beta\\), es decir, la probabilidad de detectar correctamente un cambio real cuando este realmente existe. En nuestro caso, representa la capacidad de la prueba estadística para identificar que el apoyo supera el 60% cuando esto es cierto.\nSi observamos el gráfico, la curva azul representa la distribución bajo \\(H_0\\), donde asumimos que la proporción de apoyo no supera el 60%. La curva verde, en cambio, representa la distribución bajo \\(H_1\\), la cual refleja la verdadera proporción de apoyo si en realidad es mayor al 60%.\nPor tanto, el poder de la prueba (\\(1 - \\beta\\)) es la fracción de la distribución de \\(H_1\\) que cae dentro de la región de rechazo de \\(H_0\\), es decir, la parte de la curva verde que no está sombreada en verde oscuro.\n\n\n\n\n\n\n\n\n\nParámetros en los tipos de error\nLa razón por la cual la desviación estándar, el tamaño de la muestra y el valor observado son tan importantes en un contraste de hipótesis es porque determinan nuestra capacidad para detectar diferencias reales y afectan la probabilidad de cometer errores tipo I y II. Estos factores influyen directamente en la forma en que las distribuciones de \\(H_0\\) y \\(H_1\\) se comportan.\nRecuerda que, para inferir sobre una media, la desviación estándar (\\(s\\)) representa la variabilidad de los datos en la muestra y está directamente relacionada con el error estándar (\\(SE\\)), dado por la fórmula \\(SE = \\frac{s}{\\sqrt{n}}\\). Si \\(\\sigma\\) es grande, el error estándar también será mayor, lo que implica que las medias muestrales estarán más dispersas, aumentando la superposición entre ambas distribuciones y, en consecuencia, incrementando la probabilidad de cometer un error tipo II (\\(\\beta\\)). Por el contrario, si \\(s\\) es pequeña, el error estándar se reduce, las medias muestrales estarán más concentradas, lo que facilita la detección de diferencias estadísticamente significativas y aumenta el poder de la prueba (\\(1 - \\beta\\)).\n\n\n\n\n\n\n\n\n\nDe la misma forma, el tamaño de la muestra (\\(n\\)) afecta la precisión de nuestras estimaciones, ya que está inversamente relacionado con el error estándar (\\(SE\\)) a través de la misma fórmula \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\). Cuando \\(n\\) es grande, el error estándar disminuye, lo que significa que las medias muestrales estarán menos dispersas, reduciendo la superposición entre las distribuciones de \\(H_0\\) y \\(H_1\\), lo que facilita la detección de diferencias significativas y disminuye la probabilidad de cometer un error tipo II (\\(\\beta\\)), aumentando así el poder de la prueba (\\(1 - \\beta\\)). En cambio, si el tamaño de la muestra es pequeño, el error estándar será mayor, lo que provoca que las distribuciones sean más anchas y se solapen más, dificultando la diferenciación entre \\(H_0\\) y \\(H_1\\), aumentando la incertidumbre y reduciendo la capacidad de detectar efectos reales cuando estos existen.\n\n\n\n\n\n\n\n\n\nEl valor observado, que determina la posición de la media bajo la hipótesis alternativa (\\(H_1\\)) con respecto a la hipótesis nula (\\(H_0\\)) también influye en la probabilidad de cometer errores y en la capacidad de detectar un efecto real. Cuando la diferencia entre \\(H_0\\) y \\(H_1\\) es pequeña, las distribuciones se superponen significativamente, lo que dificulta distinguir entre ambas y aumenta la probabilidad de cometer un error tipo II (\\(\\beta\\)), es decir, no rechazar \\(H_0\\) cuando en realidad \\(H_1\\) es verdadera. En este caso, el poder de la prueba (\\(1 - \\beta\\)) es menor, lo que significa que la prueba tiene menos capacidad para detectar cambios reales. En contraste, cuando la media de \\(H_1\\) está más alejada de \\(H_0\\), la superposición entre las distribuciones disminuye, facilitando la detección de diferencias y reduciendo la probabilidad de error tipo II, aumentando así el poder de la prueba.\n\n\n\n\n\n\n\n\n\nPuedes interactuar con los tipos de error en la Datáfora Interactiva: [Click Aquí]\nO usa el QR:\n\n\n\nDirígete a la pestaña Correlación\n\n\n\n\n\nEntender la relación entre nivel de confianza y los tipos de error es la clave para una interpretación adecuada",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estadística inferencial</span>"
    ]
  },
  {
    "objectID": "inf.html#supuestos-en-la-inferencia",
    "href": "inf.html#supuestos-en-la-inferencia",
    "title": "6  Estadística inferencial",
    "section": "6.3 Supuestos en la inferencia",
    "text": "6.3 Supuestos en la inferencia\nTodo esto suena muy bien, pero cuidado: Hacer inferencia estadística implica asumir ciertos supuestos sobre los datos y la forma en que fueron recolectados. Estos supuestos no son simples formalidades, sino condiciones necesarias para que los métodos sean válidos y los resultados confiables. Si alguno de estos supuestos no se cumple, las conclusiones pueden ser erróneas o, peor aún, engañosas.\nMuestreo aleatorio e independencia de las observaciones\nCada individuo de la población tiene la misma probabilidad de ser seleccionado en la muestra y que las observaciones son independientes unas de otras, es decir, el valor de una observación no influye en el valor de las demás. La independencia es crucial porque garantiza que los resultados de la muestra sean representativos de la población. Si este supuesto se viola, los resultados pueden estar sesgados, y las inferencias serán poco confiables. Es importante revisar cuidadosamente cómo se recolectaron los datos y verificar que no existan patrones obvios de dependencia entre las observaciones.\nTamaño suficiente de la muestra\nRecordarás que una muestra pequeña puede introducir una alta variabilidad en las estimaciones, lo que dificulta obtener resultados confiables. El Teorema del Límite Central (TLC), que permite que la distribución muestral sea aproximadamente normal, pero esto es solo es válido cuando el tamaño de la muestra es suficientemente grande. En general, se recomienda que (\\(n &gt; 30\\)) para confiabilidad en análisis basados en el TLC. Para proporciones, cada categoría debe tener al menos cinco observaciones para asegurar una buena aproximación. Si el tamaño de la muestra es insuficiente, los resultados pueden ser demasiado inestables.\nNormalidad de la Distribución\nInferir sobre la media puede ser complicado en ciertos casos. Si recuerdas el capítulo de estadística descriptiva, la media no siempre es un valor robusto, especialmente cuando la distribución de los datos es asimétrica o presenta valores extremos. En distribuciones con colas pesadas o sesgo pronunciado, la media puede estar fuertemente influenciada por valores atípicos, lo que afecta la validez de los resultados inferenciales. Si bien el Teorema del Límite Central permite aproximar la distribución de los estadísticos a una normal bajo ciertas condiciones, asumir su aplicación sin verificar la naturaleza de los datos puede llevar a estimaciones sesgadas o poco representativas.\nSin embargo, en la práctica, muchas variables no siguen una distribución normal exacta, lo que puede afectar la validez de los resultados si se aplican pruebas paramétricas sin verificar este supuesto. Para evaluar si los datos cumplen con la normalidad, utilizamos pruebas estadísticas como la prueba de Shapiro-Wilk, que nos permite detectar desviaciones significativas respecto a la distribución normal. Esta prueba se basa en la comparación de los datos observados con los valores esperados bajo normalidad y plantea las siguientes hipótesis:\n\nHipótesis nula (\\(H_0\\)): Los datos siguen una distribución normal.\n\nHipótesis alternativa (\\(H_1\\)): Los datos no siguen una distribución normal.\n\nSi el p-valor resultante de la prueba es menor que 0.05, se rechaza la hipótesis nula, indicando que los datos no presentan una distribución normal. En estos casos, puede ser preferible emplear métodos no paramétricos que no dependan de la normalidad, como pruebas basadas en la mediana o en los cuantiles, lo que permite realizar inferencias más robustas y menos sensibles a distribuciones atípicas.\nPor ejemplo para los datos que recolectó de las alturas:\n\nshapiro.test(muestra$alturas)\n\n\n    Shapiro-Wilk normality test\n\ndata:  muestra$alturas\nW = 0.99242, p-value = 0.07237\n\n\nEl p-valor es 0.07, por no se rechaza la hipótesis nula. Esto significa que no hay suficiente evidencia para concluir que los datos no son normales. Es decir, la prueba no detecta una desviación significativa de la normalidad, por lo que se puede asumir que la variable alturas sigue una distribución normal.\nAhora, en caso la prueba de Shapiro-Wilk indica que los datos no siguen una distribución normal, una alternativa es utilizar una prueba no parametrica. La pruebas no paramétricas no asumen normalidad en la distribución de tu muestra y cada prueba tiene una versión no paramétrica (7.6). En el caso de una prueba para una media, la prueba de los rangos con signo de Wilcoxon en lugar de la prueba t.\nLo que evalúa es lo mismo:\n\nHipótesis nula (\\(H_0\\)): La mediana de la población es igual a un valor específico (\\(mu_0\\)).\n\nHipótesis alternativa (\\(H_1\\))**: La mediana de la población es diferente de ese valor.\n\n\nwilcox.test(muestra$alturas,\n            mu = 170, \n                  conf.level = 0.95,\n                  alternative = \"two.sided\")\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  muestra$alturas\nV = 30851, p-value = 0.9419\nalternative hypothesis: true location is not equal to 170\n\n\nEl p-valor indica 0.9419, por lo que es mucho mayor que 0.05 y por tanto no se rechaza la hipótesis nula. Esto significa que no hay suficiente evidencia estadística para concluir que la mediana de las alturas es significativamente diferente de 170 cm.\nA diferencia de la prueba t, que evalúa diferencias en la media, la prueba de Wilcoxon analiza diferencias en la mediana, lo que la hace robusta a la presencia de valores atípicos o distribuciones sesgadas (recuerda las propiedad de la mediana el el capítulo 4)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estadística inferencial</span>"
    ]
  },
  {
    "objectID": "inf.html#resumen-del-capítulo",
    "href": "inf.html#resumen-del-capítulo",
    "title": "6  Estadística inferencial",
    "section": "6.4 Resumen del capítulo",
    "text": "6.4 Resumen del capítulo\nLa estadística inferencial permite hacer estimaciones y contrastes sobre una población a partir de muestras. Su fundamento es el Teorema del Límite Central, que establece que, al tomar múltiples muestras de una población, la distribución de sus estadísticos se aproximará a una normal conforme el tamaño de la muestra sea suficientemente grande.\nLa estimación es un método inferencial que busca aproximar parámetros desconocidos, como la media o la proporción poblacional, utilizando los estadísticos de la muestra. La incertidumbre en la estimación se mide a través del intervalo de confianza, que se construye considerando el error estándar y un valor crítico, determinado por el nivel de confianza elegido. A medida que el tamaño de la muestra aumenta, el error estándar disminuye, lo que genera estimaciones más precisas.\nEl contraste de hipótesis permite evaluar afirmaciones sobre una población al comparar una hipótesis nula con una hipótesis alternativa. Para ello, se establece un umbral de significancia (\\(\\alpha\\)) que define las regiones de rechazo de \\(H_0\\). Dependiendo de la dirección del análisis, las pruebas pueden ser bilaterales, cuando buscan detectar diferencias en cualquier sentido, o unilaterales, cuando solo se considera un extremo de la distribución. El resultado de la prueba se evalúa mediante valores críticos y p-valores, que indican la probabilidad de obtener un resultado igual o más extremo bajo la suposición de que \\(H_0\\) es cierta.\nEl error tipo I se comete cuando se rechaza una hipótesis nula verdadera, mientras que el error tipo II ocurre cuando no se rechaza una hipótesis nula falsa. Existe una relación inversa entre ambos: reducir la probabilidad de cometer un error tipo I aumenta la probabilidad de cometer un error tipo II y viceversa. La capacidad de detectar un efecto real cuando este existe se conoce como poder de la prueba y depende del tamaño de la muestra, la variabilidad de los datos y la distancia entre \\(H_0\\) y \\(H_1\\). A medida que la muestra es mayor o la diferencia entre la media nula y la alternativa es más grande, el poder de la prueba aumenta y la probabilidad de error tipo II disminuye.\nLa validez de estos métodos depende del cumplimiento de ciertos supuestos, como la aleatoriedad en el muestreo y la independencia de las observaciones. Además, la normalidad de los datos es esencial en pruebas paramétricas, por lo que se utiliza la prueba de Shapiro-Wilk para verificar este supuesto. Si los datos no siguen una distribución normal, se pueden emplear métodos no paramétricos, como la prueba de los rangos con signo de Wilcoxon, que no asumen una distribución específica.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estadística inferencial</span>"
    ]
  },
  {
    "objectID": "inf.html#ejercicios",
    "href": "inf.html#ejercicios",
    "title": "6  Estadística inferencial",
    "section": "6.5 Ejercicios",
    "text": "6.5 Ejercicios\n1: El Teorema del Límite Central (TLC) es un principio fundamental en la estadística inferencial. Hemos visto que tiene una amplia aplicabilidad en el análisis de muestras y la estimación de parámetros poblacionales. ¿Cuál es la principal utilidad del Teorema del Límite Central?\n\nPermite conocer la distribución de los datos originales.\nGarantiza que cualquier muestra siga una distribución normal.\nExplica que la distribución de ciertas estadísticas muestrales tiende a la normalidad conforme aumenta el tamaño de la muestra.\nSe usa exclusivamente para contrastes de hipótesis.\n\n2: Para estimar la media poblacional con un intervalo de confianza a partir de una única muestra, recordemos que un intervalo se construye tomando la estimación puntual más un margen de error. En este caso, el error estándar depende de la variabilidad dentro de la muestra y el tamaño muestral, y el valor crítico ¿Cuál de las siguientes expresiones es la correcta?\n\n\\(\\hat{p} \\pm t \\times \\sqrt{\\frac{\\hat{p} (1 - \\hat{p})}{n}}\\)\n\\(\\bar{X} \\pm t \\times \\frac{s}{\\sqrt{n}}\\)\n\\(\\sqrt{\\frac{s^2}{n}}\\)\n\\(\\bar{X} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}\\)\n\n3: Si queremos estimar una proporción poblacional con un intervalo de confianza, la fórmula del error estándar es:\nA) \\(\\frac{s}{\\sqrt{n}}\\)\nB) \\(\\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\\)\nC) \\(\\frac{\\sigma}{\\sqrt{n}}\\)\nD) \\(\\hat{p} \\pm z \\times \\text{EE}\\)\n4: Cuando construimos un intervalo de confianza, el nivel de confianza refleja la certeza con la que estimamos el parámetro poblacional. Si aumentamos el nivel de confianza de un intervalo (por ejemplo, de 95% a 99%), ¿qué efecto tendrá sobre el intervalo de confianza?\n\nMás estrecho.\nMás amplio.\nNo cambiará.\nDependerá de la desviación estándar.\n\n5: ¿Cuál de las siguientes afirmaciones es correcta sobre el valor crítico \\(t\\) en comparación con el valor crítico \\(z\\)?\nA) El valor \\(t\\) es mayor que \\(z\\) cuando la muestra es pequeña.\nB) El valor \\(t\\) siempre es menor que \\(z\\).\nC) Los valores \\(t\\) y \\(z\\) son idénticos si \\(n &lt; 30\\).\nD) El valor \\(t\\) solo se usa para pruebas de proporciones.\n6: En una prueba de hipótesis bilateral con un nivel de significancia del 5%, el área total de las colas de la distribución es 0.05, lo que implica que cada cola contiene un 2.5% de la probabilidad. Si la muestra es pequeña y se usa la distribución \\(t\\) con un determinado número de grados de libertad, ¿cuál sería el valor crítico \\(t\\) aproximado?\n\n1.645\n1.96\n2.576\nDepende de los grados de libertad\n\n7: En el contexto de un contraste de hipótesis, ¿qué representa el p-valor?\n\nLa probabilidad de que \\(H_0\\) sea verdadera, dado el resultado observado.\nLa media de la distribución muestral bajo la hipótesis nula.\nEl valor crítico de la prueba de hipótesis, que determina el umbral de rechazo.\nLa probabilidad de obtener un resultado tan extremo como el observado, dado que \\(H_0\\) sea cierta.\n\n8: Si el p-valor obtenido en una prueba de hipótesis bilateral es 0.07 y usamos un nivel de significancia de 0.05, la decisión correcta sería:\nA) Rechazar \\(H_0\\).\nB) No rechazar \\(H_0\\).\nC) El p-valor se acerca a 0.05 por lo que Rechazar \\(H_0\\). D) No se puede tomar una decisión sin conocer la desviación estándar.\n9: Si cometemos un Error Tipo I, significa que:\n\nRechazamos \\(H_0\\) cuando en realidad \\(H_0\\) es verdadera.\nNo rechazamos \\(H_0\\) cuando en realidad \\(H_0\\) es falsa.\nEl p-valor es mayor que \\(\\alpha\\).\nNuestro intervalo de confianza es demasiado estrecho.\n\n10: Si aumentamos el tamaño de la muestra en un contraste de hipótesis, ¿qué sucede con la probabilidad de cometer un Error Tipo II (\\(\\beta\\))?\nA) Aumenta.\nB) Disminuye.\nC) Se mantiene igual.\nD) Depende del nivel de confianza.\n11: Flor está evaluando si un programa de reciclaje en una ciudad ha tenido un impacto en la cantidad promedio de basura reciclada por hogar. El programa fue implementado hace 6 meses y está interesada en comparar la cantidad promedio de basura reciclada antes y después de su implementación. Si el valor \\(p\\) obtenido en la prueba es \\(0.04\\) y el nivel de significancia es \\(\\alpha = 0.05\\), ¿cuál es la interpretación correcta?\n\nNo hay evidencia suficiente para rechazar la hipótesis nula, lo que sugiere que el programa no ha tenido impacto.\nSe rechaza la hipótesis nula, lo que sugiere que el programa ha tenido un impacto significativo en la cantidad de basura reciclada.\nEl valor \\(p\\) indica que la diferencia es irrelevante y no afecta el resultado de la prueba.\nNo se puede concluir nada porque no se conoce el tamaño de la muestra.\n\n12: En un estudio sobre el consumo de energía en hogares de una ciudad, un investigador quiere saber si la proporción de hogares que utilizan energía renovable es diferente de un 30%. Después de aplicar una prueba de hipótesis sobre una única proporción, obtiene un valor \\(p\\) de \\(0.03\\) y el nivel de significancia es \\(\\alpha = 0.05\\). ¿Cuál es la conclusión correcta?\n\nNo hay evidencia suficiente para rechazar la hipótesis nula, por lo que la proporción de hogares con energía renovable es igual al 30%.\nSe rechaza la hipótesis nula, lo que indica que la proporción de hogares con energía renovable es significativamente diferente de 30%.\nEl valor \\(p\\) no es suficientemente pequeño como para rechazar la hipótesis nula.\nLa hipótesis nula no puede ser rechazada sin realizar más pruebas estadísticas.\n\n13: Un economista está evaluando el impacto de un cambio en la política fiscal sobre el ingreso promedio de los hogares en un país. El economista realiza una prueba de hipótesis sobre una media para determinar si el ingreso promedio ha cambiado significativamente después de la reforma fiscal. Si el valor \\(p\\) obtenido es \\(0.06\\) y el nivel de significancia es \\(\\alpha = 0.05\\), ¿cuál es la conclusión correcta?\n\nSe rechaza la hipótesis nula, indicando que el cambio en la política fiscal ha tenido un impacto significativo.\nEl valor \\(p\\) indica que el impacto es extremadamente relevante y debe considerarse.\nSe concluye que el cambio en la política fiscal no ha tenido impacto en los ingresos promedio.\nNo se rechaza la hipótesis nula, ya que el valor \\(p\\) es mayor que el nivel de significancia.\n\n14: Un politólogo está estudiando si el porcentaje de votantes en favor de un candidato presidencial es mayor al 40%. El investigador realiza una prueba de hipótesis sobre una proporción. ¿Cuál es el planteamiento adecuado para las hipótesis nula y alternativa? El nivel de significancia es \\(\\alpha = 0.05\\).\n\n\\(H_0\\): La proporción de votantes a favor del candidato es menor o igual al 40%.   \\(H_a\\): La proporción de votantes a favor del candidato es mayor al 40%.\n\\(H_0\\): La proporción de votantes a favor del candidato es exactamente el 40%.   \\(H_a\\): La proporción de votantes a favor del candidato es mayor al 40%.\n\\(H_0\\): La proporción de votantes a favor del candidato es igual al 40%.   \\(H_a\\): La proporción de votantes a favor del candidato es diferente al 40%.\n\\(H_0\\): La proporción de votantes a favor del candidato es mayor al 40%.   \\(H_a\\): La proporción de votantes a favor del candidato es menor o igual al 40%.\n\n15: Un sociólogo está investigando si la media de horas dedicadas al trabajo voluntario por mes ha cambiado significativamente después de la implementación de una nueva ley en una comunidad. Para ello, realiza una prueba \\(t\\) sobre una única media con los datos obtenidos antes y después de la implementación de la ley. Si el valor \\(p\\) obtenido es \\(0.04\\) y el nivel de significancia es \\(\\alpha = 0.05\\), ¿cuál es la interpretación correcta?\n\nNo se puede rechazar la hipótesis nula, ya que el valor \\(p\\) es mayor que el nivel de significancia.\nSe rechaza la hipótesis nula, lo que sugiere que el cambio en la ley ha tenido un impacto significativo en las horas dedicadas al trabajo voluntario.\nEl valor \\(p\\) indica que el resultado no es estadísticamente significativo, por lo que no hay evidencia suficiente para rechazar la hipótesis nula.\nLa hipótesis nula no puede ser rechazada sin realizar más pruebas estadísticas.\n\n\n\n\n\nCasella, George, y Roger W. Berger. 2024. Statistical Inference. CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estadística inferencial</span>"
    ]
  },
  {
    "objectID": "comp.html",
    "href": "comp.html",
    "title": "7  Comparando grupos",
    "section": "",
    "text": "7.1 Comparación entre dos medias\nFlor está investigando si existen diferencias en el tiempo de ocio semanal entre hombres y mujeres. Para ello, ha recolectado información sobre la cantidad de horas que cada persona dedica al ocio en una semana y desea comparar las medias de ambos grupos.\nocio = read_csv('horas_ocio.csv')\nglimpse(ocio)\n\nRows: 200\nColumns: 2\n$ genero     &lt;chr&gt; \"hombre\", \"mujer\", \"hombre\", \"hombre\", \"mujer\", \"mujer\", \"m…\n$ horas_ocio &lt;dbl&gt; 15, 9, 13, 12, 13, 11, 13, 15, 14, 11, 14, 16, 10, 13, 13, …\nPodemos utilizar describeBy del paquete psych para realizar los resúmenes numéricos pertinentes por cada nivel (categoría) de la variable genero.\nlibrary(psych)\ndescribeBy(ocio$horas_ocio, ocio$genero)\n\n\n Descriptive statistics by group \ngroup: hombre\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 100 15.73 2.47     15   15.59 2.97  11  24    13 0.58     0.35 0.25\n------------------------------------------------------------ \ngroup: mujer\n   vars   n  mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 100 12.13 2.31     12   12.11 1.48   6  17    11    0    -0.39 0.23\nEsto nos permite comparar cómo varían las horas de ocio entre ambos grupos, pero una manera aún más intuitiva de explorar estas diferencias es mediante una representación gráfica. Para ello, utilizamos un boxplot, que nos muestra la distribución del tiempo de ocio según el género.\nocio %&gt;% \n  ggplot(aes(x = genero, y = horas_ocio, fill = genero)) +\n  geom_boxplot() +\n  labs(title = 'Horas de ocio semanal por género', \n       y = 'Cantidad de horas') +\n  theme_minimal()\nEl gráfico muestra una diferencia en la distribución entre los grupos. La clave aquí es que tan diferentes son como para poder afirmar que efectivamente el género influye o explica la variación en las horas de ocio.\nPodemos generar un resumen descriptivo con la media y la desviación estándar utilizando dplyr\nestadisticas = ocio %&gt;%\n  group_by(genero) %&gt;%\n  summarise(\n    promedio = mean(horas_ocio),\n    sd = sd(horas_ocio),\n    n = n()\n  )\n\nestadisticas\n\n# A tibble: 2 × 4\n  genero promedio    sd     n\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 hombre     15.7  2.47   100\n2 mujer      12.1  2.31   100\nPara cuantificar esta diferencia, necesitamos calcular un estadístico de comparación, que en este caso es la diferencia de medias muestrales, representada por \\(\\Delta\\) (Delta).\ndiferencia = diff(estadisticas$promedio)\ndiferencia\n\n[1] -3.6\nPor lo que la diferencia de medias \\(\\Delta \\bar{x}\\) es: \\[\n\\Delta \\bar{x} = \\bar{x}_{Hombres} - \\bar{x}_{Mujeres} = 15.8 - 12.2 = 3.6\n\\] La distribución de ambos grupos es la siguiente",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comparando grupos</span>"
    ]
  },
  {
    "objectID": "comp.html#comparación-entre-dos-medias",
    "href": "comp.html#comparación-entre-dos-medias",
    "title": "7  Comparando grupos",
    "section": "",
    "text": "7.1.1 Estimación para la diferencia de medias\nCálculo del intervalo de confianza\nDe la misma forma que para construir un intervalo de confianza con una media. Para realizar un intervalo del 95% para la diferencia de medias, utilizamos la misma fórmula:\n\\[\n\\text{IC}_{\\Delta \\bar{x}} = \\Delta \\bar{x} \\pm z/t \\cdot \\text{EE}\n\\] Dado que estamos comparando dos grupos distintos (hombres y mujeres), cada uno con su propia media, desviación estándar y tamaño muestral, la incertidumbre en la diferencia de medias proviene de ambas muestras. Por lo tanto, el error estándar (EE) de la diferencia de medias se obtiene combinando los errores estándar individuales de cada grupo. La formula para el EE responde a la integración de ambas variables , donde:\nPara dos estimaciones provenientes de muestras independientes (una no está relacionada con la otra, más sobre este caso en “Comparaciones de grupos relacionados”) con errores estándar estimados \\(se_1\\) y \\(se_2\\), la distribución de muestreo de su diferencia tiene un error estándar estimado dado por:\n\\[\n\\text{Error estándar} = \\sqrt{(se_1)^2 + (se_2)^2}.\n\\]\nCada estimación tiene un error de muestreo, y las variabilidades se suman para determinar el error estándar de la diferencia entre las estimaciones.\nRecuerda que el error estándar estimado de una media muestral está dado por:\n\\[\nse = \\frac{s}{\\sqrt{n}},\n\\]\ndonde \\(s\\) es la desviación estándar muestral y \\(n\\) es el tamaño de la muestra.\nPor lo que si \\(n_1\\) y \\(n_2\\) son los tamaños de las muestras para las primera y segunda muestra, respectivamente. Y \\(s_1\\) y \\(s_2\\) las desviaciones estándar muestrales, que son estimaciones de las desviaciones estándar poblacionales correspondientes \\(\\sigma_1\\) y \\(\\sigma_2\\). La diferencia entre las medias muestrales \\(\\bar{x}_2 - \\bar{x}_1\\), provenientes de muestras independientes, tiene un error estándar estimado dado por:\n\\[\nEE = \\sqrt{(se_1)^2 + (se_2)^2} = \\sqrt{\\left(\\frac{s_1}{\\sqrt{n_1}}\\right)^2 + \\left(\\frac{s_2}{\\sqrt{n_2}}\\right)^2} = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}.\n\\] Por tanto la formula \\(\\text{EE} = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) define el error estándar, el cual nos indica cuánto puede variar la diferencia de medias observada debido a la variabilidad muestral.\nLa podemos calcular en base al objeto estadistica que creamos\n\nestadisticas\n\n# A tibble: 2 × 4\n  genero promedio    sd     n\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 hombre     15.7  2.47   100\n2 mujer      12.1  2.31   100\n\n\n\nee = sqrt((estadisticas$sd[1]^2 / estadisticas$n[1]) +\n          (estadisticas$sd[2]^2 / estadisticas$n[2]))\n\nee\n\n[1] 0.3377017\n\n\nSiendo \\(EE = 0.337\\)\nAhora definamos nuestro valor \\(t\\), el valor crítico para un nivel de confianza del 95% (\\(z = 1.96\\)). Si asumimos homogeneidad de varianzas(es decir que la variación en ambos grupos de homogénea), los grados de libertad se calculan como \\(gl = n_1 + n_2 - 2\\), ya que combinamos la información de ambas muestras para estimar una única varianza poblacional. Recuerda que estamos comparando medias porlo que al no conocer la desviación estándar poblacional, utilizamos la distribución t de Student en lugar de la normal estándar, ya que esta ajusta la mayor incertidumbre en muestras finitas.\n\n# Cálculo de los grados de libertad con varianzas iguales\ngl = estadisticas$n[1] + estadisticas$n[2] - 2  \n\nt = qt(0.975, df = gl)  \n\nt\n\n[1] 1.972017\n\n\nSiendo \\(t = 1.972\\)\nY la diferencia observada en nuestra muestra\n\ndiferencia = diff(estadisticas$promedio) \n\ndiferencia\n\n[1] -3.6\n\n\nSiendo \\(\\Delta\\ \\bar{x} = -3.6\\)\nEl intervalo de confianza se define por\n\\[\n\\text{IC}_{\\Delta \\bar{x}} = \\Delta \\bar{x} \\pm t \\cdot \\text{EE}\n\\]\n\nic = c(diferencia - t * ee, diferencia + t *ee)\n\nic\n\n[1] -4.265954 -2.934046\n\n\n\\[\n\\text{IC}_{\\Delta \\bar{x}} = -3.6 \\pm 0.5 = [-4.26, -2.93]\n\\]\nRepresentado graficamente tenemos lo siguiente:\n\n\n\n\n\n\n\n\n\nLas pruebas t tambien puede utilizarse para compara dos medias, con una ligera modificacion en el codigo, debemos especificar nuestra variable respuesta y explicativa utilizando ~, se puede leer como la variable ocio esta explicada (~) a travez de la variable genero.\n\nprueba_t = t.test(horas_ocio ~ genero, \n                  data = ocio)\n\nprueba_t$conf.int\n\n[1] 2.934029 4.265971\nattr(,\"conf.level\")\n[1] 0.95\n\n\nEl intervalo de confianza [2.93, 4.27] con un 95% de confianza nos dice que, si repitiésemos este estudio muchas veces, en el 95% de los casos el intervalo calculado contendría la diferencia en población de medias en la población. Un punto clave es que el intervalo no incluye el 0, lo que implica que una diferencia de cero (sean iguales ambos grupos) no es una posibilidad dentro del rango estimado. Esto es relevante porque, si el intervalo incluyera el 0, significaría que la diferencia observada en la muestra podría explicarse por el azar, ya que el valor de cero indicaría que no hay una diferencia real entre los grupos en la población. Sin embargo, dado que el intervalo se encuentra completamente por encima de 0, esto sugiere que la diferencia observada no es producto del azar, y es altamente probable que los hombres de la población de estudio realmente dediquen entre 2.93 y 4.27 horas más al ocio que las mujeres.\n\n\n7.1.2 Contraste de hipótesis para la diferencia de medias\nCuando comparamos las medias de dos grupos, el contraste de hipótesis tiene como objetivo determinar si las medias de dos poblaciones son significativamente diferentes. Es decir, evaluamos si la diferencia observada entre las dos medias es lo suficientemente extrema como para concluir que no se debe al azar.\nPasos del Contraste de Hipótesis para la Diferencia de Medias\nEl contraste de hipótesis para la diferencia de medias entre dos grupos sigue un enfoque muy similar al contraste para una media única, pero aquí evaluamos si la diferencia observada entre las dos medias es suficientemente extrema como para concluir que no se debe al azar. A continuación, describimos los pasos intuitivos del contraste para las horas de ocio entre hombres y mujeres.\n1. Planteamiento de Hipótesis\nEl primer paso es formular nuestras hipótesis estadísticas:\n\nHipótesis nula (\\(H_0\\)): No existe diferencia entre las medias de las dos poblaciones. En otras palabras: \\[\nH_0: \\mu_{Hombres} - \\mu_{Mujeres} = 0\n\\]\nHipótesis alternativa (\\(H_1\\)): Existe una diferencia significativa entre las medias de las dos poblaciones: \\[\nH_1: \\mu_{Hombres} - \\mu_{Mujeres} \\neq 0\n\\]\n\nEl contraste de hipótesis nos permite evaluar si la diferencia observada en las horas de ocio promedio es lo suficientemente grande como para no atribuirse al azar. Bajo la hipótesis nula (\\(H_0\\)), asumimos que el género no influye en las horas de ocio, es decir, que la diferencia de medias en la población es cero (\\(\\mu\\_1 = \\mu\\_2\\)). Si rechazamos (\\(H_0\\)), concluimos que el género sí tiene un efecto significativo.\nEn nuestro caso, hemos planteado una prueba bilateral, donde la hipótesis alternativa (\\(H_A\\)) simplemente indica que existe una diferencia entre los grupos, sin especificar la dirección:\n\\[\nH_A: \\mu_1 \\neq \\mu_2\n\\]\nEsto significa que estamos interesados en detectar cualquier diferencia, ya sea que un grupo tenga más o menos horas de ocio que el otro. Sin embargo, también podríamos haber planteado una prueba unilateral, en la que la hipótesis alternativa especifique una dirección concreta de la diferencia. Esto se hace cuando hay una expectativa teórica o evidencia previa que sugiera que un grupo debería tener más o menos horas de ocio que el otro.\nPrueba unilateral derecha:\n\\[\nH_A: \\mu_1 &gt; \\mu_2\n\\]\nEn este caso, solo nos interesa saber si el grupo 1 dedica significativamente más horas al ocio que el grupo 2. Se rechaza \\(H_0\\) únicamente si la diferencia observada es suficientemente positiva.\nPrueba unilateral izquierda:\n\\[\nH_A: \\mu_1 &lt; \\mu_2\n\\]\nAquí, estamos evaluando si el grupo 1 dedica significativamente menos horas al ocio que el grupo 2. Se rechaza \\(H_0\\) solo si la diferencia observada es suficientemente negativa.\nLo recomendable es usar la prueba bilateral, especialmente en estudios exploratorios, donde no hay certeza sobre la dirección del efecto. Esto nos asegura que cualquier diferencia significativa, sea positiva o negativa, será detectada correctamente.\n2. Estadísticas Descriptivas\nPodemos establecer las característica de cada grupo\nTamaños de las Muestras\nPrimero, calculamos el número de observaciones por grupo:\n\nn_hombres = ocio %&gt;% filter(genero == \"hombre\") %&gt;% nrow()\nn_mujeres = ocio %&gt;% filter(genero == \"mujer\") %&gt;% nrow()\n\nn_hombres\n\n[1] 100\n\nn_mujeres  \n\n[1] 100\n\n\n\\[\nn_{Hombres} = 100 \\quad \\text{y} \\quad n_{Mujeres} = 100\n\\]\nMedias y Desviaciones Estándar\n\nocio %&gt;%\n  group_by(genero) %&gt;%\n  summarise(\n    promedio = mean(horas_ocio),\n    sd = sd(horas_ocio)\n  )\n\n# A tibble: 2 × 3\n  genero promedio    sd\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 hombre     15.7  2.47\n2 mujer      12.1  2.31\n\n\nPor lo que:\n\n\\(\\bar{x}_{Hombres} = 15.8\\), \\(s_{Hombres} = 2.47\\)\n\\(\\bar{x}_{Mujeres} = 12.2\\), \\(s_{Mujeres} = 2.31\\)\n\nLa diferencia observada entre las medias es:\n\\[\n\\Delta \\bar{x} = \\bar{x}_{Hombres} - \\bar{x}_{Mujeres} = 15.8 - 12.2 = 3.6\n\\]\n3. Nivel de Significancia\nDefinimos un nivel de significancia de:\n\\[\n\\alpha = 0.05\n\\]\nComo en el anterior capítulo. Esto significa que aceptaremos hasta un 5% de probabilidad de cometer un error al rechazar \\(H_0\\) si en realidad es verdadera.\n4. Realización de la Prueba\nPara evaluar si la diferencia observada (\\(\\Delta \\bar{x} = 3.6\\)) es significativa, calculamos el estadístico \\(t\\) y comparamos su valor con una distribución \\(t\\)\nCálculo del Error Estándar\nEl Error Estándar (\\(EE\\)) mide cuánto esperaríamos que varíe la diferencia de medias entre muestras si \\(H_0\\) fuera cierta. Ya lo habíamos calculado en la estimación:\n\nee\n\n[1] 0.3377017\n\n\n\\[\n\\text{EE} \\approx 0.338\n\\]\nCálculo del Estadístico t\nEl estadístico \\(t\\) nos permite cuantificar cuán lejos está la diferencia observada entre medias (\\(\\Delta \\bar{x}\\)) de lo que esperaríamos bajo la hipótesis nula (\\(H_0\\)), en términos de errores estándar (\\(EE\\)). Como bajo \\(H_0\\) asumimos que no hay diferencia real entre los grupos, el valor esperado es 0, por lo que el estadístico \\(t\\) se calcula como:\n\\[\nt = \\frac{\\Delta \\bar{x} - 0}{EE} = \\frac{\\Delta \\bar{x}}{EE}\n\\]\nEste valor nos indica cuántas veces el error estándar cabe en la diferencia observada.\n\\[\nt = \\frac{\\Delta \\bar{x} - 0}{\\text{EE}}\n\\]\nSustituyendo:\n\\[\nt = \\frac{3.6}{0.338} \\approx 10.66\n\\]\nEl valor \\(t = 10.66\\) indica que la diferencia observada (\\(3.6\\)) está a 10.66 \\(EE\\) de distancia de lo esperado bajo \\(H_0\\). Recuerda que este estadístico se compara con el valor crítico \\(t_{\\alpha/2}\\) de la distribución t con los grados de libertad cque calculamos, para determinar si se encuentra en la zona de rechazo. Aquí una representación gráfica, nota como el valor observado se encuentra en el area de rechazo.\n\n\n\n\n\n\n\n\n\nEl gráfico muestra la distribución nula y la región de rechazo para el contraste de hipótesis. La diferencia de medias observada \\(\\Delta \\bar{x} = -3.6\\) está claramente fuera del intervalo de confianza \\([-0.66, 0.66]\\), lo que indica que el resultado es altamente improbable bajo \\(H_0\\). Esto se alinea con el valor \\(t = 10.66\\), que nos dice que la diferencia observada está 10.66 errores estándar alejada de lo esperado bajo \\(H_0\\). Como el valor observado cae en la zona de rechazo, hay evidencia suficiente para rechazar \\(H_0\\) y concluir que la diferencia entre los grupos es estadísticamente significativa.\nFelizmente R puede hacer todo el cálculo por nosotros con la función t.test(), en este caso para la diferencia de muestras. Vamos a utilizar ~ para indicar la relación (se peude leer como horas_ocio es explicada por genero).\nNota: A partir de ahora usaremos uso del paquete broom y su función tidy(), el cual ordena los resultados de las pruebas estadísticas en tibble de forma ordenada.\n\nlibrary(broom)\n\n\nprueba_t = t.test(horas_ocio ~ genero, \n                  data = ocio, \n                  conf.level = 0.95)\n\ntidy(prueba_t)\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic  p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      3.6      15.7      12.1      10.7 3.04e-21      197.     2.93      4.27\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nLa prueba nos proporciona el p-valor, que representa la probabilidad de obtener una diferencia tan extrema como la observada si la hipótesis nula \\(H_0\\) fuera cierta. Del capítulo anterior, sabemos que el valor está directamente relacionado con el estadístico t, ya que a mayor valor absoluto de t, menor será el p-valor, lo que indica que la diferencia observada es menos probable bajo \\(H_0\\). En este caso, el p-valor es menor que \\(\\alpha = 0.05\\) (\\(2.2e-16\\)), por lo que rechazamos \\(H_0\\) y concluimos que la diferencia en las horas de ocio entre hombres y mujeres es estadísticamente significativa.\nAdemás, esta conclusión ya se podía intuir desde la construcción del intervalo de confianza, donde observamos que los hombres tienen entre 2.9 y 4.2 horas más de ocio semanal que las mujeres y, crucialmente, el intervalo no incluía el 0, lo que indicaba una diferencia significativa. El contraste de hipótesis nos ayuda a formalizar esta interpretación, permitiéndonos afirmar que esta diferencia no puede atribuirse únicamente al azar, lo que sugiere que el género influye en las horas de ocio.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comparando grupos</span>"
    ]
  },
  {
    "objectID": "comp.html#comparación-entre-proporciones",
    "href": "comp.html#comparación-entre-proporciones",
    "title": "7  Comparando grupos",
    "section": "7.2 Comparación entre proporciones",
    "text": "7.2 Comparación entre proporciones\nCuando trabajamos con variables cualitativas o categóricas, como el género, la preferencia electoral o la afiliación política, las comparaciones entre dos grupos se centran en analizar las proporciones asociadas a cada categoría. En lugar de comparar medias como en el caos anterior, aquí nos interesa evaluar si la proporción de individuos que pertenecen a una categoría específica difiere entre los grupos.\nRecuerda que la proporción muestral (\\(\\hat{p}\\)) es una estimación del porcentaje de individuos que pertenecen a una categoría específica en la población. Se calcula como:\n\\[\n\\hat{p} = \\frac{x}{n}\n\\]\ndonde:\n- \\(x\\) es el número de éxitos (individuos que apoyan la política ambiental).\n- \\(n\\) es el tamaño total de la muestra en cada grupo.\nFlor está investigando si existe una diferencia en el nivel de apoyo a una política ambiental entre personas que viven en zonas urbanas y rurales. Para ello, ha recolectado datos sobre el porcentaje de personas de cada grupo que han expresado estar a favor de la medida. El objetivo es comparar estas proporciones y determinar si la diferencia observada en la muestra es estadísticamente significativa o si simplemente podría explicarse por la variabilidad del muestreo.\nAsí como en la comparación de medias analizamos si la diferencia entre los promedios de dos grupos es significativa, aquí evaluamos si la diferencia entre proporciones (\\(\\Delta p\\))es lo suficientemente grande como para no atribuirse al azar.\nSi definimos las proporciones muestrales de apoyo como:\n\\[\n\\hat{p}_{urbana} = \\frac{x_{urbana}}{n_{urbana}}, \\quad \\hat{p}_{rural} = \\frac{x_{rural}}{n_{rural}}\n\\]\nentonces, la diferencia de proporciones muestrales se calcula como:\n\\[\n\\Delta p = \\hat{p}_{urbana} - \\hat{p}_{rural}\n\\]\nSi \\(\\Delta p\\) es cercano a cero, indicaría que ambos grupos tienen un nivel de apoyo similar. Si es positivo o negativo y lo suficientemente grande, podríamos sospechar que la zona de residencia influye en la opinión sobre la política ambiental.\nPara explorar estos datos, primero carga el conjunto de datos y examina su estructura:\n\nvotantes = read_csv(\"votantes.csv\")\n\n\nglimpse(votantes)\n\nRows: 350\nColumns: 2\n$ zona  &lt;chr&gt; \"urbana\", \"urbana\", \"urbana\", \"urbana\", \"urbana\", \"urbana\", \"urb…\n$ apoyo &lt;chr&gt; \"sí\", \"no\", \"sí\", \"no\", \"no\", \"sí\", \"sí\", \"no\", \"sí\", \"sí\", \"no\"…\n\n\nVisualizamos las proporciones de cada grupo, al ser variables categóricas utilizaremos una gráfico de barras apilado.\n\nvotantes %&gt;%\n  group_by(zona, apoyo) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  mutate(proporcion = n / sum(n)) %&gt;%\n  ggplot(aes(x = zona, y = proporcion, fill = apoyo)) +\n  # Esto permite realizar barras apiladas\n  geom_bar(stat = \"identity\", position = \"fill\") + \n  labs(\n    title = \"Proporción de apoyo por zona\",\n    y = \"Proporción\",\n    x = \"Zona\",\n    fill = \"Apoyo\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEfectivamente hay una diferencia, realicemos un intervalo de confianza y evaluemos su significancia.\n\n7.2.1 Estimación para la diferencia de proporciones\nAl comparar proporciones entre dos grupos, estamos interesados en la diferencia entre ellas. La diferencia muestral de proporciones se calcula como:\n\\[\n\\Delta {p} = \\hat{p}_1 - \\hat{p}_2\n\\]\nEn este caso, las proporciones observadas para las zonas urbanas y rurales son\n\n# Calcular las proporciones de apoyo en cada zona\nproporciones = votantes %&gt;%\n  group_by(zona) %&gt;%\n  summarise(\n    total = n(),\n    apoyo_si = sum(apoyo == \"sí\"),\n    proporcion_apoyo = apoyo_si / total\n  )\n\nproporciones\n\n# A tibble: 2 × 4\n  zona   total apoyo_si proporcion_apoyo\n  &lt;chr&gt;  &lt;int&gt;    &lt;int&gt;            &lt;dbl&gt;\n1 rural    150       70            0.467\n2 urbana   200      141            0.705\n\n\n\\[\n\\hat{p}_1 = 0.705 \\quad \\text{(proporción de apoyo en zonas urbanas)}\n\\]\n\\[\n\\hat{p}_2 = 0.467 \\quad \\text{(proporción de apoyo en zonas rurales)}\n\\]\nIntervalo de Confianza para la Diferencia de Proporciones\nEl intervalo de confianza para la diferencia de proporciones se calcula de manera similar al de la diferencia de medias, con la fórmula:\n\\[\n\\text{IC}_{\\Delta \\hat{p}} = \\Delta \\hat{p} \\pm z \\cdot \\text{EE},\n\\]\ndonde \\(\\text{EE}\\) es el error estándar estimado, dado por:\n\\[\n\\text{EE} = \\sqrt{\\frac{\\hat{p}_1(1 - \\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1 - \\hat{p}_2)}{n_2}},\n\\]\ny \\(z\\) es el valor crítico para el nivel de confianza deseado (por ejemplo, \\(z = 1.96\\) para un 95% de confianza).\nPodemos calcularlo manualmente en R\n\n# Proporción de apoyo en zonas urbanas\nprop_urbanos = votantes %&gt;%\n  filter(zona == \"urbana\", apoyo == \"sí\") %&gt;%\n  nrow() / votantes %&gt;%\n  filter(zona == \"urbana\") %&gt;%\n  nrow()\n\n# Proporción de apoyo en zonas rurales\nprop_rurales = votantes %&gt;%\n  filter(zona == \"rural\", apoyo == \"sí\") %&gt;%\n  nrow() / votantes %&gt;%\n  filter(zona == \"rural\") %&gt;%\n  nrow()\n\nprop_urbanos\n\n[1] 0.705\n\nprop_rurales\n\n[1] 0.4666667\n\n\nEn este caso:\n\\[\nn_1 = 200, \\quad n_2 = 150\n\\]\nLa diferencia muestral\n\ndiferencia_prop = prop_urbanos - prop_rurales\ndiferencia_prop\n\n[1] 0.2383333\n\n\nPor lo tanto, la diferencia muestral de proporciones es:\n\\[\n\\Delta \\hat{p} = 0.705 - 0.467 = 0.238\n\\] Cálculo del error estándar\n\nn_urbanos = votantes %&gt;% \n  filter(zona == \"urbana\") %&gt;% nrow()\n\nn_rurales = votantes %&gt;% \n  filter(zona == \"rural\") %&gt;% nrow()\n\nee = sqrt(\n  (prop_urbanos * (1 - prop_urbanos) / n_urbanos) +\n  (prop_rurales * (1 - prop_rurales) / n_rurales)\n)\n\nee\n\n[1] 0.05195319\n\n\nEl error estándar calculado es:\n\\[\n\\text{EE} = \\sqrt{\\frac{0.705 \\cdot (1 - 0.705)}{200} + \\frac{0.467 \\cdot (1 - 0.467)}{150}} = 0.05195\n\\]\nPor lo que el intervalo de confianza\n\nz = qnorm(0.975)\n\n# Siendo\n\nz\n\n[1] 1.959964\n\n# Intervalo de confianza\nic = c(diferencia_prop - z * ee, \n        diferencia_prop + z * ee)\n\nic\n\n[1] 0.1365069 0.3401597\n\n\n\\[\n\\text{IC}_{\\Delta \\hat{p}} = 0.238 \\pm 1.96 \\cdot 0.05195\n\\]\nPor lo que:\n\\[\n\\text{IC}_{\\Delta \\hat{p}} = [0.136, 0.340]\n\\]\nComo siempre, podemos calcular todo ello directamente en R, aunque primero debemos generar una tabla con los resultados por grupo:\n\ntabla = table(votantes$zona, votantes$apoyo)\n\ntabla\n\n        \n          no  sí\n  rural   80  70\n  urbana  59 141\n\n\n\nprueba_prop = prop.test(tabla, conf.level = 0.95)\n\n\nprueba_prop$conf.int\n\n[1] 0.1306736 0.3459931\nattr(,\"conf.level\")\n[1] 0.95\n\n\nInterpretación\nEl intervalo de confianza indica que, con un 95% de confianza, la diferencia en las proporciones poblacionales de apoyo entre las zonas urbanas y rurales está entre \\(13.6\\) y \\(34.0\\%\\).\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Contraste de hipótesis para la diferencia de proporciones\nEl contraste de hipótesis para la diferencia de proporciones entre dos grupos sigue una estructura similar al de medias, pero evalúa si la diferencia observada entre las proporciones muestrales es suficientemente extrema como para concluir que no se debe al azar. Aquí aplicamos este análisis al ejemplo del apoyo en zonas urbanas y rurales.\n1. Planteamiento de Hipótesis\nEl primer paso es formular nuestras hipótesis estadísticas:\n\nHipótesis nula (\\(H_0\\)): No existe diferencia en las proporciones poblacionales. Es decir: \\[\nH_0: \\pi_1 - \\pi_2 = 0\n\\]\nHipótesis alternativa (\\(H_1\\)): Existe una diferencia significativa en las proporciones poblacionales: \\[\nH_1: \\pi_1 - \\pi_2 \\neq 0\n\\]\n\nBajo \\(H_0\\), las proporciones de apoyo entre zonas urbanas y rurales son iguales. Si rechazamos \\(H_0\\), concluimos que hay una diferencia significativa entre las proporciones.\n2. Estadísticas Descriptivas\nPara realizar el contraste de hipótesis, primero presentamos las características de cada grupo.\nSabemos realizar la estimación que:\n\nProporción de apoyo en zonas urbanas (\\(\\hat{p}_1\\)): \\[\n\\hat{p}_1 = 0.705\n\\]\nProporción de apoyo en zonas rurales (\\(\\hat{p}_2\\)): \\[\n\\hat{p}_2 = 0.467\n\\]\n\nLa diferencia observada entre las proporciones es: \\[\n\\Delta \\hat{p} = \\hat{p}_1 - \\hat{p}_2 = 0.705 - 0.467 = 0.238.\n\\]\nLas muestras de cada grupo son:\n\\[\nn_1 = 200, \\quad n_2 = 150.\n\\]\n3. Nivel de Significancia\nDefinimos un nivel de significancia de: \\[\n\\alpha = 0.05,\n\\] lo que significa que aceptamos hasta un 5% de probabilidad de cometer un error al rechazar \\(H_0\\) si en realidad es verdadera.\n4. Realización de la Prueba\nPara evaluar si la diferencia observada \\(\\Delta \\hat{p} = 0.238\\) es significativa, calculamos el estadístico \\(z\\) bajo \\(H_0\\), es decir, bajo la presunción de \\(H_0\\) de que \\(\\pi_1 = \\pi_2\\)\nCálculo de la Estimación Combinada \\((\\hat{\\pi})\\)\nPara la realicación de la prueba debemos estimar el valor común de las proporciones poblacionales utilizando la proporción combinada para toda la muestra. Esta proporción \\((\\hat{\\pi})\\) se calcula como:\n\\[\n\\hat{\\pi} = \\frac{\\text{Éxitos totales}}{\\text{Tamaño total de la muestra}}.\n\\]\nDado que los datos son los siguientes:\n\n\n\nZona\nSí (\\(x\\))\nNo (\\(n - x\\))\nTotal (\\(n\\))\n\n\n\n\nRural\n70\n80\n150\n\n\nUrbana\n141\n59\n200\n\n\n\n\nÉxitos en zonas rurales: \\(x_1 = 70\\).\nÉxitos en zonas urbanas: \\(x_2 = 141\\).\nTamaño total de la muestra: \\(n_1 + n_2 = 150 + 200 = 350\\).\n\nSustituyendo en la fórmula:\n\\[\n\\hat{\\pi} = \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{70 + 141}{150 + 200} = \\frac{211}{350} \\approx 0.603.\n\\]\nLa proporción combinada (\\(\\hat{\\pi}\\)) indica que, bajo la hipótesis nula, el apoyo total se distribuye igualmente entre las dos zonas.\n\npi_combinada = votantes %&gt;%\n  group_by(zona) %&gt;%\n  summarise(\n    exitos = sum(apoyo == \"sí\"),   \n    total = n()                    \n  ) %&gt;%\n  summarise(\n    exitos_totales = sum(exitos),  \n    total_muestra = sum(total),   \n    pi = exitos_totales / total_muestra  \n  ) %&gt;%\n  pull(pi) \n\nPor lo que:\n\npi_combinada\n\n[1] 0.6028571\n\n\n\\[\n\\hat{\\pi} = 0.603.\n\\]\nCálculo del Error Estándar bajo \\(H_0\\) (\\(se_0\\))\nEl error estándar bajo \\(H_0\\) se calcula como:\n\\[\nse_0 = \\sqrt{\\hat{\\pi} (1 - \\hat{\\pi}) \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}.\n\\] Siendo \\(\\hat{\\pi}\\) es la porción combinada.\nSustituyendo: \\[\nse_0 = \\sqrt{0.603 \\cdot (1 - 0.603) \\left(\\frac{1}{200} + \\frac{1}{150}\\right)} = \\sqrt{0.603 \\cdot 0.397 \\cdot 0.01167} = 0.0528.\n\\]\nEn R\n\nse_nula = sqrt(\n  pi_combinada * (1 - pi_combinada) * (1 / n_urbanos + 1 / n_rurales)\n)\nse_nula\n\n[1] 0.05285109\n\n\nCálculo del Estadístico \\(z\\)\nEl estadístico \\(z\\) mide cuántos errores estándar separan la diferencia observada (\\(\\Delta \\hat{p}\\)) del valor esperado bajo \\(H_0\\) (que es 0):\n\\[\nz = \\frac{\\Delta \\hat{p} - 0}{se_0}.\n\\]\nSustituyendo: \\[\nz = \\frac{0.238}{0.0528} \\approx 4.51.\n\\]\n\nZonas de Rechazo\n\nBajo \\(H_0\\), las zonas de rechazo se definen como:\n\\[\np_0 \\pm z \\cdot se_0.\n\\] Sabemos que \\(z\\) es igual a\n\nz_alpha = qnorm(0.975) \n\nz_alpha\n\n[1] 1.959964\n\n\nSustituyendo \\(p_0 = 0\\), \\(z_{\\alpha/2} = 1.96\\), y \\(se_0 = 0.0528\\): \\[\n[-1.96 \\cdot 0.0528, 1.96 \\cdot 0.0528] = [-0.1035, 0.1035].\n\\] En R\n\n# Límites de las zonas de rechazo\nlim_inf &lt;- 0 - z_alpha * se_nula\nlim_sup &lt;- 0 + z_alpha * se_nula\n\nlim_inf\n\n[1] -0.1035862\n\nlim_sup\n\n[1] 0.1035862\n\n\nDado que el valor observado (\\(0.238\\)) está fuera de este intervalo, cae en la zona de rechazo.\nNuevamente, esto se puede hacer facilmente en R utilizando la tabla de contingencia ya creada.\n\nprueba_prop = prop.test(tabla, conf.level = 0.95)\n\ntidy(prueba_prop)\n\n# A tibble: 1 × 9\n  estimate1 estimate2 statistic   p.value parameter conf.low conf.high method   \n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1     0.533     0.295      19.4 0.0000109         1    0.131     0.346 2-sample…\n# ℹ 1 more variable: alternative &lt;chr&gt;\n\n\nEl estadístico \\(z = 4.51\\) corresponde a un valor \\(p\\) extremadamente pequeño (\\(p \\approx 0.000008\\)), menor que \\(\\alpha = 0.05\\). Por lo tanto rechazamos \\(H_0\\) y podemos concluir que existe una diferencia significativa en las proporciones de apoyo entre zonas urbanas y rurales.\nLa interpretación gráfica es la siguiente:\n\n\n\n\n\n\n\n\n\nEste análisis nos permite afirmar que las diferencias observadas no son atribuibles al azar, con un nivel de confianza del 95%.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comparando grupos</span>"
    ]
  },
  {
    "objectID": "comp.html#comparaciones-de-dos-grupos-relacionados",
    "href": "comp.html#comparaciones-de-dos-grupos-relacionados",
    "title": "7  Comparando grupos",
    "section": "7.3 Comparaciones de dos grupos relacionados",
    "text": "7.3 Comparaciones de dos grupos relacionados\nImagina que Flor ahora está interesada en si existe una diferencia en el rendimiento de un grupo de estudiantes antes y después de una intervención educativa. Para ello, ha recopilado datos sobre el puntaje de los mismos estudiantes antes y después del programa, permitiendo analizar si hubo una mejora significativa en sus resultados.\nEste es un caso de muestras relacionadas o pareadas, ya que cada observación en la primera condición (antes de la intervención) tiene una relación directa con una observación en la segunda condición (después de la intervención). A diferencia de la comparación entre grupos independientes, aquí no estamos viendo si dos poblaciones son distintas, sino si hay un cambio dentro de la misma población en dos momentos distintos.\nFlor está investigando si un programa de refuerzo educativo mejora el rendimiento de los estudiantes en una evaluación estandarizada. Para ello, ha diseñado un estudio donde mide el desempeño de un grupo de estudiantes antes y después de recibir el programa de apoyo. En este caso, se utiliza una prueba estandarizada con una escala de 0 a 100 puntos. El diseño del estudio es de muestras pareadas porque cada estudiante tiene dos mediciones:\n- antes: Puntaje en la prueba antes de recibir el programa de refuerzo.\n- después: Puntaje en la misma prueba, aplicada nuevamente tras la intervención.\nEste enfoque permite analizar si el programa tuvo un impacto real en el rendimiento de los estudiantes, comparando los puntajes de cada individuo en dos momentos distintos. En lugar de comparar grupos independientes, aquí evaluamos el cambio dentro de los mismos estudiantes, lo que hace que el análisis sea más preciso y reduzca la variabilidad causada por diferencias individuales. Por tanto, Flor está evaluando si la diferencia observada en los puntajes antes y después del programa es estadísticamente significativa y si puede atribuirse al programa de refuerzo educativo en lugar del azar.\nCarga los datos y observa su estructura:\n\nrendimiento = read_csv('rendimiento.csv')\n\n\nglimpse(rendimiento)\n\nRows: 50\nColumns: 3\n$ estudiante &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ antes      &lt;dbl&gt; 41, 45, 67, 49, 50, 69, 54, 33, 40, 43, 63, 52, 53, 49, 41,…\n$ después    &lt;dbl&gt; 53, 56, 78, 67, 60, 88, 57, 47, 35, 32, 76, 60, 62, 55, 32,…\n\n\nPara entender la distribución de los puntajes antes y después de la intervención, utilizamos describeBy del paquete psych.\n\nlibrary(psych)\n\ndescribeBy(rendimiento, group = NULL)\n\n           vars  n  mean    sd median trimmed   mad min max range  skew\nestudiante    1 50 25.50 14.58   25.5   25.50 18.53   1  50    49  0.00\nantes         2 50 48.38 11.14   47.0   48.12 10.38  24  74    50  0.17\ndespués       3 50 57.68 14.24   60.0   57.88 11.86  28  88    60 -0.14\n           kurtosis   se\nestudiante    -1.27 2.06\nantes         -0.53 1.58\ndespués       -0.53 2.01\n\n\nFíjate en los valores de la media, mediana y desviación estándar en los puntajes antes y después de la intervención. La media pasó de 48.38 a 57.68, lo que sugiere una mejora en el rendimiento de los estudiantes. La mediana aumentó de 47 a 60, lo que indica que más de la mitad de los estudiantes tienen puntajes más altos después del programa. Sin embargo, la desviación estándar también aumentó (de 11.14 a 14.24), lo que nos dice que los puntajes después de la intervención son más dispersos. Esto podría deberse a que algunos estudiantes mejoraron, mientras que otros se mantuvieron iguales o incluso bajaron su rendimiento. Pero la verdadera pregunta es ¿qué tan diferente es este cambio? ¿Es suficiente para concluir que el programa realmente tuvo un efecto o pudo haber ocurrido por azar? Para responder esto, necesitamos un análisis estadístico más riguroso.\nPodemos visualizar la diferencia de manera más intuitiva con un boxplot pareado, donde observamos cómo cambian los valores antes y después para cada estudiante.\n\nrendimiento %&gt;%\n  pivot_longer(cols = c(antes, después), names_to = \"momento\", values_to = \"puntaje\") %&gt;%\n  ggplot(aes(x = momento, y = puntaje, fill = momento)) +\n  geom_boxplot() +\n  labs(title = \"Distribución de puntajes antes y después del programa\", \n       x = \"\", y = \"Puntaje\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nVemos que efectivamente hay una diferencia. La clave aquí es determinar qué tan grande es la diferencia y si puede atribuirse al azar o a un efecto real del programa.\nDado que estamos trabajando con muestras pareadas, en lugar de comparar las medias de dos grupos independientes, calculamos las diferencias dentro de cada individuo\n\\[\nd_i = \\text{después}_i - \\text{antes}_i\n\\]\nLuego, analizamos la media y la dispersión de estas diferencias:\n\nrendimiento = rendimiento %&gt;%\n  mutate(diferencia = después - antes)\n\nestadisticas = rendimiento %&gt;%\n  summarise(\n    promedio = mean(diferencia),\n    sd = sd(diferencia),\n    n = n()\n  )\n\nestadisticas\n\n# A tibble: 1 × 3\n  promedio    sd     n\n     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1      9.3  7.85    50\n\n\nEsto nos da la diferencia promedio observada en los puntajes tras la intervención.\nPara evaluar si el efecto de la intervención es significativo, construimos un intervalo de confianza del 95% para la diferencia de medias pareadas. La fórmula sigue la misma lógica que en el caso de muestras independientes, pero ahora el error estándar se basa únicamente en la distribución de las diferencias pareadas:\n\\[\n\\text{Error estándar} = \\frac{s_d}{\\sqrt{n}}\n\\]\ndonde \\(s_d\\) es la desviación estándar de las diferencias y \\(n\\) es el número de pares de observaciones.\nAquí tienes el resto del código con la notación LaTeX correctamente aplicada:\nPor lo que\n\\[\n\\text{Error estándar} = \\frac{7.85}{\\sqrt{50}} = 1.1\n\\]\nCalculamos el error estándar en R:\n\nee = estadisticas$sd / sqrt(estadisticas$n)\nee\n\n[1] 1.109973\n\n\nPor lo que \\(EE = 1.1\\)\nAhora obtenemos el valor crítico \\(t\\) para un intervalo de confianza del 95%, utilizando la distribución \\(t\\) de Student con \\(n-1\\) grados de libertad:\n\\[\ngl = n - 1 = 50 - 1 = 49\n\\]\n\ngl = estadisticas$n - 1  \nt = qt(0.975, df = gl)\nt\n\n[1] 2.009575\n\n\nPor lo que el valor crítico \\(t\\) es aproximadamente \\(t_{0.975, 49} = 2.009\\)\nFinalmente, construimos el intervalo de confianza para la diferencia de medias:\n\\[\nIC = \\bar{d} \\pm t \\cdot EE\n\\]\n\nic = c(estadisticas$promedio - t * ee, estadisticas$promedio + t * ee)\nic\n\n[1]  7.069425 11.530575\n\n\nLo que nos da un intervalo de confianza de:\n\\[\nIC = (5.2, 10.1)\n\\]\nEsto significa que, con un 95% de confianza, podemos decir que la media de las diferencias individuales en la población (\\(\\bar{d}\\)) se encuentra dentro de este rango. Como el intervalo no incluye el 0, esto sugiere que la diferencia observada en los puntajes no se debe al azar, sino que hay evidencia de que el programa tuvo un efecto real en el rendimiento de los estudiantes. Pero realicemos un contraste de hipótesis para complementar el analisis.\nEl t-test para muestras pareadas nos permite evaluar si la diferencia observada es estadísticamente significativa.\n\nprueba_pareada = t.test(rendimiento$antes, \n                        rendimiento$después, \n                        # Indicamos que los datos están relacionados\n                        paired = TRUE,  \n                        mu = 0, \n                        alternative = \"two.sided\", \n                        conf.level = 0.95)\n\ntidy(prueba_pareada)\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1     -9.3     -8.38 5.07e-11        49    -11.5     -7.07 Paired t… two.sided  \n\n\nAnalizando el resultado de la prueba vemos que el estadístico t es -8.38, lo que indica que la diferencia entre los puntajes antes y después es mucho mayor de lo que esperaríamos por azar (la diferencia observada esta mas de 8 errores estándar lejos de la media de \\(H_0:\\text{No hay diferencia}(\\bar d= 0)\\)). Además, el p-valor es 5.07e-11, es decir, prácticamente cero, lo que nos permite rechazar la hipótesis nula \\(H_0\\) con un altísimo nivel de confianza.\nDado que el p-valor es menor que \\(\\alpha = 0.05\\), tenemos evidencia suficiente para concluir que la intervención educativa tuvo un impacto significativo en el rendimiento de los estudiantes. La consistencia entre la prueba de hipótesis y el intervalo de confianza refuerza la conclusión de que el efecto no es producto del azar, sino una diferencia real en la población.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comparando grupos</span>"
    ]
  },
  {
    "objectID": "comp.html#comparación-de-múltiples-grupos",
    "href": "comp.html#comparación-de-múltiples-grupos",
    "title": "7  Comparando grupos",
    "section": "7.4 Comparación de múltiples grupos",
    "text": "7.4 Comparación de múltiples grupos\nPerfecto, ya sabemos que cuando queremos comparar dos grupos, utilizamos pruebas como el t-test para evaluar si hay diferencias significativas en sus medias. Pero, ¿qué sucede cuando tenemos más de dos grupos?\n\n7.4.1 ANOVA\nCuando comparamos variación en las medias de los dinstitnos grupos, la prueba de ANOVA (Análisis de Varianza) es la herramienta adecuada. Las pruebas ANOVA siguen la misma lógica e untición matemática vista en las anteriores pruebas pero los cálculos matemáticos son algo más complejas y están fuera del propósito del libro, aunque te animo a revisarlas por ti mismo.\nImagina que Flor está investigando si existe una diferencia en la satisfacción laboral entre distintos sectores económicos. Para ello, ha recopilado datos de empleados en cinco sectores: Tecnología, Finanzas, Educación, Salud y Manufactura. Su objetivo es determinar si el nivel de satisfacción laboral varía significativamente entre los grupos.\nPara medir la satisfacción, cada empleado calificó su experiencia laboral en una escala de 0 a 10, donde:\n\n\n\nPuntuación\nNivel de Satisfacción\n\n\n\n\n0 - 2\nMuy insatisfecho\n\n\n3 - 4\nInsatisfecho\n\n\n5 - 6\nNeutral\n\n\n7 - 8\nSatisfecho\n\n\n9 - 10\nMuy satisfecho\n\n\n\nANOVA nos permite comparar las medias de tres o más grupos y determinar si al menos uno de ellos es significativamente diferente de los demás. En lugar de realizar múltiples t-tests (lo que aumentaría el riesgo de error tipo I), ANOVA evalúa si la variabilidad entre los grupos es mayor que la variabilidad dentro de cada grupo, lo que indicaría que las diferencias observadas no se deben al azar.\nEn pruebas anteriores vimos que, cuando comparamos dos grupos, usamos la distribución t para evaluar la diferencia entre sus medias. La distribución \\(t\\) es útil para comparar dos muestras porque se basa en la diferencia de medias y la variabilidad dentro de cada grupo. En lugar de eso, ANOVA utiliza la distribución F, que compara la variabilidad entre los grupos con la variabilidad dentro de los grupos en una sola prueba.\nEl estadístico F se define como:\n\\[\nF = \\frac{\\text{Variabilidad entre grupos}}{\\text{Variabilidad dentro de los grupos}}\n\\]\n\n¿Pero por qué nos interesa comparar la variabilidad entre los grupos con la variabilidad dentro de los grupos? Porque esto nos dice si las diferencias observadas son reales o simplemente producto del azar. Si la variabilidad entre los grupos es similar a la variabilidad dentro de ellos, significa que las diferencias en las medias no son mayores que las fluctuaciones normales que ocurren dentro de cada grupo. En otras palabras, aunque haya cierta dispersión en los datos, esta dispersión es comparable en todos los grupos, por lo que no hay razones para pensar que alguno de ellos sea realmente diferente. Sin embargo, si la variabilidad entre los grupos es mucho mayor que la variabilidad dentro de ellos, entonces estamos viendo diferencias que van más allá de la variación esperada dentro de cada grupo. Esto nos sugiere que al menos un grupo tiene una media que se aleja significativamente de los demás. Esa es la clave del ANOVA.\nLa distribución F tiene características particulares que la hacen adecuada para comparar varianzas y analizar relaciones entre grupos.Por un lado, la distribución F siempre toma valores positivos. Esto ocurre porque se construye a partir de la razón de dos varianzas, y una varianza nunca puede ser negativa. Por esta razón, su forma es asimétrica y se extiende hacia la derecha, lo que significa que valores grandes de F indican una mayor diferencia entre los grupos analizados.\nOtra propiedad importante es que la forma de la distribución F cambia según los grados de libertad. En cada prueba ANOVA, hay dos conjuntos de grados de libertad: uno asociado a la variabilidad entre los grupos y otro a la variabilidad dentro de los grupos. A medida que aumentan los grados de libertad, la distribución F se va pareciendo más a una distribución normal, pero sigue siendo asimétrica.\n\n\n\nExtraído de: https://es.wikipedia.org/wiki/Distribución_F\n\n\nLa distribución F nos permite determinar qué tan extremo debe ser un valor F para rechazar la hipótesis nula. Valores bajos de F indican que las diferencias entre los grupos no son mayores que las diferencias dentro de ellos, mientras que valores altos sugieren que al menos un grupo es significativamente distinto. Por eso, en las pruebas ANOVA, cuando obtenemos un F suficientemente grande, podemos concluir que hay diferencias estadísticamente significativas entre los grupos analizados.\nEl primer paso es formular nuestras hipótesis estadísticas:\n\nHipótesis nula (\\(H_0\\)): No existe diferencia en la satisfacción laboral entre los sectores. Es decir:\n\n\\[\n    H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5\n\\]\n\nHipótesis alternativa (\\(H_1\\)): Al menos un sector tiene una media de satisfacción diferente:\n\n\\[\n    H_1: \\text{Al menos un } \\mu_i \\text{ es diferente}\n\\]\nBajo (\\(H_0\\)), los niveles de satisfacción en los distintos sectores son iguales. Si rechazamos (\\(H_0\\)), concluimos que existe una diferencia significativa en la satisfacción laboral entre al menos un grupo.\nCargamos los datos de la encuesta\n\nsatisfaccion = read_csv('base_satisfaccion.csv')\n\n\nglimpse(satisfaccion)\n\nRows: 150\nColumns: 2\n$ sector             &lt;chr&gt; \"Educación\", \"Educación\", \"Educación\", \"Educación\",…\n$ nivel_satisfaccion &lt;dbl&gt; 7, 9, 0, 9, 5, 4, 5, 9, 0, 10, 6, 7, 6, 2, 9, 7, 4,…\n\n\nAntes de realizar la prueba ANOVA, exploramos las características de cada grupo:\n\n# Estadísticas descriptivas por sector\nsatisfaccion %&gt;%\n  group_by(sector) %&gt;%\n  summarise(\n    media = mean(nivel_satisfaccion),\n    desviacion = sd(nivel_satisfaccion),\n    n = n()\n  )\n\n# A tibble: 5 × 4\n  sector      media desviacion     n\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1 Educación    4.9        2.89    30\n2 Finanzas     4.03       2.74    30\n3 Manufactura  2.4        1.40    30\n4 Salud        5.97       2.46    30\n5 Tecnología   4.93       2.50    30\n\n\nPara visualizar las diferencias en satisfacción laboral entre sectores, podemos utilizar un boxplot:\n\nggplot(satisfaccion, aes(x = sector, \n                         y = nivel_satisfaccion, \n                         fill = sector)) +\n  geom_boxplot() +\n  labs(title = \"Distribución de la satisfacción laboral por sector\",\n       x = \"Sector Económico\",\n       y = \"Nivel de Satisfacción\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nVemos que los niveles de satisfacción varían entre los sectores, pero ¿son estas diferencias lo suficientemente grandes como para concluir que el sector influye en la satisfacción laboral? En otras palabras, ¿la variabilidad en la satisfacción está asociada al sector o varía de forma independiente? En ANOVA, el concepto de independencia significa que, si no hay relación entre el sector y la satisfacción laboral, la variabilidad en los niveles de satisfacción sería similar tanto dentro de cada sector como entre ellos. En este caso, diríamos que la satisfacción laboral es independiente del sector, ya que pertenecer a uno u otro no influye en la percepción de satisfacción.\nPor el contrario, si los niveles de satisfacción son muy distintos entre sectores pero relativamente homogéneos dentro de cada uno, significa que la satisfacción depende del sector económico. Es decir, el sector en el que una persona trabaja influye en su nivel de satisfacción laboral, y las diferencias observadas no pueden atribuirse únicamente al azar.\nANOVA nos permite responder esta cuestión: ¿las diferencias en satisfacción laboral reflejan un efecto real del sector o simplemente variaciones dentro de los grupos? Si encontramos diferencias significativas, concluimos que la satisfacción no es independiente del sector y que existe una relación que merece un análisis más detallado.\nUtilizamos la función aov() que realiza un ANOVA. Esta función ajusta un modelo estadístico donde la variable repsuesta (satisfacción laboral) se compara entre los niveles de la variable explicativa (sector económico). Al ejecutar summary(), obtenemos una tabla con los resultados del análisis, incluyendo el estadístico F y su p-valor, que nos indican si existe una diferencia significativa entre los grupos. Fíjate en el Pr(&gt;F)\n\n# Realizar ANOVA de una vía para comparar la satisfacción laboral entre sectores\nanova_resultado = aov(nivel_satisfaccion ~ sector, \n                      data = satisfaccion)\n\n# Mostrar el resumen del ANOVA con el estadístico F y el p-valor\nsummary(anova_resultado)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsector        4  213.4   53.34   8.853 2.01e-06 ***\nResiduals   145  873.7    6.03                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPodmeos usar tidy() de la misma forma\n\ntidy(anova_resultado)\n\n# A tibble: 2 × 6\n  term         df sumsq meansq statistic     p.value\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 sector        4  213.  53.3       8.85  0.00000201\n2 Residuals   145  874.   6.03     NA    NA         \n\n\nEl p-valor de 2.1e-06 indica los mismo que anteriores pruebas, es decir, la probabilidad de obtener una variabilidad entre los sectores tan grande como la observada si en realidad no hubiera ninguna diferencia real en la satisfacción laboral (es decir, si la hipótesis nula fuera cierta). Dado que este valor es mucho menor que 0.05, rechazamos la hipótesis nula con un alto nivel de confianza. Esto significa que la satisfacción laboral no varía de manera independiente del sector económico, sino que existe una diferencia significativa entre al menos uno de los sectores, lo que sugiere que el sector de trabajo influye en el nivel de satisfacción.\nOjo, el resultado del ANOVA nos indica que existen diferencias significativas en la satisfacción laboral entre los sectores, pero no nos dice qué sectores son distintos entre sí ni cómo esas diferencias afectan al resto de los grupos. ANOVA solo nos informa que al menos un sector tiene una media diferente, pero no nos especifica cuáles ni en qué dirección.\nPara profundizar en estos resultados, necesitamos una prueba post-hoc, como la prueba de Tukey, que compara todas las combinaciones posibles de sectores y ajusta los valores p para evitar aumentar el error tipo I. Esto es clave porque, en algunos casos, una diferencia extrema en un grupo puede estar impulsando el resultado global del ANOVA y, al mismo tiempo, influir en las diferencias entre otros sectores.\n\ntukey = TukeyHSD(anova_resultado)\n\ntidy(tukey)\n\n# A tibble: 10 × 7\n   term   contrast            null.value estimate conf.low conf.high adj.p.value\n   &lt;chr&gt;  &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 sector Finanzas-Educación           0  -0.867    -2.62      0.884 0.649      \n 2 sector Manufactura-Educac…          0  -2.50     -4.25     -0.749 0.00116    \n 3 sector Salud-Educación              0   1.07     -0.684     2.82  0.448      \n 4 sector Tecnología-Educaci…          0   0.0333   -1.72      1.78  1.000      \n 5 sector Manufactura-Finanz…          0  -1.63     -3.38      0.117 0.0800     \n 6 sector Salud-Finanzas               0   1.93      0.183     3.68  0.0225     \n 7 sector Tecnología-Finanzas          0   0.900    -0.851     2.65  0.616      \n 8 sector Salud-Manufactura            0   3.57      1.82      5.32  0.000000907\n 9 sector Tecnología-Manufac…          0   2.53      0.783     4.28  0.000954   \n10 sector Tecnología-Salud             0  -1.03     -2.78      0.717 0.481      \n\n\nPara interpretar los resultados, observamos cuatro elementos clave. La columna diff muestra la diferencia promedio en satisfacción entre los sectores comparados; un valor positivo indica que el primer sector tiene mayor satisfacción que el segundo, mientras que un valor negativo indica lo contrario. Las columnas lwr y upr representan el intervalo de confianza del 95%, lo que significa que, si este intervalo incluye el cero, la diferencia entre los sectores no es estadísticamente significativa. Finalmente, la columna p adj nos indica si la diferencia observada es lo suficientemente grande como para ser considerada significativa, siendo relevante cuando es menor a 0.05.\nEn nuestra muestra, los resultados muestran que manufactura tiene una satisfacción laboral significativamente menor en comparación con Educación, Finanzas, Salud y Tecnología, ya que sus diferencias de medias son negativas y tienen p-valores menores a 0.05. En cambio, entre los otros sectores no hay diferencias significativas, esto nos puede hacer sospechar que la variabilidad detectada en el ANOVA se debe principalmente a Manufactura, mientras que el resto de los sectores presentan niveles de satisfacción comparables.\nPara verificar si la variabilidad detectada en el ANOVA se debe principalmente a manufactura, realizamos nuevamente la prueba excluyendo este sector y comparando únicamente Tecnología, Finanzas, Educación y Salud. S\n\n# Filtrar la base de datos excluyendo el sector Manufactura\nsatisfaccion_sin_manufactura = satisfaccion %&gt;%\n  filter(sector != \"Manufactura\")\n\n# Realizar ANOVA de una vía sin Manufactura\nanova_sin_manufactura = aov(nivel_satisfaccion ~ sector, \n                             data = satisfaccion_sin_manufactura)\n\n# Mostrar los resultados del ANOVA\ntidy(anova_sin_manufactura)\n\n# A tibble: 2 × 6\n  term         df sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 sector        3  56.3  18.8       2.67  0.0511\n2 Residuals   116 816.    7.04     NA    NA     \n\n\nEl nuevo ANOVA sin Manufactura muestra que ya no hay diferencias significativas en la satisfacción laboral entre los sectores restantes (Tecnología, Finanzas, Educación y Salud). El p-valor de 0.51 muy superior al umbral de 0.05, lo que significa que no hay suficiente evidencia para decir que estos sectores tienen niveles de satisfacción distintos. En otras palabras, cualquier diferencia observada entre ellos es pequeña y puede explicarse simplemente por variaciones dentro de los grupos, sin que el sector tenga un efecto real sobre la satisfacción laboral.\n¿Ves lo importante que es complementar un ANOVA con otras pruebas? Si solo nos hubiéramos quedado con el primer resultado, podríamos haber pensado que todos los sectores tenían diferencias significativas en la satisfacción laboral. Sin embargo, al hacer la prueba sin Manufactura, el ANOVA nos dice algo muy diferente: las diferencias son menores. Esto confirma que la variabilidad detectada en el primer ANOVA se debía principalmente a Manufactura, que tenía niveles de satisfacción claramente más bajos.\nEsto nos ayuda a evitar errores de interpretación. Si hubiéramos concluido que todos los sectores eran diferentes, habríamos exagerado el efecto del sector en la satisfacción laboral. Aquí es donde las pruebas post-hoc como Tukey son clave, porque nos permiten entender qué sectores están generando las diferencias en el ANOVA y evitar conclusiones erróneas.\nEl ejemplo que hemos descrito es el de un ANOVA de un solo factor, que se utiliza para comparar las medias de tres o más grupos basados en un solo factor. Este tipo de ANOVA permite determinar si existe una diferencia significativa entre las medias de los grupos. Existen otros tipos de ANOVA, como la ANOVA de dos factores, que evalúa la interacción entre dos factores, y la ANOVA de medidas repetidas, que se usa cuando se mide la misma variable en los mismos sujetos en diferentes condiciones. Sugiero le eches un vistazo a estos métodos ya que son importantes en algunos contextos más complejos.\n\n\n7.4.2 Chi-Cuadrado\nDe acuerdo, si la prueba ANOVA nos permite comparar medias entre tres o más grupos cuando los datos son numéricos.¿Qué ocurre cuando queremos analizar proporciones en grupos diferentes? En estos casos, la prueba de Chi-cuadrado es la herramienta adecuada.\nFlor ahora está investigando si el nivel educativo de una persona influye en el tipo de empleo que obtiene. Para ello, ha recopilado datos de 500 personas y ha clasificado su educación en tres categorías:\n\nBásica: Primaria o secundaria incompleta.\n\nMedia: Secundaria completa o educación técnica.\n\nSuperior: Educación universitaria o posgrado.\n\nTambién ha registrado el tipo de empleo de cada persona:\n\nInformal: Trabajo sin beneficios ni contrato legal.\n\nFormal: Trabajo con contrato y beneficios laborales.\n\nEl objetivo de Flor es determinar si la distribución de empleo formal e informal varía según el nivel educativo.\nSi la educación no influye en el tipo de empleo, esperaríamos que las proporciones de empleados formales e informales sean similares en todos los niveles educativos. Sin embargo, si hay una relación, podríamos ver que, por ejemplo, las personas con educación superior tienen más empleo formal que aquellas con educación básica. a prueba de Chi-cuadrado de independencia nos ayuda a responder la pregunta:\n\n¿Existe una relación entre el nivel educativo y el tipo de empleo, o las diferencias observadas podrían explicarse solo por azar?\n\nPara hacerlo, la prueba compara las frecuencias observadas con las frecuencias esperadas. Las frecuencias observadas son simplemente los datos de nuestra muestra, es decir, cuántas personas en cada nivel educativo tienen empleo formal o informal. Pero, ¿qué pasaría si el nivel educativo no tuviera ninguna relación con el tipo de empleo? En ese caso, las proporciones de empleo formal e informal deberían ser aproximadamente las mismas en todos los niveles educativos. Estas son las frecuencias esperadas, es decir, cuántas personas en cada categoría deberíamos ver si no existiera ninguna relación entre las variables. Si las frecuencias observadas son muy diferentes de las esperadas, significa que el tipo de empleo varía según el nivel educativo y que existe una asociación entre ambas variables. La prueba de Chi-cuadrado compara estas diferencias y nos dice si son lo suficientemente grandes como para descartar que se deban al azar.\nLa hipótesis nula \\((H_0)\\) plantea que no hay relación entre el nivel educativo y el tipo de empleo:\n\\[\nH_0: \\text{El tipo de empleo es independiente del nivel educativo}\n\\]\nLa hipótesis alternativa \\((H_1)\\) establece que sí existe una relación:\n\\[\nH_1: \\text{El tipo de empleo depende del nivel educativo}\n\\]\nSi rechazamos \\(H_0\\), significa que el nivel educativo influye en la probabilidad de obtener empleo formal o informal.\nImporta los datos\n\nedu_emp = read_csv(\"educacion_emp.csv\")\n\n\nglimpse(edu_emp)\n\nRows: 500\nColumns: 2\n$ nivel_educativo &lt;chr&gt; \"Media\", \"Superior\", \"Básica\", \"Superior\", \"Superior\",…\n$ tipo_empleo     &lt;chr&gt; \"Formal\", \"Formal\", \"Informal\", \"Formal\", \"Formal\", \"F…\n\n\nPara visualizar mejor la distribución, utilizamos un gráfico de barras apiladas:\n\nedu_emp %&gt;%\n  ggplot(aes(x = nivel_educativo, fill = tipo_empleo)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Distribución del tipo de empleo según nivel educativo\",\n       x = \"Nivel Educativo\",\n       y = \"Proporción\",\n       fill = \"Tipo de empleo\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nPodemos observar una tendencia: a medida que aumenta el nivel educativo, la frecuencia de empleo informal se reduce y el empleo formal se vuelve más común. Sin embargo, ¿esta diferencia es lo suficientemente significativa? Es decir, ¿es suficiente para afirmar que el nivel educativo y el tipo de empleo están relacionados, o podría tratarse solo de una variación aleatoria en los datos? Para responder a esta pregunta, utilizamos la prueba de Chi-cuadrado, que nos permite evaluar si la distribución del empleo formal e informal varía significativamente según el nivel educativo.\nLa prueba de Chi-cuadrado mide cuánto difieren las frecuencias observadas de las frecuencias esperadas bajo la hipótesis nula. Se calcula con la fórmula:\n\\[\n\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n\\]\nDonde:\n\n\\(O\\) son las frecuencias observadas (datos reales).\n\\(E\\) son las frecuencias esperadas bajo la hipótesis de independencia.\n\nAl igual que ANOVA utiliza la distribución F para comparar varianzas entre grupos, la prueba de Chi-cuadrado (\\(\\chi^2\\)) se basa en la distribución Chi-cuadrado para comparar frecuencias observadas y esperadas. Esta distribución siempre toma valores positivos, porque está basada en la suma de cuadrados de las diferencias entre frecuencias observadas y esperadas. Además, la forma de la distribución \\(\\chi^2\\) depende del número de grados de libertad (\\(k\\)), que en este caso están determinados por el número de categorías en la tabla de contingencia. Cuando hay pocos grados de libertad, la distribución es altamente asimétrica y sesgada hacia la derecha. A medida que los grados de libertad aumentan, la distribución se va pareciendo más a una normal, lo que permite que los valores críticos sean más estables.\nEn la prueba de Chi-cuadrado para tablas de contingencia, el estadístico \\(\\chi^2\\) mide qué tan grandes son las diferencias entre las frecuencias observadas y las frecuencias esperadas bajo la hipótesis nula. Si las diferencias son pequeñas, el estadístico \\(\\chi^2\\) tomará valores bajos, lo que sugiere que los grupos tienen distribuciones similares. Pero si las diferencias son grandes, el valor de \\(\\chi^2\\) será alto, lo que indica que al menos una de las categorías se aleja de lo esperado. La región de rechazo se encuentra en la cola derecha de la distribución, ya que valores altos de \\(\\chi^2\\) implican una mayor discrepancia entre lo observado y lo esperado.\n\n\n\nExtraído de: https://en.wikipedia.org/wiki/Chi-squared_distribution\n\n\nEn R, realizamos la prueba con chisq.test() volviendo los datos una tabla primero:\n\ntabla_educacion = table(edu_emp)\n\nprueba_chi = chisq.test(tabla_educacion)\n\ntidy(prueba_chi)\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      10.3 0.00585         2 Pearson's Chi-squared test\n\n\nLa prueba nos arroja una serie de estadísticos, interpétemoslos:\n\\[\n\\chi^2 = 10.3, \\quad p\\text{-valor} = 0.00585, \\quad \\text{k} = 2\n\\]\n\n\\(\\chi^2 = 10.3\\) es el estadístico de Chi-cuadrado, que mide cuánto difieren las frecuencias observadas de las esperadas. Un valor mayor indica una mayor discrepancia entre lo que observamos en la muestra y lo que esperaríamos si las variables fueran independientes.\n\n\\(p\\text{-valor} = 0.00585\\) representa la probabilidad de obtener una diferencia tan extrema entre lo observado y lo esperado si en realidad no hubiera relación entre el nivel educativo y el tipo de empleo.\n\n\\(\\text{k} = 2\\) son los grados de libertad, que dependen del número de categorías en cada variable y determinan la forma de la distribución de Chi-cuadrado. Se calculan como:\n\n\\[\n  \\text{k} = (\\text{número de filas} - 1) \\times (\\text{número de columnas} - 1)\n\\]\nEn este caso, tenemos 3 niveles educativos y 2 tipos de empleo, por lo que:\n\\[\n  \\text{k} = (3-1) \\times (2-1) = 2\n\\]\n\nEl p-valor de 0.00585 es menor que 0.05. Por lo tanto, rechazamos la hipótesis nula y concluimos que el nivel educativo y el tipo de empleo no son independientes. Es decir, el hecho de que una persona tenga empleo formal o informal en nuestra población de estudio es altamente improbable que sea debido al azar, sino que está influenciado por su nivel educativo. En an análisis descriptivos observamos que en los niveles medio y superior, hay una mayor proporción de empleo formal en comparación con el nivel básico, lo que nos indica que a mayor nivel educativo, mayor es la probabilidad de acceder a un empleo formal.\n\n\n\nComparar múltiples grupos requiere atención al detalle",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comparando grupos</span>"
    ]
  },
  {
    "objectID": "comp.html#supuestos",
    "href": "comp.html#supuestos",
    "title": "7  Comparando grupos",
    "section": "7.5 Supuestos",
    "text": "7.5 Supuestos\nLos supuestos de estos tipos de pruebas guardan estrecha similitud con las pruebas para una muestra o proporción, con algunos ajustes necesarios:\nMuestreo Aleatorio e Independencia de las Observaciones\nEn la comparación de grupos, el supuesto de muestreo aleatorio y la independencia de las observaciones sigue siendo importante. Es necesario garantizar que las observaciones dentro de cada grupo sean independientes, lo que significa que el valor de una observación no debe influir en el valor de otra dentro del mismo grupo. Además, debe asegurarse que los grupos sean independientes entre sí, es decir, que no haya influencia o relación directa entre las muestras de los grupos comparados. En el ejemplo del estudio de zonas rurales y urbanas, es importante verificar que los participantes de una zona no pertenezcan o tengan algún tipo de relación con la otra (por ejemplo, alguien que vive en zona rural pero trabaja en zona urbana). La falta de independencia puede sesgar los resultados y hacer que las inferencias no sean confiables.\nTamaño Suficiente de la Muestra\nEl tamaño de la muestra es un factor esencial y en este caso, deben ser suficiente en cada grupo individual. El Teorema del Límite Central (TLC) garantiza que las distribuciones muestrales se aproximen a una normalidad cuando el tamaño de la muestra sea grande (\\(n &gt; 30\\)), pero este criterio debe cumplirse para cada grupo. Si uno de los grupos tiene un tamaño de muestra pequeño, los resultados pueden ser menos confiables.\nNormalidad de la Distribución\nEn la comparación de grupos, este supuesto debe cumplirse no solo para cada grupo individual, sino también para la distribución de las diferencias entre los grupos. Si los tamaños de muestra son pequeños (\\(n \\leq 30\\)), es necesario verificar explícitamente la normalidad de los datos en cada grupo.\nHomogeneidad de Varianzas\nAdicionalmente, tenemos un supuesto específico de la comparación de grupo. La homogeneidad de las varianzas. Este supuesto establece que la variabilidad dentro de cada grupo debe ser similar. Si las varianzas son significativamente diferentes, el grupo con mayor variabilidad tendrá un impacto desproporcionado en el cálculo del estadístico. Las pruebas que asumen homogeneidad pueden arrojar resultados sesgados.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comparando grupos</span>"
    ]
  },
  {
    "objectID": "comp.html#pruebas-no-paramétricas",
    "href": "comp.html#pruebas-no-paramétricas",
    "title": "7  Comparando grupos",
    "section": "7.6 Pruebas no paramétricas",
    "text": "7.6 Pruebas no paramétricas\nYa vimos que cuando la prueba de normalidad no se cumple o cuando las varianzas son muy desiguales, las pruebas paramétricas como el t-test, el z-test de proporciones o el ANOVA pueden generar resultados poco confiables. En estos casos, podemos recurrir a pruebas no paramétricas, que son más flexibles y robustas frente a distribuciones desconocidas, datos asimétricos o la presencia de valores atípicos.\nLas pruebas no paramétricas no requieren que los datos sigan una distribución normal, y en lugar de trabajar directamente con los valores originales, suelen basarse en rangos, conteos o permutaciones para evaluar diferencias entre grupos o muestras. Estas pruebas son especialmente útiles en estudios con muestras pequeñas, donde la normalidad no puede asumirse con certeza.\nAquí las principales alternativas no paramétricas para el análisis de medias y proporciones, incluyendo una alternativa para ANOVA, que nos permitirá comparar más de dos grupos sin depender de la normalidad de los datos.\n1. Medias en una Muestra: Prueba de Wilcoxon Signed-Rank\nLa prueba de Wilcoxon Signed-Rank se usa como alternativa a la prueba \\(t\\)-test para una muestra cuando no se cumple la normalidad.\n\nmuestra = read_csv('alturas.csv')\n\n\nwilcoxon = wilcox.test(muestra$alturas, \n            mu = 170, \n            alternative = \"two.sided\")\n\ntidy(wilcoxon)\n\n# A tibble: 1 × 4\n  statistic p.value method                                           alternative\n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;chr&gt;      \n1     30851   0.942 Wilcoxon signed rank test with continuity corre… two.sided  \n\n\n2. Comparación de Medias entre Dos Grupos: Prueba de Mann-Whitney U (Wilcoxon Rank-Sum Test)\nLa prueba de Mann-Whitney U se usa como alternativa al \\(t\\)-test para muestras independientes cuando no se cumple la normalidad o la homogeneidad de varianzas.\n\nwilcoxon_2 = wilcox.test(horas_ocio ~ genero, \n                  data = ocio)\n\ntidy(wilcoxon_2)\n\n# A tibble: 1 × 4\n  statistic  p.value method                                          alternative\n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                                           &lt;chr&gt;      \n1      8563 2.08e-18 Wilcoxon rank sum test with continuity correct… two.sided  \n\n\n3. Comparación de Medias entre Más de Dos Grupos: Prueba de Kruskal-Wallis\nCuando queremos comparar más de dos grupos y el ANOVA no es adecuado porque la normalidad o la homogeneidad de varianzas no se cumplen, usamos la prueba de Kruskal-Wallis. Esta prueba no paramétrica evalúa si hay diferencias significativas en la distribución de los grupos, basándose en rangos en lugar de valores absolutos.\n\nkruskal = kruskal.test(nivel_satisfaccion ~ sector, data = satisfaccion)\n\ntidy(kruskal)\n\n# A tibble: 1 × 4\n  statistic    p.value parameter method                      \n      &lt;dbl&gt;      &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                       \n1      30.1 0.00000478         4 Kruskal-Wallis rank sum test\n\n\nAl igual que el ANOVA, Kruskal-Wallis solo nos dice si hay diferencias entre los grupos, pero no nos indica cuáles son diferentes entre sí. Para ello, realizamos una comparación post-hoc, como la prueba de Dunn, que es equivalente a la prueba de Tukey en ANOVA.\n\nlibrary(FSA)  # Para la prueba de Dunn\ndunnTest(nivel_satisfaccion ~ sector, data = satisfaccion, method = \"bonferroni\")\n\n                 Comparison          Z      P.unadj        P.adj\n1      Educación - Finanzas  1.2193784 2.227006e-01 1.000000e+00\n2   Educación - Manufactura  3.6058333 3.111528e-04 3.111528e-03\n3    Finanzas - Manufactura  2.3864549 1.701169e-02 1.701169e-01\n4         Educación - Salud -1.5720417 1.159409e-01 1.000000e+00\n5          Finanzas - Salud -2.7914201 5.247732e-03 5.247732e-02\n6       Manufactura - Salud -5.1778750 2.244275e-07 2.244275e-06\n7    Educación - Tecnología -0.1300073 8.965607e-01 1.000000e+00\n8     Finanzas - Tecnología -1.3493856 1.772131e-01 1.000000e+00\n9  Manufactura - Tecnología -3.7358405 1.870892e-04 1.870892e-03\n10       Salud - Tecnología  1.4420344 1.492927e-01 1.000000e+00\n\n\n4. Proporción en una Muestra: Prueba Binomial\nCuando analizamos una proporción en una muestra, la prueba binomial se usa como alternativa al \\(z\\)-test de proporciones.\n\napoyo = read_csv('apoyo.csv')\n\n\nbinom = binom.test(210, \n           350, \n           p = 0.6, \n           conf.level = 0.95)\n\ntidy(binom)\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      0.6       210       1       350    0.547     0.652 Exact bin… two.sided  \n\n\n5. Comparación de Proporciones entre Dos Grupos: Prueba de Fisher o Chi-Cuadrado\nCuando se comparan proporciones en dos grupos, las pruebas de Fisher o Chi-cuadrado son alternativas al \\(z\\)-test de proporciones.\nQueremos comparar la proporción de apoyo entre zonas urbanas y rurales.\n\n# Prueba de Fisher\ntidy(\n  fisher.test(tabla)\n  )\n\n# A tibble: 1 × 6\n  estimate    p.value conf.low conf.high method                      alternative\n     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                       &lt;chr&gt;      \n1     2.72 0.00000910     1.71      4.35 Fisher's Exact Test for Co… two.sided  \n\n# Prueba de Chi-cuadrado\ntidy(\n  chisq.test(tabla)\n)\n\n# A tibble: 1 × 4\n  statistic   p.value parameter method                                          \n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                           \n1      19.4 0.0000109         1 Pearson's Chi-squared test with Yates' continui…\n\n\n\n\n\n\n\n\n\n\nSituación\nPrueba Paramétrica\nAlternativa No Paramétrica\n\n\n\n\nMedia en una muestra\nt-test para una muestra\nWilcoxon Signed-Rank Test\n\n\nMedias entre dos grupos\nt-test para muestras independientes\nMann-Whitney U (Wilcoxon Rank-Sum)\n\n\nMedias entre más de dos grupos\nANOVA\nKruskal-Wallis Test + Prueba de Dunn\n\n\nProporción en una muestra\nz-test de proporciones\nPrueba Binomial\n\n\nProporciones entre grupos\nz-test de diferencia de proporciones\nFisher o Chi-Cuadrado",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comparando grupos</span>"
    ]
  },
  {
    "objectID": "comp.html#remuestreo-e-inferencia-con-el-paquete-infer",
    "href": "comp.html#remuestreo-e-inferencia-con-el-paquete-infer",
    "title": "7  Comparando grupos",
    "section": "7.7 Remuestreo e inferencia con el paquete infer",
    "text": "7.7 Remuestreo e inferencia con el paquete infer\nHasta ahora, hemos trabajado con pruebas paramétricas como la prueba t, prueba z o ANOVA, todas basadas en distribuciones teóricas como la normal, t de Student o F. Estas pruebas son herramientas muy útiles y prácticas, pero como ya sabemos, dependen de determinados supuestos (normalidad, independencia, tamaño de muestra suficiente). Cuando estos supuestos no se cumplen, sus resultados pueden volverse poco confiables.\nAdemás de las pruebas no paramétricas, ¿existe otra forma de estimar parámetros y evaluar la significancia estadística sin depender de distribuciones teóricas? Sabemos que, gracias al Teorema del Límite Central (TLC), los estadísticos de muestra tienden a seguir ciertas distribuciones, pero ¿qué pasaría si, en lugar de asumirlo, nosotros mismos generamos esas distribuciones a partir de nuestros datos? Es decir, en vez de basarnos en modelos teóricos predefinidos, podríamos replicar el proceso de muestreo una y otra vez y observar empíricamente cómo se comportan los estadísticos. Esto es justamente lo que hacen los métodos basados en simulaciones .\nEn lugar de suponer una forma específica para la distribución de los datos, estos métodos generan una distribución empírica a partir de los datos observados mediante un proceso conocido como re-muestreo (Çetinkaya-Rundel y Hardin 2021). En este enfoque, en lugar de confiar en distribuciones teóricas como la normal, generamos un número elevado de muestras simuladas y construimos nuestra propia distribución a partir de los datos. Esto nos permite calcular probabilidades, intervalos de confianza y contrastes de hipótesis sin depender estrictamente de los supuestos. El el paquete infer (Couch et al. 2021) existen 3 diferentes formas de aplicar este enfoque, cada una con una utilidad particular: bootstrap, permutación y simulación binomial (draw).\nBootstrap: Estimación sin supuestos de normalidad\nEl método bootstrap nos permite estimar la variabilidad de un estadístico de interés (media, mediana, proporción, etc.) a partir de múltiples muestras obtenidas con reemplazo de los datos originales. La idea es simple: si nuestra muestra es representativa de la población, podemos usarla para generar nuevas muestras y calcular el estadístico de interés muchas veces, obteniendo así una distribución empírica de sus valores.\nEl bootstrap es particularmente útil para construir intervalos de confianza sin asumir normalidad, lo que lo hace ideal cuando el tamaño muestral es pequeño o cuando la distribución de los datos es desconocida. En términos simples, en lugar de confiar en la teoría para estimar la variabilidad de nuestros datos, usamos los propios datos para generar la información que necesitamos.\nPermutación: Pruebas de hipótesis sin distribuciones teóricas\nEl método de permutación se usa en pruebas de hipótesis cuando queremos evaluar si dos grupos son realmente diferentes o si la diferencia observada es producto del azar. Para ello, redistribuimos aleatoriamente los valores observados entre los grupos y calculamos la diferencia esperada en cada reordenamiento. Esto nos permite generar una distribución nula, es decir, una distribución de diferencias bajo la suposición de que no hay efecto real.\nSi la diferencia observada en nuestros datos es mucho mayor que las diferencias obtenidas en las permutaciones aleatorias, entonces podemos concluir que es poco probable que se deba al azar, lo que nos da evidencia para rechazar la hipótesis nula.\nDraw: Simulación de pruebas para proporciones\nCuando queremos evaluar hipótesis sobre una proporción, el método draw nos permite generar una distribución empírica basada en simulaciones de datos bajo la hipótesis nula. En este caso, en lugar de reorganizar los datos existentes (como en la permutación), generamos nuevos valores aleatorios siguiendo un modelo binomial que representa el escenario nulo.\nPara poder relizar todo esot haremos uso del paquete infer. infer son da una seride de herrmientas y metodología que facilita la implementación de estos métodos sin necesidad de escribir código complejo. Nos permite realizar pruebas estadísticas. El análisis con infer sigue un proceso estructurado:\n\nDefinición del problema: Con la función specify(), se establece qué variable o relación entre variables se analizará. Esto puede incluir, por ejemplo, la evaluación de una media poblacional (\\(\\mu\\)), una proporción (\\(p\\)) o una diferencia entre grupos (\\(\\mu\\_1 - \\mu\\_2\\)).\nPlanteamiento de la hipótesis nula: En caso estemos haciendo una prueba de significancia, la función hypothesize() formaliza la hipótesis nula (\\(H_0\\)). En el caso de una prueba de una muestra, \\(H_0\\) puede establecer que la media de la población es igual a un valor dado (\\(H_0: \\mu = \\mu_0\\)). En una prueba de dos grupos, \\(H_0\\) podría declarar que no hay diferencia en las medias entre ellos (\\(H_0: \\mu_1 = \\mu_2\\)). Este argumento se utiliza si se desea hacer un contraste de hipótesis.\nSimulación de datos bajo (H0): Con la función generate(), se generan datos simulados que reflejan el escenario donde \\(H_0\\) es verdadera. Para pruebas de hipótesis, esto implica generar una distribución nula a través de permutaciones de los datos observados. Para estimación, el bootstrap genera múltiples muestras con reemplazo a partir de los datos originales. Este argumento es el que genera las múltiples muestras.\nCálculo de la estadística de interés: Finalmente, la función calculate() evalúa una estadística específica (como una media, proporción, diferencia, o estadístico t o z) en las distribuciones simuladas. Esto permite comparar el valor observado en los datos con la distribución nula para calcular un p-valor o construir intervalos de confianza.\n\nUtilizaremos los ejemplos que llevamos usando estos dos capítulos:\n\nlibrary(infer)\n\n\n7.7.1 Estimación y contraste de hipótesis para una media\n\nmuestra = read_csv('alturas.csv')\n\nPara estimar un intervalo de confianza del 95% para la media de la población, utilizamos la técnica de bootstrap\n\ndist_bootstrap_media = muestra %&gt;%\n  specify(response = alturas) %&gt;%       # Definimos la variable de interés\n  generate(type = \"bootstrap\", reps = 10000) %&gt;%  # Generamos muestras bootstrap\n  calculate(stat = \"mean\")              # Calculamos la media en cada muestra\n\nAhora, obtenemos los percentiles 2.5% y 97.5% de la distribución que acabamos de generar usando pull()\n\nic_bootstrap = dist_bootstrap_media %&gt;%\n  summarise(\n    lower = quantile(stat, 0.025),  # Límite inferior (percentil 2.5%)\n    upper = quantile(stat, 0.975)   # Límite superior (percentil 97.5%)\n  )\n\nic_lower = ic_bootstrap %&gt;% pull(lower)\nic_upper = ic_bootstrap %&gt;% pull(upper)\nmedia_observada = mean(muestra$alturas)\n\nic_lower\n\n    2.5% \n169.3135 \n\nic_upper\n\n   97.5% \n171.2978 \n\nmedia_observada\n\n[1] 170.2981\n\n\nEl intervalo de confianza del 95% obtenido mediante bootstrap es:\n\\[\nIC_{95\\%} = [169.28, 171.32]\n\\]\nEsto significa que, con un 95% de confianza, podemos afirmar que la media poblacional \\(\\mu\\) se encuentra dentro de este rango.\nLo podemos ver gráficamente:\n\n# Graficar la distribución bootstrap con el IC\ng_bootstrap_ic = dist_bootstrap_media %&gt;%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.1, \n                 fill = \"lightblue\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  geom_vline(xintercept = ic_bootstrap$lower, \n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = ic_bootstrap$upper, \n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = mean(muestra$alturas), \n             color = \"darkgreen\", \n             linetype = \"solid\", \n             size = 1) +\n  labs(title = \"Distribución bootstrap de la media\",\n       subtitle = \"Líneas rojas: Intervalo de confianza (95%) | Línea verde: Media observada\",\n       x = \"Media Bootstrap\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_bootstrap_ic\n\n\n\n\n\n\n\n\nAhora, queremos investigar si la altura promedio (\\(\\mu\\)) de una población es diferente de 170 cm. Bajo la hipótesis nula (\\(H_0\\)), suponemos que la media poblacional es exactamente 170 (\\(H_0: \\mu = 170\\)). En infer, seguimos estos pasos:\nGeneramos la distribución nula\n\ndist_nula_media = muestra %&gt;%\n  specify(response = alturas) %&gt;%                  # Definimos la variable de interés\n  hypothesize(null = \"point\", mu = 170) %&gt;%        # Hipótesis nula: media = 170\n  generate(type = \"bootstrap\", reps = 10000) %&gt;%   # Generamos muestras bootstrap\n  calculate(stat = \"mean\")                         # Calculamos la media\n\nLo podemos ver gráficamente\n\ng_dist_nula = dist_nula_media %&gt;% \n  ggplot(aes(x = stat, )) +\n  geom_histogram(fill = \"lightgray\", color = 'black') +\n  labs(title = 'Distribución nula de la media (reps = 10000)') +\n  theme_minimal()\n\ng_dist_nula\n\n\n\n\n\n\n\n\nCalcular la media observada\n\nmedia_observada = mean(muestra$alturas)\n\nmedia_observada\n\n[1] 170.2981\n\n\nCalcular el p-valor\n\np_valor_media = dist_nula_media %&gt;%\n  summarise(\n    p_value = mean(abs(stat - 170) &gt;= abs(media_observada - 170))\n  )\n\np_valor_media\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.561\n\n\nVisualmente\n\n# Graficar la distribución nula con el valor observado\ng_dist_nula = dist_nula_media %&gt;% \n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.1, \n                 fill = \"lightgray\", \n                 color = \"black\") +\n  geom_vline(xintercept = mean(muestra$alturas), \n             color = \"red\", \n             linetype = \"solid\", \n             size = 1) +\n  geom_vline(xintercept = 170, \n             color = \"blue\", \n             linetype = \"dashed\", \n             size = 1) +\n  labs(title = \"Distribución nula de la media\",\n       subtitle = \"Línea azul: Media esperada bajo H0 (170 cm) | Línea roja: Media observada\",\n       x = \"Media Bootstrap\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_dist_nula\n\n\n\n\n\n\n\n\nGraficamos la distribución nula con las zonas de rechazo\n\n# Definimos los límites de la zona de rechazo\nzona_rechazo = dist_nula_media %&gt;%\n  summarise(\n    lower = quantile(stat, 0.025),\n    upper = quantile(stat, 0.975)\n  )\n\nzona_rechazo\n\n# A tibble: 1 × 2\n  lower upper\n  &lt;dbl&gt; &lt;dbl&gt;\n1  169.  171.\n\n\n\\[\n\\text{Zona de Rechazo} = \\left[169, 171\\right]\n\\]\n\ng_dist_nula_rechazo = dist_nula_media %&gt;% \n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.1, \n                 fill = \"lightgray\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  geom_vline(xintercept = mean(muestra$alturas), \n             color = \"red\", \n             linetype = \"solid\", \n             size = 1) +\n  geom_vline(xintercept = 170, \n             color = \"blue\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = zona_rechazo$lower, \n             color = \"purple\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = zona_rechazo$upper, \n             color = \"purple\", \n             linetype = \"dashed\", \n             size = 1) +\n  labs(title = \"Distribución nula con la zona de rechazo\",\n  subtitle = \"Líneas púrpuras: Zona de rechazo (IC 95%)|Línea roja: Media observada\",\n       x = \"Media Bootstrap\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_dist_nula_rechazo\n\n\n\n\n\n\n\n\n\n\n7.7.2 Estimación y contraste de hipótesis para una proporción\n\napoyo = read_csv('apoyo.csv')\n\nQueremos estimar un intervalo de confianza del 95% para la proporción de apoyo al Partido A en la población. Para ello, utilizamos la técnica de bootstrap, generando múltiples muestras a partir de los datos observados.\n\ndist_bootstrap_prop = apoyo %&gt;%\n  specify(response = apoyo, success = \"A\") %&gt;%   # Definimos la variable de interés\n  generate(type = \"bootstrap\", reps = 10000) %&gt;%  # Generamos muestras bootstrap\n  calculate(stat = \"prop\")                        # Calculamos la proporción en cada muestra\n\nExtraemos los datos\n\nic_bootstrap = dist_bootstrap_prop %&gt;%\n  summarise(\n    lower = quantile(stat, 0.025),  # Límite inferior (percentil 2.5%)\n    upper = quantile(stat, 0.975)   # Límite superior (percentil 97.5%)\n  )\n\nic_lower = ic_bootstrap %&gt;% pull(lower)\nic_upper = ic_bootstrap %&gt;% pull(upper)\nprop_observada = mean(apoyo$apoyo == \"A\")\n\nic_lower\n\n     2.5% \n0.5485714 \n\nic_upper\n\n    97.5% \n0.6514286 \n\nprop_observada\n\n[1] 0.6\n\n\n\\[\nIC_{95\\%} = [0.548, 0.651]\n\\] Visualmente\n\ng_bootstrap_ic_prop = dist_bootstrap_prop %&gt;%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.01, \n                 fill = \"lightblue\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  geom_vline(xintercept = ic_lower, \n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = ic_upper, \n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = prop_observada, \n             color = \"darkgreen\", \n             linetype = \"solid\", \n             size = 1) +\n  labs(title = \"Distribución bootstrap de la proporción\",\n       subtitle = \"Líneas rojas: Intervalo de confianza (95%) | Línea verde: Proporción observada\",\n       x = \"Proporción Bootstrap\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_bootstrap_ic_prop\n\n\n\n\n\n\n\n\nAhora, queremos determinar si la proporción de apoyo al partido A (\\(p\\)) es diferente de 50%. Bajo la hipótesis nula (\\(H_0\\)), asumimos que la proporción poblacional es exactamente 0.5 (\\(H_0: p = 0.5\\)). En infer, seguimos estos pasos:\nGeneramos la distribución nula factorizando la variable apoyo previamente:\n\napoyo$apoyo = factor(apoyo$apoyo) \n\nAl ser una proporción, generamos la hipoteiss nula usando draw\n\ndist_nula_prop = apoyo %&gt;%\n  specify(response = apoyo, success = \"A\") %&gt;%    # Definimos la variable de éxito\n  hypothesize(null = \"point\", p = 0.5) %&gt;%        # Hipótesis nula: proporción = 0.6\n  generate(type = \"draw\", reps = 10000) %&gt;%       # Generamos la distribución nula\n  calculate(stat = \"prop\")                        # Calculamos la proporción\n\nVisualizamos la distribución nula:\n\ng_dist_nula_prop = dist_nula_prop %&gt;% \n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.01, \n                 fill = \"lightgray\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  labs(title = \"Distribución nula de la proporción\",\n       subtitle = \"Generada mediante simulaciones (reps = 10,000)\",\n       x = \"Proporción Simulada\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_dist_nula_prop\n\n\n\n\n\n\n\n\nCalculamos la proporción observada:\n\nprop_observada = mean(apoyo$apoyo == \"sí\")\n\nprop_observada\n\n[1] 0\n\n\nCalculamos el p-valor:\n\np_valor_prop = dist_nula_prop %&gt;%\n  summarise(\n    p_value = mean(abs(stat - 0.6) &gt;= abs(prop_observada - 0.6))\n  )\n\np_valor_prop\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nVisualizamos\n\nzona_rechazo = dist_nula_prop %&gt;%\n  summarise(\n    lower = quantile(stat, 0.025), \n    upper = quantile(stat, 0.975)\n  )\n\nzona_rechazo\n\n# A tibble: 1 × 2\n  lower upper\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.449 0.551\n\n\n\\[\n\\text{Zona de Rechazo} = \\left[0.449, 0.554\\right]\n\\]\n\ng_dist_nula_rechazo_prop = dist_nula_prop %&gt;% \n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.01, \n                 fill = \"lightgray\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  geom_vline(xintercept = prop_observada, \n             color = \"red\", \n             linetype = \"solid\", \n             size = 1) +\n  geom_vline(xintercept = zona_rechazo$lower, \n             color = \"purple\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = zona_rechazo$upper, \n             color = \"purple\", \n             linetype = \"dashed\", \n             size = 1) +\n  labs(title = \"Distribución Nula con Zona de Rechazo\",\n  subtitle = \"Líneas púrpuras: Zona de rechazo (IC 95%)|Línea roja: Proporción observada\",\n       x = \"Proporción Simulada\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_dist_nula_rechazo_prop\n\n\n\n\n\n\n\n\n\n\n7.7.3 Estimación y contraste de hipótesis de medias entre dos grupos\nQueremos estimar un intervalo de confianza del 95% para la diferencia de medias en las horas de ocio entre hombres y mujeres. Usamos bootstrap\n\ndist_bootstrap_dif_media = ocio %&gt;%\n  specify(horas_ocio ~ genero) %&gt;%                # Definimos la relación entre variables\n  generate(type = \"bootstrap\", reps = 10000) %&gt;%  # Generamos muestras bootstrap\n  calculate(stat = \"diff in means\", \n            order = c(\"hombre\", \"mujer\"))  # Calculamos la diferencia en medias\n\nAhora, extraemos los percentiles 2.5% y 97.5% para construir el intervalo de confianza:\n\nic_bootstrap_dif_media = dist_bootstrap_dif_media %&gt;%\n  summarise(\n    lower = quantile(stat, 0.025),  # Límite inferior\n    upper = quantile(stat, 0.975)   # Límite superior\n  )\n\nic_lower = ic_bootstrap_dif_media %&gt;% pull(lower)\nic_upper = ic_bootstrap_dif_media %&gt;% pull(upper)\ndif_media_observada = mean(ocio$horas_ocio[ocio$genero == \"hombre\"]) - \n                      mean(ocio$horas_ocio[ocio$genero == \"mujer\"])\n\nic_lower\n\n   2.5% \n2.95086 \n\nic_upper\n\n   97.5% \n4.254522 \n\ndif_media_observada\n\n[1] 3.6\n\n\nEl intervalo de confianza del 95% obtenido mediante bootstrap es:\n\\[\nIC_{95\\%} = [2.94, 4.27]\n\\]\nVisualizamos la distribución bootstrap con el intervalo de confianza:\n\ng_bootstrap_ic_dif_media = dist_bootstrap_dif_media %&gt;%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.1, \n                 fill = \"lightblue\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  geom_vline(xintercept = ic_lower, \n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = ic_upper, \n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = dif_media_observada, \n             color = \"darkgreen\", \n             linetype = \"solid\", \n             size = 1) +\n  labs(title = \"Distribución bootstrap de la diferencia de medias\",\n  subtitle = \"Líneas rojas: Intervalo de confianza (95%)|Línea verde: Diferencia observada\",\n       x = \"Diferencia de Medias Bootstrap\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_bootstrap_ic_dif_media\n\n\n\n\n\n\n\n\nAhora, queremos evaluar si la diferencia de medias observada es lo suficientemente grande como para considerarla estadísticamente significativa. Bajo la hipótesis nula (\\(H_0\\)), asumimos que no hay diferencia entre las medias (\\(H_0: \\mu_1 = \\mu_2\\)). En infer, seguimos estos pasos:\nGeneramos la distribución nula usando permutaciones, ya que estamos evaluando la independencia entre las variables:\n\ndist_nula_dif_media = ocio %&gt;%\n  specify(horas_ocio ~ genero) %&gt;%                # Relación entre variables\n  hypothesize(null = \"independence\") %&gt;%          # Hipótesis nula: independencia\n  generate(type = \"permute\", reps = 10000) %&gt;%    # Generamos permutaciones\n  calculate(stat = \"diff in means\", \n            order = c(\"hombre\", \"mujer\"))  # Calculamos la diferencia en medias\n\nVisualizamos la distribución nula:\n\ng_dist_nula_dif_media = dist_nula_dif_media %&gt;% \n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.1, \n                 fill = \"lightgray\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  labs(title = \"Distribución Nula de la diferencia de medias\",\n       subtitle = \"Generada mediante permutaciones (reps = 10,000)\",\n       x = \"Diferencia Simulada\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_dist_nula_dif_media\n\n\n\n\n\n\n\n\nCalculamos la diferencia de medias observada:\n\ndif_media_obs = mean(ocio$horas_ocio[ocio$genero == \"hombre\"]) - \n                mean(ocio$horas_ocio[ocio$genero == \"mujer\"])\n\ndif_media_obs\n\n[1] 3.6\n\n\nCalculamos el p-valor:\n\np_valor_dif_media = dist_nula_dif_media %&gt;%\n  summarise(\n    p_value = mean(abs(stat) &gt;= abs(dif_media_obs))\n  )\n\np_valor_dif_media\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nAhora, definimos la zona de rechazo, que representa los valores extremos de la distribución nula:\n\nzona_rechazo = dist_nula_dif_media %&gt;%\n  summarise(\n    lower = quantile(stat, 0.025), \n    upper = quantile(stat, 0.975)\n  )\n\nzona_rechazo\n\n# A tibble: 1 × 2\n  lower upper\n  &lt;dbl&gt; &lt;dbl&gt;\n1 -0.84  0.82\n\n\n\\[\n\\text{Zona de Rechazo} = \\left[-0.84, 0.84\\right]\n\\]\nFinalmente, graficamos la distribución nula con la zona de rechazo y la diferencia observada:\n\ng_dist_nula_rechazo_dif_media = dist_nula_dif_media %&gt;% \n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.1, \n                 fill = \"lightgray\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  geom_vline(xintercept = dif_media_obs, \n             color = \"red\", \n             linetype = \"solid\", \n             size = 1) +\n  geom_vline(xintercept = zona_rechazo$lower, \n             color = \"purple\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = zona_rechazo$upper, \n             color = \"purple\", \n             linetype = \"dashed\", \n             size = 1) +\n  labs(title = \"Distribución Nula con Zona de Rechazo\",\n       subtitle = \"Líneas púrpuras: Zona de rechazo (IC 95%) | Línea roja: Diferencia observada\",\n       x = \"Diferencia Simulada\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_dist_nula_rechazo_dif_media\n\n\n\n\n\n\n\n\n\n\n7.7.4 Estimación y contraste de hipótesis de proporción entre dos grupos\nQueremos estimar un intervalo de confianza del 95% para la diferencia de proporciones en el apoyo al Partido A entre votantes de zonas urbanas y rurales. Usamos bootstrap\n\ndist_bootstrap_dif_prop = votantes %&gt;%\n  specify(apoyo ~ zona, success = \"sí\") %&gt;%       # Definimos la relación entre variables\n  generate(type = \"bootstrap\", reps = 10000) %&gt;%  # Generamos muestras bootstrap\n  calculate(stat = \"diff in props\", \n            order = c(\"urbana\", \"rural\"))         # Calculamos la diferencia en proporciones\n\nAhora, extraemos los percentiles 2.5% y 97.5% para construir el intervalo de confianza:\n\nic_bootstrap_dif_prop = dist_bootstrap_dif_prop %&gt;%\n  summarise(\n    lower = quantile(stat, 0.025),  # Límite inferior\n    upper = quantile(stat, 0.975)   # Límite superior\n  )\n\nic_lower = ic_bootstrap_dif_prop %&gt;% pull(lower)\nic_upper = ic_bootstrap_dif_prop %&gt;% pull(upper)\ndif_prop_observada = mean(votantes$apoyo[votantes$zona == \"urbana\"] == \"sí\") - \n                     mean(votantes$apoyo[votantes$zona == \"rural\"] == \"sí\")\n\nic_lower\n\n     2.5% \n0.1375324 \n\nic_upper\n\n   97.5% \n0.340112 \n\ndif_prop_observada\n\n[1] 0.2383333\n\n\nEl intervalo de confianza del 95% obtenido mediante bootstrap es:\n\\[\nIC_{95\\%} = [0.13, 0.34]\n\\]\nVisualizamos la distribución bootstrap con el intervalo de confianza:\n\ng_bootstrap_ic_dif_prop = dist_bootstrap_dif_prop %&gt;%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.01, \n                 fill = \"lightblue\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  geom_vline(xintercept = ic_lower, \n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = ic_upper, \n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = dif_prop_observada, \n             color = \"darkgreen\", \n             linetype = \"solid\", \n             size = 1) +\n  labs(title = \"Distribución bootstrap de la diferencia de proporciones\",\n       subtitle = \"Líneas rojas: Intervalo de confianza (95%) | Línea verde: Diferencia observada\",\n       x = \"Diferencia de Proporciones Bootstrap\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_bootstrap_ic_dif_prop\n\n\n\n\n\n\n\n\nAhora, queremos evaluar si la diferencia de proporciones observada es lo suficientemente grande como para considerarla estadísticamente significativa. Bajo la hipótesis nula (\\(H_0\\)), asumimos que no hay diferencia entre las proporciones (\\(H_0: p_1 = p_2\\)). En infer, seguimos estos pasos:\nGeneramos la distribución nula usando permutaciones, ya que estamos evaluando la independencia entre las variables:\n\ndist_nula_dif_prop = votantes %&gt;%\n  specify(apoyo ~ zona, success = \"sí\") %&gt;%       # Relación entre variables\n  hypothesize(null = \"independence\") %&gt;%          # Hipótesis nula: independencia\n  generate(type = \"permute\", reps = 10000) %&gt;%    # Generamos permutaciones\n  calculate(stat = \"diff in props\", \n            order = c(\"urbana\", \"rural\"))         # Calculamos la diferencia en proporciones\n\nVisualizamos la distribución nula:\n\ng_dist_nula_dif_prop = dist_nula_dif_prop %&gt;% \n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.015, \n                 fill = \"lightgray\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  labs(title = \"Distribución Nula de la diferencia de proporciones\",\n       subtitle = \"Generada mediante permutaciones (reps = 10,000)\",\n       x = \"Diferencia Simulada\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_dist_nula_dif_prop\n\n\n\n\n\n\n\n\nCalculamos la diferencia de proporciones observada:\n\ndif_prop_obs = mean(votantes$apoyo[votantes$zona == \"urbana\"] == \"sí\") - \n               mean(votantes$apoyo[votantes$zona == \"rural\"] == \"sí\")\n\ndif_prop_obs\n\n[1] 0.2383333\n\n\nCalculamos el p-valor:\n\np_valor_dif_prop = dist_nula_dif_prop %&gt;%\n  summarise(\n    p_value = mean(abs(stat) &gt;= abs(dif_prop_obs))\n  )\n\np_valor_dif_prop\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nAhora, definimos la zona de rechazo, que representa los valores extremos de la distribución nula:\n\nzona_rechazo = dist_nula_dif_prop %&gt;%\n  summarise(\n    lower = quantile(stat, 0.025), \n    upper = quantile(stat, 0.975)\n  )\n\nzona_rechazo\n\n# A tibble: 1 × 2\n  lower  upper\n  &lt;dbl&gt;  &lt;dbl&gt;\n1  -0.1 0.0983\n\n\n\\[\n\\text{Zona de Rechazo} = \\left[-0.1, 0.09\\right]\n\\]\nFinalmente, graficamos la distribución nula con la zona de rechazo y la diferencia observada:\n\ng_dist_nula_rechazo_dif_prop = dist_nula_dif_prop %&gt;% \n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.015, \n                 fill = \"lightgray\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  geom_vline(xintercept = dif_prop_obs, \n             color = \"red\", \n             linetype = \"solid\", \n             size = 1) +\n  geom_vline(xintercept = zona_rechazo$lower, \n             color = \"purple\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(xintercept = zona_rechazo$upper, \n             color = \"purple\", \n             linetype = \"dashed\", \n             size = 1) +\n  labs(title = \"Distribución Nula con Zona de Rechazo\",\n       subtitle = \"Líneas púrpuras: Zona de rechazo (IC 95%) | Línea roja: Diferencia observada\",\n       x = \"Diferencia Simulada\",\n       y = \"Frecuencia\") +\n  theme_minimal()\n\ng_dist_nula_rechazo_dif_prop",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comparando grupos</span>"
    ]
  },
  {
    "objectID": "comp.html#resumen-del-capítulo",
    "href": "comp.html#resumen-del-capítulo",
    "title": "7  Comparando grupos",
    "section": "7.8 Resumen del capítulo",
    "text": "7.8 Resumen del capítulo\nEn las ciencias sociales, la comparación entre grupos permite identificar patrones y diferencias en fenómenos de interés. Para ello, se analizan variables cuantitativas (como medias) y cualitativas (como proporciones), siempre considerando si la relación observada es significativa o producto del azar.\nCuando se comparan dos grupos, se diferencian la variable explicativa (criterio de clasificación) y la variable respuesta (lo que se mide). Dependiendo del tipo de variable respuesta, se aplican distintas herramientas estadísticas.\nEn el caso de comparación de medias, se utilizan intervalos de confianza y pruebas t para evaluar diferencias significativas. La estimación del error estándar permite cuantificar la variabilidad en la diferencia de medias, y el contraste de hipótesis determina si esta diferencia es estadísticamente relevante.\nPara comparaciones de proporciones, se emplean pruebas z, intervalos de confianza y pruebas de Chi-cuadrado. Se verifica si la distribución de proporciones entre grupos varía de manera significativa, permitiendo determinar si una diferencia observada es real o producto del azar.\nCuando se analizan grupos relacionados (como mediciones antes y después en los mismos individuos), se utilizan pruebas pareadas como la prueba t para muestras dependientes, que permite evaluar si un cambio dentro del mismo grupo es estadísticamente significativo.\nPara comparaciones entre múltiples grupos, se aplica ANOVA, que analiza si existen diferencias entre tres o más grupos en términos de medias. Si el resultado es significativo, se recurre a pruebas post-hoc como Tukey para identificar cuáles grupos difieren. En el caso de variables cualitativas con múltiples categorías, la prueba de Chi-cuadrado evalúa la independencia entre dos variables, permitiendo determinar si la distribución de una variable depende de la otra.\nSi no se cumplen los supuestos de normalidad o homogeneidad de varianzas, se aplican pruebas no paramétricas como Wilcoxon, Mann-Whitney, Kruskal-Wallis y la prueba de Dunn, que permiten comparar grupos sin asumir una distribución específica de los datos. Además, los métodos de remuestreo, como bootstrap y permutación, ofrecen alternativas para estimar intervalos de confianza y realizar pruebas de hipótesis sin depender de distribuciones teóricas. Estos métodos, implementados en el paquete infer, permiten obtener resultados estadísticamente robustos en distintos contextos.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comparando grupos</span>"
    ]
  },
  {
    "objectID": "comp.html#ejercicios",
    "href": "comp.html#ejercicios",
    "title": "7  Comparando grupos",
    "section": "7.9 Ejercicios",
    "text": "7.9 Ejercicios\n1: Un investigador realiza una prueba \\(t\\) de Student para comparar dos grupos y obtiene un valor de \\(t = 2.5\\). Sabiendo que el valor crítico para un nivel de significancia de \\(\\alpha = 0.05\\) es \\(t_\\text{crítrico} = 2.0\\), ¿cuál es la conclusión correcta?\n\nNo podemos concluir nada sin conocer los grados de libertad.\nSe acepta la hipótesis nula porque el valor crítico es menor.\nSe rechaza la hipótesis nula porque el valor de ( t ) supera el valor crítico.\nEl valor crítico indica que la diferencia entre grupos no es significativa.\n\n2: Si en una prueba \\(Z\\) obtenemos un valor de \\(Z = -1.96\\) con un nivel de significancia del \\(5\\%\\), ¿qué indica este resultado?\n\nEstamos justo en el límite del rechazo de la hipótesis nula.\nNo hay evidencia para rechazar la hipótesis nula.\nEl valor \\(Z\\) nos dice que la diferencia es altamente significativa.\nSe debe hacer otra prueba porque el valor \\(Z\\) no es válido.\n\n3: Flor se ha empeñado en evaluar si existe una diferencia en la puntuación promedio de los estudiantes al utilizar dos métodos de enseñanza distintos. Su objetivo es determinar si uno de los métodos conduce a un mejor o peor desempeño académico en comparación con el otro. Para ello, debe establecer correctamente la hipótesis nula y la hipótesis alternativa. Recordemos que la hipótesis nula representa la ausencia de diferencia entre las medias poblacionales, mientras que la hipótesis alternativa indica que sí hay una diferencia significativa. ¿Cuál de las siguientes opciones es la correcta?\n\nLa hipótesis nula establece que hay una diferencia entre los métodos, mientras que la hipótesis alternativa afirma que no hay diferencia.\n\\(H_0: \\mu_1 \\neq \\mu_2\\) y \\(H_a: \\mu_1 = \\mu_2\\)\nLa hipótesis nula indica que no hay diferencia en la puntuación promedio entre ambos métodos, mientras que la hipótesis alternativa sostiene que sí existe una diferencia. \\(H_0: \\mu_1 = \\mu_2\\) y \\(H_a: \\mu_1 \\neq \\mu_2\\)\nLa hipótesis nula y la alternativa se formulan en términos de medias muestrales en lugar de medias poblacionales. \\(H_0: \\bar{X}_1 = \\bar{X}_2\\) y \\(H_a: \\bar{X}_1 \\neq \\bar{X}_2\\)\nLa hipótesis nula afirma que el primer método es mejor que el segundo, mientras que la hipótesis alternativa establece que el segundo método es superior al primero.\n\\(H_0: \\mu_1 &gt; \\mu_2\\) y \\(H_a: \\mu_1 &lt; \\mu_2\\)\n\n4.Se quiere quiere comparar las medias de dos grupos diferentes con distribución normal y varianzas desconocidas. ¿Qué prueba se debería usar?\nA. ANOVA\nB. Prueba de Chi-cuadrado.\nC. Prueba Z para dos proporciones.\nD. Prueba t para muestras independientes.\n5: Flor está investigando si el nivel de confianza en el gobierno varía según el grupo etario de la población. Para ello, realiza una encuesta donde clasifica la confianza en tres niveles (alta, media, baja) y agrupa a los encuestados en tres rangos de edad (jóvenes, adultos y adultos mayores). ¿Qué prueba estadística es la más adecuada para analizar si existe una relación entre estas variables?\n\nPrueba de Chi-cuadrado.\nPrueba \\(t\\) de muestras emparejadas.\nANOVA de un solo factor.\nPrueba \\(Z\\) para una muestra.\n\n6: Un economista quiere analizar si la reciente reforma laboral ha afectado la cantidad promedio de horas trabajadas por semana en tres sectores: industria, comercio y servicios. Para ello, recopila datos de registros laborales en empresas de cada sector y desea determinar si existen diferencias significativas en la cantidad de horas trabajadas entre los grupos. ¿Qué prueba estadística es la más adecuada para este análisis?\n\nPrueba \\(t\\) de Student.\nANOVA.\nPrueba de Chi-cuadrado.\nPrueba de McNemar.\n\n7: Un investigador en psicología quiere evaluar el impacto de una terapia en los niveles de ansiedad de un grupo de pacientes. Para ello, mide la ansiedad de cada paciente antes y después del tratamiento ¿Qué prueba estadística es la más adecuada para este análisis?\n\nPrueba \\(t\\) de muestras emparejadas.\nPrueba de Wilcoxon.\nPrueba de Chi-cuadrado.\nPrueba de McNemar.\n\n8.Si se quiere evaluar si la distribución de una variable en dos muestras independientes no normales es diferente, ¿qué prueba es la más adecuada?\nA) Prueba Z para dos muestras.\nB) Prueba t de Student.\nC) Prueba U de Mann-Whitney.\nD) Prueba de Chi-cuadrado.\n9.¿Cuál es el supuesto de homogeneidad de varianzas en una prueba t o ANOVA?\n\nA) Que la media de los grupos sea la misma.\nB) Que las varianzas de los grupos sean iguales.\nC) Que la distribución de los datos sea normal en todos los casos.\nD) Que siempre se rechace la hipótesis nula.\n10. ¿Cuál es la principal ventaja de los métodos de permutación y bootstrapping en inferencia estadística?\n\nPermiten estimar distribuciones sin asumir normalidad.**\n\nSolo se pueden aplicar en estudios con más de 1000 observaciones.\n\nSiempre proporcionan valores p más pequeños que las pruebas paramétricas.\n\nNo dependen de los datos originales, solo de los intervalos de confianza.\n\n11: Para estimar la diferencia entre dos medias poblacionales con un intervalo de confianza, recordemos que un intervalo se construye tomando la estimación puntual más un margen de error. ¿Cuál de las siguientes expresiones es la correcta?\n\n\\(\\Delta \\hat{p} \\pm z \\times \\sqrt{\\frac{\\hat{p}_1 (1 - \\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2 (1 - \\hat{p}_2)}{n_2}}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\)\n\\(\\Delta \\bar{X} \\pm t \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\)\n\n12: Un investigador quiere analizar si existe una diferencia en el rendimiento académico entre estudiantes que asisten a clases presenciales y aquellos que estudian de manera virtual. Para ello, aplica una prueba \\(t\\) para comparar las medias de ambos grupos y obtiene un valor \\(p = 0.03\\). Si ha establecido un nivel de significancia de \\(\\alpha = 0.05\\), ¿cuál es la interpretación correcta de este resultado?\n\nSe rechaza la hipótesis nula, lo que sugiere que hay evidencia suficiente para afirmar una diferencia entre los grupos.\nNo se rechaza la hipótesis nula, ya que el valor \\(p\\) es menor que \\(0.05\\).\nSe concluye que la diferencia entre grupos es exactamente \\(3\\%\\).\nEste resultado indica una diferencia estadísticamente significativa, pero no necesariamente una diferencia de gran magnitud.\n\n13: Un grupo de nutricionistas está evaluando el impacto de una nueva dieta en la pérdida de peso. Para ello, miden el peso de un grupo de participantes antes de iniciar la dieta y nuevamente después de cuatro semanas siguiendo el plan alimenticio. Si el objetivo es determinar si hubo un cambio significativo en el peso promedio tras el tratamiento, ¿qué prueba estadística es la más adecuada?\n\nPrueba \\(t\\) para muestras independientes.\nPrueba \\(t\\) para muestras relacionadas (pareadas).\nPrueba de Chi-cuadrado.\nANOVA de un factor.\n\n14: Un estudio analiza la proporción de personas que aprueban una política pública en dos regiones distintas. Se obtiene un intervalo de confianza del 95% para la diferencia de proporciones que va de -0.02 a 0.10. ¿Cuál es la interpretación correcta?\nA) No hay evidencia concluyente de que la proporción de aprobación difiera entre las regiones.\nB) Hay evidencia de que una región tiene mayor aprobación que la otra.\nC) Se debe rechazar la hipótesis nula porque el intervalo incluye valores positivos.\nD) Se debe realizar una prueba ANOVA para confirmar el resultado.\n15: Un estudio analiza la relación entre nivel educativo (primaria, secundaria y universitaria) y la preferencia por un candidato político. ¿Qué prueba estadística es más adecuada para evaluar la asociación entre estas variables?\n\nA) Prueba t de Student.\nB) Prueba de correlación de Pearson.\nC) ANOVA de un factor.\nD) Prueba de chi-cuadrado.\n\n\n\n\nÇetinkaya-Rundel, Mine, y Joanna Hardin. 2021. Introduction to Modern Statistics.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, y Mine Çetinkaya-Rundel. 2021. «infer: An R package for tidyverse-friendly statistical inference» 6: 3661. https://doi.org/10.21105/joss.03661.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comparando grupos</span>"
    ]
  },
  {
    "objectID": "regr.html",
    "href": "regr.html",
    "title": "8  Regresión",
    "section": "",
    "text": "8.1 Regresión lineal simple\n¿Recuerdas nuestro ejemplo sobre la relación entre el PIB per cápita y la esperanza de vida de los países en 2007, basado en el conjunto de datos de Gapminder? En ese análisis, utilizamos un gráfico de dispersión para visualizar la relación entre estas dos variables.\nlibrary(gapminder)\nlibrary(tidyverse)\nFiltramos al 2007\ngp2007 = gapminder::gapminder %&gt;% \n  filter(year == 2007)\nEn el eje X colocamos la variable explicativa, el PIB per cápita (gdpPerCap), mientras que en el eje Y situamos la variable respuesta, la esperanza de vida (lifeExp).\ng_gp2007 = gp2007 %&gt;% \n  ggplot(aes(x = log10(gdpPercap), \n             y = lifeExp)) +\n  geom_point(color = 'darkgreen') +\n  theme_minimal()\n  \n  g_gp2007\nPodemos apreciar visualmente como a medida que avanzamos por el eje \\(x\\) (mayor esperanza de vida), las observaciones tienden subir por el eje \\(y\\) (mayor esperanza de vida).\nOtro ejemplo clásico de análisis de regresión lo encontramos cuando queremos entender la relación entre el precio de una vivienda (variable cuantitativa) en función de determinados factores. Toma por ejemplo este conjunto de datos sobre precios de viviendas. Este conjunto contiene información sobre el precio de viviendas en Taiwan y su relación con otras variables como la distancia a la estación de METRO más cercana (dist_to_mrt_m, medida en metros) y el precio promedio por metro cuadrado (price_twd_msq, medido en dólares taiwaneses).\nAquí también podemos utilizar un gráfico de dispersión para explorar cómo el precio de las viviendas podría variar según la proximidad al transporte público.\ncasas = read_csv('taiwan_casas.csv')\nglimpse(casas)\n\nRows: 414\nColumns: 4\n$ dist_to_mrt_m   &lt;dbl&gt; 84.87882, 306.59470, 561.98450, 561.98450, 390.56840, …\n$ n_convenience   &lt;dbl&gt; 10, 9, 5, 5, 5, 3, 7, 6, 1, 3, 1, 9, 5, 4, 4, 2, 6, 1,…\n$ house_age_years &lt;chr&gt; \"30 to 45\", \"15 to 30\", \"0 to 15\", \"0 to 15\", \"0 to 15…\n$ price_twd_msq   &lt;dbl&gt; 11.467474, 12.768533, 14.311649, 16.580938, 13.040847,…\nUtilizamos geom_point. Dado que hay una gran superposición entre los puntos, es recomendable ajustar el parámetro alpha para controlar la transparencia y así mejorar la visualización, permitiendo distinguir mejor la densidad de los datos.\ng_casa = casas %&gt;%\n  ggplot(aes(x = dist_to_mrt_m, y = price_twd_msq)) +\n  geom_point(alpha = 0.6, # Útil para distinguir mejor la aglomeración\n             color = \"darkblue\") +  # Puntos semitransparentes en azul\n  labs(\n    title = \"Relación entre distancia al Metro y precio por metro cuadrado\",\n    x = \"Distancia al metro (metros)\",\n    y = \"Precio (TWD/m²)\"\n  ) +\n  theme_minimal()  \n\ng_casa\nPodemos notar un tendencia negativa conforma la distancia de la vivienda es mayor a una estación de metro. Si nuestro objetivo es analizar cómo los valores de la variable respuesta (\\(y\\)) cambian en función de la variable explicativa (\\(x\\)). Al observar los gráficos de dispersión, podemos identificar un patrón: parece que hay una relación entre las variables, y esta relación muestra un comportamiento que, a simple vista, podríamos aproximar como lineal. Basándonos en esta observación, proponemos que la relación entre \\(x\\) e \\(y\\) sea lineal, es decir, que los cambios en la variable explicativa (\\(x\\)) produzcan cambios proporcionales (constantes) en la variable respuesta (\\(y\\)).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "regr.html#regresión-lineal-simple",
    "href": "regr.html#regresión-lineal-simple",
    "title": "8  Regresión",
    "section": "",
    "text": "8.1.1 Ecuación de regresión lineal\nDe hecho, podemos representar la linea recta que mejor represente esta relación entre las variables utilizando geom_smooth y definiendo un modelo lineal o linear model (lm).\n\ng_gp2007 +\n  labs(title = 'Relación PBI per cápita y esperanza de vida') +\n  geom_smooth(method = 'lm',\n              color = 'red',\n              se = F # Vamos a omitir el error \n                     # estándar por el mometo\n              )\n\n\n\n\n\n\n\n\n\ng_casa +\n  geom_smooth(method = 'lm', \n              color = 'red',\n              se = F)\n\n\n\n\n\n\n\n\nEsta línea no es solo una herramienta visual, también nos permite aproximar cómo los cambios en una variable (la explicativa, \\(x\\)) afectan a otra variable (la respuesta, \\(y\\)) de forma intuitiva. Siendo una línea recta, podemos describirla con una fórmula matemática. La ecuación de la regresión lineal es la siguiente:\n\\[y = \\alpha + \\beta X\\]\nDonde:\n\\[Y(\\text{variable respuesta}) = \\alpha + \\beta X(\\text{variable explicativa})\\]\nEsto permite verla de forma analítica explorando cómo \\(y\\) responde a diferentes valores de \\(x\\). Antes de desglosar esta fórmula, pensemos intuitivamente en qué hace esta línea:\n\nResume un patrón general: Aunque los puntos en el gráfico pueden no alinearse perfectamente, la línea representa la tendencia principal en los datos.\nNos permite simplificar: En lugar de observar todos los puntos dispersos en un gráfico, podemos usar esta línea para interpretar cómo las dos variables están conectadas en términos generales.\nHace que la relación sea predecible: Una vez que tenemos la línea, podemos usarla para calcular aproximadamente qué valor de \\(y\\) esperaríamos para un determinado valor de \\(x\\).\n\n\nAhora bien, si fórmula que describe esta línea es:\n\\[Y(\\text{variable respuesta}) = \\alpha + \\beta X(\\text{variable explicativa})\\]\nVamos a aclarar cada uno de sus componentes. Sabemos que buscamos ajustar una línea recta a los datos. Una línea recta se puede expresar en una ecuación y se definen por estas dos propiedades: su intercepto y su pendiente.\n\n\\(\\alpha\\): Es el intercepto. Es el punto donde la línea cruza el eje \\(y\\). Nos dice el valor de \\(y\\) cuando \\(x = 0\\).\n\\(\\beta\\): Es la inclinación o pendiente de la línea. Indica cuánto cambia \\(y\\) en promedio, por cada unidad (1) que cambia \\(x\\).\n\nEn el modelo, la ecuación de regresión toma estos dos elementos como los parámetros del modelo.\nEl intercepto, representado como (\\(\\alpha\\)) en la ecuación de la línea recta (\\(y = \\alpha + \\beta x\\)), es el punto donde la línea cruza el eje (\\(y\\)). Es decir, nos indica el nivel inicial de la variable dependiente (\\(y\\)) cuando la variable independiente (\\(x\\)) es igual a cero. En otras palabras, el intercepto nos da una idea del estado de (\\(y\\)) en ausencia de cualquier influencia de (\\(x\\)). En algunos casos, este valor tiene un significado práctico claro, pero en otros puede ser más abstracto, especialmente si (\\(x = 0\\)) no tiene sentido en el contexto del problema.\n\nLa pendiente, representada como (\\(\\beta\\)) en la misma ecuación, describe cómo cambia la variable dependiente (\\(y\\)) por cada unidad adicional de la variable independiente (\\(x\\)). Dicho de otra manera, la pendiente nos indica la tasa de cambio de (\\(y\\)) con respecto a (\\(x\\)). Una pendiente positiva (\\(\\beta &gt; 0\\)) significa que (\\(y\\)) aumenta a medida que (\\(x\\)) lo hace, mostrando una relación directa entre las dos variables. Por el contrario, una pendiente negativa (\\(\\beta &lt; 0\\)) implica que (\\(y\\)) disminuye a medida que (\\(x\\)) aumenta, lo que refleja una relación inversa. Si la pendiente es igual a cero (\\(\\beta = 0\\)), significa que (\\(y\\)) no cambia en absoluto con (\\(x\\)), lo que indica que las dos variables no están relacionadas.\n\nMientras el intercepto determina el punto de inicio de la línea en el eje (\\(y\\)), la pendiente define su inclinación. El hecho de que podamos asignarle una ecuación a la relación cuantificarla y hacer predicciones. Por ejemplo, sabiendo el intercepto y la pendiente, podemos calcular el valor esperado de (\\(y\\)) para cualquier valor de (\\(x\\)).\nPara el caso de las viviendas:\n\ng_casa +\n  geom_smooth(method = 'lm', \n              color = 'red',\n              se = F)\n\n\n\n\n\n\n\n\nPuedes interactuar con los parámetros de la ecuación lineal en la Datáfora Interactiva: [Click Aquí]\nO usa el QR:\n\n\n\nDirígete a la pestaña Regresión lineal\n\n\nEn R podemos ajustar el modelo de regresión lineal y extraer los parametros de ecuación usando lm(). Su estructura básica sigue la lógica de una fórmula: lm(y ~ x, data = ...), donde y representa la variable dependiente o respuesta, y x la variable independiente. El símbolo ~ se lee como “explicado por” o “en función de”, por lo que la expresión y ~ x indica que estamos tratando de explicar los valores de y a partir de los valores de x. El argumento data = ... especifica el nombre del data frame donde se encuentran esas variables.\n\n# Ajustamos el modelo lineal\nmodelo = lm(price_twd_msq ~ dist_to_mrt_m, data = casas)\n\n# Vemos los coeficientes (intercepto y pendiente)\ncoef(modelo)\n\n  (Intercept) dist_to_mrt_m \n 13.873351606  -0.002197292 \n\n\nCon esta información, ya podemos escribir la ecuación del modelo muestral:\n\\[\n\\text{Precio de la vivienda} = 13.87 - 0.0022 \\cdot \\text{Distancia en }m^2\n\\]\nEsto significa que, en base a la información que tenemos disponible sobre las viviendas, la pendiente del modelo es \\(-0.0022\\). Es decir, por cada metro adicional de distancia a la estación de metro, el precio promedio por metro cuadrado disminuye en alrededor de 0.0022 dólares taiwaneses. Por otro lado, el intercepto es \\(13.87\\), lo que indica que cuando la distancia a la estación es cero (es decir, en una vivienda ubicada justo al lado del metro, \\(x=0\\)) el precio estimado por metro cuadrado sería de 13.87 dólares taiwaneses. Si bien en la práctica puede que no existan viviendas exactamente a cero metros del metro, este valor nos da una referencia inicial del modelo y completa la ecuación de la línea recta ajustada.\nAunque esta representación matemática puede parecer un poco abstracta al principio, es simplemente una manera formal de expresar algo que podemos ver intuitivamente en el gráfico. La línea recta nos ayuda a simplificar y entender mejor los patrones en nuestros datos. Sin embargo, como toda simplificación, debemos ser conscientes de sus limitaciones: no todos los puntos se alinean perfectamente con la línea, por lo que la relación que estamos estableciendo es una aproximación. Todos los modelos son aproximaciones simples de la realidad. Un modelo es, en esencia, una representación simplificada de la relación entre variables en una población. Nos ayuda a interpretar patrones observados en los datos.\nSin embargo, es fundamental destacar que este tipo de modelos no pretende capturar toda la complejidad de la realidad. Más bien, funcionan como una herramienta para aproximarnos a las relaciones observadas en los datos. Es crucial recordar que correlación no implica causalidad, y que la relación entre variables dentro de un modelo estadístico no siempre refleja un vínculo de causa y efecto.\nPor ejemplo, si un modelo muestra que un mayor PIB per cápita se asocia con una mayor esperanza de vida, esto no significa que un país pueda simplemente aumentar su PIB per cápita y, como resultado directo, su población vivirá más tiempo. Lo que el modelo realmente indica es que, según los datos disponibles, al comparar dos países, aquel con un PIB per cápita más alto tiende a presentar una mayor esperanza de vida. Sin embargo, esto no implica que el PIB sea el único elemento asociado a esta diferencia, sino que señala una relación observada en los datos, la cual puede estar influenciada por múltiples factores adicionales (debemos ser muy cautos al interpetar estos modelos como asociaciónes, no causas)\nOtro elementos importante a considerar es que, no todas las relaciones entre variables son lineales, por lo que antes de ajustar un modelo de regresión, es fundamental visualizar los datos mediante un gráfico de dispersión. Esto permite identificar si la relación observada puede representarse adecuadamente con una línea recta o si, por el contrario, sería más apropiado considerar otro tipo de modelado.\n\nPor ejemplo, en ambos casos, aunque es posible ajustar una línea recta para describir la relación, se observa claramente que esta no captura correctamente el patrón de los datos para el primer caso. En consecuencia, una regresión lineal no sería el modelo más adecuado, y sería necesario explorar alternativas que reflejen mejor la estructura de la relación entre las variables.\nDe hecho, podemos recurrir a otros métodos para construir modelos (aproximaciones de la realidad) que no dependan de una fórmula lineal. LOESS (Local Regression Smoothing), por ejemplo, es un enfoque no paramétrico que permite suavizar los datos y detectar tendencias en relaciones no lineales entre variables. A diferencia de la regresión lineal tradicional, que impone una relación global fija en todo el conjunto de datos, LOESS realiza múltiples ajustes locales sobre subconjuntos, asignando mayor peso a los puntos cercanos.\n\ng_gp2007 +\n  labs(title = 'Relación PBI per cápita y esperanza de vida') +\n  geom_smooth(method = 'loess', # Metodo loess en ves de lm\n              color = 'red',\n              se = F \n              )\n\n\n\n\n\n\n\n\n\ng_casa +\n  geom_smooth(method = 'loess', \n              color = 'red', \n              se = FALSE)\n\n\n\n\n\n\n\n\nAhora bien, aunque métodos como LOESS pueden ofrecer una representación más precisa de la relación entre variables, especialmente cuando esta no es lineal, su principal desventaja es que resultan menos interpretables, ya que no generan una ecuación clara que podamos analizar o comunicar fácilmente (como la ecuación lineal).\nPor esta razón, y con el objetivo de facilitar la comprensión del modelo, en esta etapa nos centraremos únicamente en las regresiones lineales simples, que si bien pueden ser menos precisas en algunos casos, permiten identificar patrones generales de forma directa y facilmente interpetable. Estoy seguro de que puede proporcionará un punto de partida útil para el propósito de este libro, pero ten en cuenta de que no es el único.\n\n\n8.1.2 Mínimos cuadrados ordinarios (OLS)\nAhora que entendemos la utilidad de una línea recta para modelar relaciones entre variables, el siguiente paso es preguntarnos: ¿cómo se determina esa línea? ¿De dónde sale y cómo sabemos cuál es la más representativa de la relación observada en los datos? Esta línea no se elige al azar, sino que se construye a partir de un método formal conocido como Mínimos Cuadrados Ordinarios (OLS, por sus siglas en inglés). El objetivo del OLS es encontrar los valores óptimos del intercepto (\\(\\alpha\\)) y la pendiente (\\(\\beta\\)) que definan la línea recta que mejor se ajusta a los datos observados, minimizando la suma de los errores al cuadrado entre los valores reales y los valores predichos por el modelo.\nPero, ¿qué significa “mejor ajuste”? Intuitivamente, queremos que la línea esté lo más cerca posible de todos los puntos del gráfico. Para lograr esto, el OLS minimiza la suma de los errores al cuadrado. El “error” en este contexto se refiere a la distancia vertical entre un punto observado (\\(y_i\\)) y el valor predicho por la línea (\\(\\hat{y}_i\\)). Representa cuánto se desvía el modelo de los datos reales en cada punto.\n\nSi un punto está exactamente sobre la línea, el error es cero, si está por encima o por debajo, el error indica cuánto el modelo subestimó o sobreestimó el valor real. El error captura las imperfecciones del modelo. Es decir:\n\\[\n\\text{Error} = y_i - \\hat{y}_i\n\\]\nDonde:\n\n(\\(y_i\\)) es el valor observado de la variable dependiente para un punto específico.\n(\\(\\hat{y}_i\\)) es el valor predicho por la línea para el mismo punto.\n\nEl método OLS eleva al cuadrado estas diferencias y luego suma todos los valores. Esto da como resultado una métrica conocida como la suma de los errores al cuadrado. El OLS ajusta la línea recta de manera que esta suma sea lo más pequeña posible. En otras palabras, la línea resultante es aquella que minimiza la diferencia total entre los valores observados y los valores predichos.\n\\[\n\\text{Suma de errores al cuadrado} = \\sum (y_i - \\hat{y}_i)^2\n\\]\nPero ¿por qué minimizamos el cuadrado de los errores y no simplemente los errores? Elevarlos al cuadrado tiene dos ventajas importantes:\n\nPenaliza más los errores grandes, lo que asegura que la línea no esté significativamente lejos de ningún punto.\nConvierte todos los errores en valores positivos, lo que evita que los errores positivos y negativos se anulen mutuamente.\n\nEste método asegura que la línea recta que obtenemos sea, en cierto sentido, la “mejor representación posible” de los datos según el criterio de los mínimos cuadrados. Al aplicar el OLS, establecemos la linea recta adecuada para nuestras observaciones y, en consecuencia, podemos determinar numéricamente los valores del intercepto (\\(\\alpha\\)) y la pendiente (\\(\\beta\\)) que la definen.\n\n\n8.1.3 Desviación estándar condicional\nCuando usamos una línea de regresión para describir la relación entre dos variables, debemos recordar que no estamos representando los datos de manera exacta. La línea de regresión es una aproximación que resume, de forma promedio, cómo se relacionan las variables (\\(x\\)) (predictora) e (\\(y\\)) (respuesta). En realidad, los puntos de los datos suelen estar dispersos alrededor de esta línea, lo que significa que existe variación en los valores reales de (\\(y\\)) para cada (\\(x\\)).\nPara capturar esta dispersión, el modelo introduce un parámetro adicional, (\\(\\sigma\\)), que representa la desviación estándar condicional. Este parámetro mide, en promedio, cuánto varían los valores de (\\(y\\)) respecto a la línea de regresión para un valor dado de (\\(x\\)). Si (\\(\\sigma\\)) es pequeño, significa que los puntos están más cerca de la línea y que el modelo describe mejor la relación entre las variables. Por el contrario, si (\\(\\sigma\\)) es grande, la dispersión alrededor de la línea es mayor, lo que indica que el modelo es menos preciso en su descripción.\n\nEs importante resaltar que en este modelo estamos asumiendo que la desviación estándar condicional, representada por \\(\\sigma\\), es la misma para todos los valores de \\(x\\). Esto significa que la cantidad de variación de los valores de \\(y\\) alrededor de la línea de regresión es constante en todo el rango de \\(x\\), una condición conocida como homocedasticidad. En otras palabras, independientemente del valor de \\(x\\), se espera que la dispersión de los puntos alrededor de la línea siga siendo similar.\nPero, ¿qué significa esto realmente? Tomando como ejemplo la relación entre el precio de una vivienda y la distancia a la estación de metro, debemos tener en cuenta que al ajustar una línea recta no estamos diciendo que cada valor de \\(x\\) se asocie con un único valor exacto de \\(y\\), sino que la línea representa el valor promedio esperado de \\(y\\) para cada valor de \\(x\\). Es decir, no es una predicción exacta, sino una tendencia general.\nPodemos extraerlo de la siguiente forma\n\nsummary(modelo)$sigma\n\n[1] 3.046457\n\n\nEste valor significa que, en promedio, los precios reales por metro cuadrado se desvían unos 3.05 dólares taiwaneses del valor estimado por el modelo. Por ahora, nos enfocamos en entender que este modelo es solo una aproximación, y que \\(\\sigma\\) nos da una idea clave de cuánta variabilidad existe alrededor de esa tendencia. Más adelante, abordaremos con más detalle estas suposiciones y su impacto en el análisis y la interpretación de los resultados.\n\n\n8.1.4 La correlación y la pendiente\nPara poder evaluar la calidad del ajuste del modelo podemos utilizar una métrica que ya hemos visto previamente: el coeficiente de correlación de Pearson. Como discutimos en el capítulo de estadística descriptiva, este indicador no solo mide la fuerza y la dirección de la relación entre las variables, sino que también nos da una idea de qué tan lineal es dicha relación. Una correlación cercana a 1 o -1 indica que los puntos están cerca de la línea de regresión, mientras que valores más bajos sugieren mayor dispersión alrededor de la línea.\nSabemos que la pendiente (\\(\\beta\\)) de la línea de regresión nos dice en qué dirección va la relación (positiva o negativa) y cuánto cambia la variable dependiente (\\(y\\)) por cada unidad de cambio en la variable independiente (\\(x\\)). Sin embargo, debemos ser cuidadosos al interpretar la pendiente como una medida de la fuerza de la relación. La razón de esto es que el valor numérico de la pendiente depende de las unidades en las que estén medidas las variables.\nAgresti (2018), en Statistical Methods for the Social Sciences, ilustra claramente cómo \\(\\beta\\) depende de las unidades de medición. Supongamos que \\(y\\) representa la tasa de homicidios por cada 100,000 habitantes y \\(x\\) es el porcentaje de personas que viven bajo la línea de pobreza. En este caso, una ecuación de regresión podría ser:\n\\[\n\\hat{y} = -0.86 + 0.58x\n\\]\nAquí, la pendiente (\\(\\beta = 0.58\\)) significa que, por cada incremento de (1%) en la pobreza (\\(x\\)), se espera un aumento de 0.58 homicidios por cada 100,000 habitantes en la tasa de homicidios (\\(y\\)). Sin embargo, si cambiamos las unidades de medición de (\\(y\\)) para expresar la tasa de homicidios por cada 1,000,000 habitantes, el valor de la pendiente también cambia. Como (\\(1,000,000\\)) es 10 veces (\\(100,000\\)), la pendiente se multiplicará por 10, resultando en:\n\\[\n\\beta = 5.8\n\\]\nEsto significa que ahora, por cada incremento de (\\(1 \\%\\)) en la pobreza, se espera un aumento de 5.8 homicidios por cada 1,000,000 habitantes.\nEs importante notar que este cambio en la pendiente no altera la relación entre las variables, ya que esta depende exclusivamente de los datos subyacentes y no de las unidades en que se expresan. El cambio afecta únicamente la escala de la pendiente, no la fuerza o naturaleza intrínseca de la asociación. Por ello, no debemos interpretar (\\(\\beta\\)) como una medida de la fuerza de la relación, ya que su valor puede ser manipulado simplemente cambiando las unidades de las variables.\nAquí es donde el coeficiente de correlación (\\(r\\)) se vuelve especialmente útil. A diferencia de la pendiente (\\(\\beta\\)), el valor de (\\(r\\)) no depende de las unidades de medición. Esto significa que, sin importar cómo midamos las variables (\\(x\\)) e (\\(y\\)), ya sea en porcentajes, tasas o cantidades absolutas, el coeficiente de correlación siempre será el mismo. Aunque el cálculo detrás de (\\(r\\)) implica ciertos ajustes matemáticos para tomar en cuenta la variabilidad de las variables, no es necesario conocer esos detalles para entender su utilidad. Por lo tanto, aunque la pendiente es útil para interpretar el efecto de (\\(x\\)) sobre (\\(y\\)) en términos absolutos, el coeficiente de correlación proporciona una medida estándar que nos permite comparar la fuerza de las relaciones entre diferentes pares de variables, independientemente de sus unidades.\nPuedes interactuar con la correlación en la Datáfora Interactiva: [Click Aquí]\nO usa el QR:\n\n\n\nDirígete a la pestaña Correlación\n\n\n\n\n8.1.5 Coeficiente de determinación\nUna vez que ajustamos el modelo, es natural preguntarse qué tan bien está funcionando. Para eso usamos el coeficiente de determinación, representado como \\(r^2\\). Esta medida nos dice qué proporción de la variabilidad de la variable dependiente (\\(y\\)) puede ser explicada por la variable independiente (\\(x\\)).\nPara entenderlo, se comparan dos enfoques para predecir \\(y\\). El primero consiste en usar únicamente el promedio de \\(y\\) (\\(\\bar{y}\\)) como predicción constante, sin tomar en cuenta \\(x\\). Esto genera una predicción básica pero no considera ninguna relación entre \\(x\\) e \\(y\\). El segundo enfoque utiliza la ecuación de regresión:\n\\[\ny = \\alpha + \\beta X\n\\]\nEsta ecuación incorpora la relación observada entre \\(x\\) e \\(y\\), ajustando las predicciones según los valores de \\(x\\). La utilidad del modelo de regresión se evalúa midiendo cuánto se reducen los errores de predicción al pasar de usar \\(\\bar{y}\\) como predicción a usar \\(\\hat{y}\\).\nEl cálculo de \\(r^2\\) se basa en la proporción de reducción del error. Primero, se mide el error total al usar \\(\\bar{y}\\) para predecir \\(y\\), llamado suma total de cuadrados (\\(TSS\\)):\n\\[\nTSS = \\sum (y - \\bar{y})^2\n\\]\nLuego, se mide el error al usar la ecuación de regresión, llamado suma de cuadrados de los errores (\\(SSE\\)):\n\\[\nSSE = \\sum (y - \\hat{y})^2\n\\]\n\n\n\n\n\n\n\n\n\nLa reducción relativa del error se calcula como:\n\\[\nr^2 = \\frac{TSS - SSE}{TSS}\n\\]\nPero, ¿qué significa esto de forma más intuitiva? Volviendo al ejemplo de las viviendas, imagina que estás tratando de predecir el precio de una vivienda en función de su distancia a la estación de metro. Puedes extrar el coefiecente de la siguiente forma:\n\nsummary(modelo)$r.squared\n\n[1] 0.4537543\n\n\nSi obtenemos un \\(r^2 = 0.45\\), esto quiere decir que el 45% de la variabilidad en los precios puede explicarse solo a partir de la distancia a la estación de metro. En otras palabras, conocer cuán lejos está una vivienda del metro sí aporta información útil sobre su precio, pero también nos deja claro que el 55% restante de la variabilidad se debe a otros factores no incluidos en el modelo, como el tamaño del inmueble, su estado de conservación, el nivel socioeconómico de la zona, o simplemente a variaciones aleatorias.\nPara capturar una mayor proporción de esa variabilidad, será necesario realizar nuevas evaluaciones e incorporar más variables al modelo, algo que abordaremos más adelante en el apartado dedicado a la regresión múltiple.\n\n\n8.1.6 Resumen del modelo en R con summary()\nEn R podemos ajustar el modelo con lm(), y una vez que le asignamos un nombre, podemos usar summary() para obtener un resumen completo que incluye todos los elementos que hemos comentado hasta el momento y algunos más.\n\nsummary(modelo)\n\n\nCall:\nlm(formula = price_twd_msq ~ dist_to_mrt_m, data = casas)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7097  -1.8177  -0.3617   1.4616  22.2338 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   13.8733516  0.1974616   70.26   &lt;2e-16 ***\ndist_to_mrt_m -0.0021973  0.0001188  -18.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 412 degrees of freedom\nMultiple R-squared:  0.4538,    Adjusted R-squared:  0.4524 \nF-statistic: 342.2 on 1 and 412 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nElaboración propia\n\n\nEn la imagen podemos observar claramente los valores estimados para los parámetros del modelo: el intercepto y la pendiente, que se encuentran en la parte superior de la tabla. Justo al costado de cada uno, también aparecen otras columnas con información adicional como el error estándar (Std. Error), el valor t (t value) y el valor p (Pr(&gt;|t|)), que nos permiten hacer inferencias estadísticas sobre estos coeficientes. Por ahora, dejaremos esos elementos para la siguiente sección del capítulo, donde los abordaremos con más detalle.\nEn la parte inferior del resumen, se presentan dos indicadores importantes que ya hemos discutido: la desviación estándar condicional (Residual standard error), que nos dice cuánto tienden a desviarse los valores reales de los valores estimados por el modelo, y el coeficiente de determinación \\(r^2\\), que mide qué proporción de la variabilidad total en el precio por metro cuadrado es explicada por la distancia a la estación de metro.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "regr.html#inferencia-de-coeficientes-para-el-modelo-lineal",
    "href": "regr.html#inferencia-de-coeficientes-para-el-modelo-lineal",
    "title": "8  Regresión",
    "section": "8.2 Inferencia de coeficientes para el modelo lineal",
    "text": "8.2 Inferencia de coeficientes para el modelo lineal\nHasta este punto, ya hemos explorado todas las características de un modelo lineal: cómo se ajusta, qué representa y cómo evaluamos su desempeño. Sin embargo, al igual que sucede con los resúmenes estadísticos como la media o la desviación estándar, en la práctica no trabajamos con toda la población, sino con una muestra limitada de datos. Esto significa que el modelo lineal que construimos no es el modelo de la población, sino una estimación basada en los datos a los que tuvimos acceso (la muestra). Y aquí es donde entra en juego la inferencia: lo que nos interesa ahora no es solo ajustar una línea, sino preguntarnos si esa relación entre las dos variables que observamos en la muestra es suficientemente fuerte o consistente como para pensar que también existe en la población completa. En este contexto, la fórmula cambia ligeramente, porque lo que estamos evaluando ya no es solo el modelo ajustado, sino la plausibilidad de esa relación a nivel poblacional.\nEn el caso de la regresión lineal, cuando hablamos del modelo poblacional, la relación entre las variables se expresa de la siguiente manera:\n\\[\nY = \\alpha + \\beta X\n\\]\nAquí, \\(\\alpha\\) y \\(\\beta\\) son parámetros poblacionales desconocidos, es decir, todas aquellas influencias sobre \\(Y\\) que no se explican a través de \\(X\\). Sin embargo, como en la práctica no tenemos acceso a toda la población, lo que hacemos es trabajar con una muestra y, por lo tanto, estimar esos parámetros a partir de los datos disponibles.Es por ello que el modelo cambia de forma y se convierte en una versión estimada:\n\\[\n\\hat{Y} = a + bX\n\\]\nEn esta nueva expresión, \\(a\\) y \\(b\\) son los estadísticos muestrales que usamos para aproximar a \\(\\alpha\\) y \\(\\beta\\), respectivamente. Es decir, son los valores que obtenemos al aplicar el método de Mínimos Cuadrados Ordinarios (OLS) sobre nuestra muestra. La diferencia entre el valor real \\(Y\\) y el valor estimado \\(\\hat{Y}\\) ahora se denomina residuo, y se representa con:\n\\[\ne = Y - \\hat{Y}\n\\]\nEste cambio de notación no es solo simbólico: refleja una transición fundamental en la forma de pensar el modelo. Ya no estamos afirmando con certeza cómo se comporta la población, sino construyendo una estimación con incertidumbre basada en los datos que tenemos.\nAhora que contamos con los valores estimados \\(a\\) y \\(b\\), en lugar de los parámetros poblacionales \\(\\alpha\\) y \\(\\beta\\), el objetivo pasa a ser poner a prueba la existencia de una relación lineal entre las variables. Para ello, utilizamos las herramientas de la inferencia: la estimación y el contraste de hipótesis, que ya conocemos por otros contextos, como el análisis de medias o proporciones.\nEn este caso, nos centraremos especialmente en el estadístico \\(b\\), que es la estimación puntual del parámetro \\(\\beta\\). Recordemos que \\(\\beta\\) nos da información clave sobre la dirección y magnitud de la relación entre las variables: si \\(\\beta &gt; 0\\), la relación es creciente; si \\(\\beta &lt; 0\\), es decreciente; y si \\(\\beta = 0\\), no hay relación lineal.\nAl igual que otros estadísticos, \\(b\\) está sujeto a variabilidad muestral y, gracias al Teorema del Límite Central (TLC), sabemos que, bajo ciertas condiciones, su distribución se aproxima a una distribución normal. Esto nos permite aplicar la misma intuición que usamos con otros estadísticos: construir un intervalo de confianza alrededor del valor observado de \\(b\\) y realizar un contraste de hipótesis.\n\n8.2.1 Estimación de la pendiente\nUna vez que hemos ajustado nuestro modelo de regresión lineal simple, lo que obtenemos en la práctica no es el verdadero parámetro poblacional \\(\\beta\\), sino una estimación muestral, a la que llamamos \\(b\\). Como hemos visto, \\(b\\) representa la pendiente de la recta ajustada y nos indica cuánto cambia, en promedio, la variable dependiente \\(y\\) por cada unidad de cambio en la variable independiente \\(x\\).\nPero, como toda estimación basada en una muestra, \\(b\\) está sujeta a variabilidad muestral. Es decir, si tomáramos otra muestra diferente de la población, obtendríamos un valor distinto de \\(b\\). Para poder trabajar con esta incertidumbre, recurrimos a técnicas de inferencia estadística que nos permiten construir intervalos de confianza alrededor del valor observado de \\(b\\).\nBajo determinados supuestos que el modelo lineal clásico establece (linealidad, independencia, homocedasticidad y normalidad del error), el estadístico \\(b\\) se distribuye aproximadamente como una normal. Con esa información, podemos construir un intervalo de confianza al 95% para \\(\\beta\\), utilizando la distribución t de Student con \\(n - 2\\) grados de libertad:\n\\[\nIC_{95\\%}(\\beta) = b \\pm t_{(1 - \\alpha/2, \\, n - 2)} \\cdot \\text{EE}(b)\n\\]\nEste intervalo nos da un rango plausible de valores para el verdadero \\(\\beta\\) en la población, asumiendo que el modelo está bien especificado y que los supuestos se cumplen. Recuerda que para in intervalo de confianza es fundamental conocer el Error Estándar: \\(\\text{SE}(b)\\). Este valor nos da una idea de cuánta incertidumbre hay en la estimación de la pendiente. Mientras más pequeño sea el error estándar, más precisa será nuestra estimación y más estrecho será el intervalo. La fórmula es algo más complicada por lo que, para mantener la simplicidad del libro, no nos centraremos en ella. R ya se encarga de hacer este cálculo por nosotros al ajustar el modelo, por lo que en este capítulo nos enfocaremos únicamente en interpretar el resultado y en comprender lo que significa en el contexto del análisis.\nPara nuestro nuestro modelo de viviendas podemos ver que para nuestra variable dist_to_mrt_m la columna Std. Error nos indica el EE\n\nCon el valor estimado de la pendiente \\(b = -0.0021973\\) y su error estándar \\(\\text{SE}(b) = 0.0001188\\), podemos construir manualmente un intervalo de confianza al 95% para el parámetro poblacional \\(\\beta\\).\nRecordemos que al usar un nivel de confianza del 95%, estamos dejando un 2.5% de probabilidad en cada extremo de la distribución t, por lo que buscamos el cuantil \\(t_{(1 - \\alpha/2)} = t_{0.975}\\), considerando los grados de libertad \\(df = n - 2\\) (en regresión lineal simple, siempre restamos 2: uno por la pendiente y otro por el intercepto).\nSabemos que el modelo se ha ajustado con 414 observaciones, por lo que los grados de libertad del modelo son:\n\\[\ndf = 414 - 2 = 412\n\\]\nAl trabajar con un nivel de confianza del 95%, estamos dejando un 5% de probabilidad repartida en ambos extremos de la distribución, es decir, 2.5% a cada lado. Por eso usamos el valor crítico de la distribución t de Student para \\(df = 412\\) y una cola de 2.5%:\n\nqt(0.975, df = 412)\n\n[1] 1.965739\n\n\nCon esto, el intervalo de confianza se construye como:\n\\[\nIC_{95\\%}(\\beta) = b \\pm t_{(0.975, df=412)} \\cdot \\text{SE}(b)\n\\]\nSustituyendo los valores:\n\\[\nIC_{95\\%}(\\beta) = -0.0021973 \\pm 1.966885 \\cdot 0.0001188\n\\]\nCalculamos el margen de error:\n\\[\n1.966885 \\cdot 0.0001188 \\approx 0.0002337\n\\]\nPor lo tanto, el intervalo de confianza es:\n\\[\n[-0.002431, \\; -0.001964]\n\\]\nEsto significa que, con un 95% de confianza, el valor verdadero de la pendiente poblacional \\(\\beta\\) se encuentra entre \\(-0.002431\\) y \\(-0.001964\\).\n\n\n8.2.2 Prueba de independencia: contraste de hipótesis sobre \\(\\beta\\)\nAdemás de estimar un intervalo para \\(\\beta\\), también podemos realizar un contraste de hipótesis formal para evaluar si la relación observada entre \\(x\\) e \\(y\\) es estadísticamente significativa. En este contexto, llevar a cabo una prueba de independencia implica preguntarnos si la relación que vemos en los datos es lo suficientemente consistente como para pensar que no se debe simplemente al azar.\nAl igual que en otros contrastes que hemos trabajado, partimos de la hipótesis nula (\\(H_0\\)), que plantea la ausencia de efecto, es decir, que no existe una relación lineal entre las variables. En términos del modelo, esta hipótesis se formula como que la pendiente poblacional es cero, es decir, \\(\\beta = 0\\). Frente a esta, la hipótesis alternativa (\\(H_1\\)) sostiene que sí hay un efecto, es decir, que la pendiente es distinta de cero y, por tanto, sí existe una relación lineal significativa entre \\(x\\) e \\(y\\).\nPor lo tanto, planteamos las hipótesis de la siguiente forma:\n\nHipótesis nula:\n\\[\nH_0: \\beta = 0\n\\] No hay relación lineal en la población; \\(x\\) e \\(y\\) son independientes.\nHipótesis alternativa:\n\\[\nH_1: \\beta \\neq 0\n\\] Existe una relación lineal entre \\(x\\) e \\(y\\).\n\nDonde el estadístico \\(t\\) para este contraste se calcula como:\n\\[\nt = \\frac{b - 0}{\\text{SE}(b)}\n\\]\nEste valor nos indica cuántas desviaciones estándar se encuentra \\(b\\) por encima o por debajo de 0, y se conoce como estadístico t. Lo comparamos con una distribución t de Student con \\(n - 2\\) grados de libertad, ya que en regresión lineal simple estimamos dos parámetros: el intercepto y la pendiente. En el resumen del modelo, este valor aparece en la columna t value.\n\nAl tener un \\(t\\)-valor de -18.5, podemos observar que la probabilidad de tener un \\(\\beta\\) como el de nuestra muestra bajo \\(H_0\\) es extremamente bajo. Podemos calcularlo para ser más exactos:\n\npt(-18.5, df = 412)\n\n[1] 2.313134e-56\n\n\nDe hecho, si observas bien, ya está calculado al lado.\nSiguiendo la lógica de pruebas anteriores, el valor p asociado a ese estadístico \\(t\\) nos da la probabilidad de obtener un valor tan extremo como el observado, bajo el supuesto de que la hipótesis nula es verdadera, es decir, si \\(\\beta = 0\\). Si el valor p es menor que el nivel de significancia que hayamos definido (por ejemplo, \\(\\alpha = 0.05\\)), entonces rechazamos la hipótesis nula y sostenemos que la relación lineal observada en la muestra es suficientemente fuerte como para no atribuirla al azar. En ese caso, aceptamos la posibilidad de que \\(x\\) e \\(y\\) no sean independientes.\nLa función summary() en R indica visualmente el nivel de significancia con asteriscos al costado de cada coeficiente: mientras más asteriscos aparecen, más pequeño es el valor p.\n\nCabe resaltar que esta misma conclusión también podría haberse alcanzado a partir del intervalo de confianza para \\(\\beta\\). Como vimos anteriormente, el intervalo al 95% no incluye el valor 0, lo cual implica que los valores plausibles para la pendiente poblacional son todos negativos. En otras palabras, incluso considerando la variabilidad muestral, el efecto negativo de la distancia sobre el precio se mantiene dentro de todo el rango de valores posibles definidos por el intervalo. Por eso, tanto el contraste de hipótesis como el intervalo de confianza apuntan en la misma dirección: la relación negativa observada entre la distancia a la estación de metro y el precio por metro cuadrado es suficientemente consistente como para no atribuirla únicamente al azar.\n\n\n8.2.3 Supuestos y calidad de ajuste\nAl igual que el resto de pruebas que ya hemos visto, la regresión lineal simple también se construye sobre una serie de supuestos que nos permiten interpretar correctamente sus resultados y confiar en las conclusiones que se derivan del modelo y las pruebas que hemos realizado.\nEn primer lugar, se asume que la relación entre la variable independiente \\(x\\) y el valor medio de la variable dependiente \\(y\\) es lineal, es decir, que puede aproximarse razonablemente bien mediante una recta. Esto no implica que la relación real sea exactamente lineal( de hecho, rara vez lo es) sino que la línea proporciona una simplificación de la tendencia general. Por eso, antes de ajustar cualquier modelo, es importante examinarlo con un gráfico de dispersión para verificar visualmente si hay indicios de linealidad o si, por el contrario, la relación tiene una forma curva, más compleja o simplemente no hay relación alguna, lo que indicaría que los cambios en \\(x\\) no explican las variaciones en \\(y\\). Sin este primer supuesto, ni siquiera tendría sentido calcular una pendiente.\nPero además de este supuesto de linealidad, el modelo de regresión simple reposa sobre otros supuestos estadísticos:\nSe asume que los errores del modelo (los residuos) son independientes entre sí. Esto significa que el error cometido al estimar un valor de \\(y\\) no debe estar relacionado con el error cometido al estimar otro.\nSe presupone que los errores tienen varianza constante a lo largo de todos los valores de \\(x\\), lo que se conoce como homocedasticidad. Si los errores tienden a ser pequeños para ciertos valores de \\(x\\) y grandes para otros (heterocedasticidad), los errores estándar de los coeficientes estarán mal calculados, y en consecuencia, los intervalos de confianza y los valores p podrían resultar engañosos.\nAdemás, se asume que los errores se distribuyen normalmente alrededor de la recta de regresión. Este supuesto es clave cuando queremos construir intervalos de confianza o realizar contrastes de hipótesis, ya que garantiza que los estadísticos del modelo (como la pendiente \\(b\\)) tendrán una distribución muestral que puede ser razonablemente aproximada por una distribución \\(t\\). En muestras pequeñas, si los residuos son marcadamente no normales, los valores \\(p\\) pueden no ser demasiado confiables, aunque en muestras grandes, el Teorema del Límite Central suaviza este problema.\nPor último, como en cualquier análisis inferencial, se parte del supuesto de que los datos provienen de una muestra aleatoria representativa de la población. Si los datos están sesgados, o si hay algún mecanismo de selección que no ha sido considerado, las estimaciones pueden ser internamente coherentes pero irrelevantes a nivel poblacional. En ese caso, cualquier generalización que se haga desde la muestra al conjunto más amplio será injustificada y poco realista, algo que por momento se olvida.\nTodos estos supuestos están profundamente entrelazados con la validez del modelo y la fiabilidad de las inferencias que realizamos a partir de él. Y aunque se suelen presentar por separado, en la práctica los analizamos a través de un mismo enfoque: el estudio del comportamiento de los residuos.\nRecuerda que los residuos son las partes de la relación que el modelo no ha podido capturar. Si los residuos muestran patrones sistemáticos, acumulaciones, variaciones anómalas o comportamientos extremos, eso indica que el modelo está dejando aspectos importantes sin capturar, lo que pone en cuestión la validez de los coeficientes estimados y de cualquier inferencia que se derive del modelo.\nPor eso, el análisis de residuos es una herramienta clave de diagnóstico, y en regresión lineal existen cuatro gráficos principales que nos permiten inspeccionarlos desde distintos ángulos:\n\nResiduals vs Fitted\nEste gráfico nos permite evaluar la validez del supuesto de linealidad. Si el modelo es adecuado, deberíamos observar una nube de puntos dispersa alrededor de la línea horizontal en cero, sin patrones definidos. Ondas, curvas o estructuras indican que el modelo lineal no está capturando adecuadamente la forma real de la relación entre \\(x\\) e \\(y\\).\n\n\nplot(modelo, which = 1)\n\n\n\n\n\n\n\n\nPodemos ver que, en el caso de nuestro modelo para las viviendas, aunque la nube de puntos en el gráfico Residuals vs Fitted se distribuye de forma más o menos horizontal, la línea de suavizado roja muestra una ligera curvatura, especialmente en los extremos. Esto sugiere que la relación entre la distancia al metro y el precio por metro cuadrado no es perfectamente lineal, y que el modelo tiende a ajustarse peor conforme aumentan los valores de \\(x\\).\n\nNormal Q-Q Plot\nAquí examinamos si los residuos siguen una distribución aproximadamente normal, como lo requiere la inferencia basada en mínimos cuadrados. Si los residuos se alinean con la línea diagonal, el supuesto de normalidad es razonable. Desviaciones sistemáticas (especialmente en los extremos) pueden señalar asimetría, colas pesadas o valores atípicos. El eje \\(y\\) representa ambos extremos de la distribución en desviaicones estándar.\n\n\nplot(modelo, which = 2)\n\n\n\n\n\n\n\n\nPara nuestro modelo, la mayor parte de los puntos siguen bien la línea diagonal, lo que sugiere una distribución aproximadamente normal. Sin embargo, en los extremos (colas) hay algunas desviaciones, especialmente por encima del percentil 95.\n\nScale-Location (Spread-Location)\nEste gráfico nos ayuda a verificar el supuesto de homocedasticidad, es decir, que la varianza de los errores sea constante para todos los valores ajustados. En un modelo bien especificado, deberíamos ver los puntos distribuidos de forma pareja a lo largo de la línea horizontal. Si en cambio se observa una forma de embudo (estrecho al inicio y abierto al final o viceversa), es una señal clara de heterocedasticidad.\n\n\nplot(modelo, which = 3)\n\n\n\n\n\n\n\n\nPara nuestro modelo, la dispersión de los puntos aumenta ligeramente con los valores ajustados: los residuos son más pequeños cuando el valor ajustado es bajo, y más grandes a medida que se incrementa. Esto sugiere una posible heterocedasticidad leve.\n\nResiduals vs Leverage\nEl gráfico Residuals vs Leverage nos permite identificar observaciones que podrían estar influyendo desproporcionadamente en el ajuste del modelo, es decir, puntos que no solo están alejados del centro de los datos en el eje \\(x\\) (alto leverage), sino que además presentan residuos grandes. Estos casos son importantes porque, por su ubicación y comportamiento, tienen mayor capacidad de “empujar” o inclinar la recta de regresión hacia sí mismos en detrimento de las demás observaciones, afectando la estimación de los coeficientes. No todo punto con alto leverage es problemático, pero cuando se combina con un residuo elevado, conviene prestar atención: puede estar distorsionando el modelo más de lo que parece. En este gráfico, las líneas punteadas indican valores de referencia según la Cook’s distance, que ayuda a identificar esos puntos influyentes. Si alguna observación cae cerca o por encima de esas curvas, es recomendable revisarla con más detalle, no necesariamente para eliminarla, sino para entender por qué se comporta de forma tan distinta al resto y qué impacto tiene sobre el modelo.\n\n\nplot(modelo, which = 5)\n\n\n\n\n\n\n\n\nPara nuestro modelo vemos algunos puntos etiquetados (como 271, 149 y 2500) que tienen mayor leverage y residuos relativamente grandes, lo que sugiere que podrían estar influyendo en los coeficientes del modelo. Sin embargo, ninguno parece extremadamente fuera de rango en cuanto a distancia de Cook (curvas punteadas).\nPodemos evaluarlo individualmente:\n\n# Calculamos Cook's Distance y lo añadimos al dataset\ncasas_cook = casas %&gt;%\n  mutate(cooks_distance = cooks.distance(modelo))\n\n# Vemos las observaciones influyentes\ncasas_cook %&gt;%\n  arrange(desc(cooks_distance)) %&gt;% \n  head(5)\n\n# A tibble: 5 × 5\n  dist_to_mrt_m n_convenience house_age_years price_twd_msq cooks_distance\n          &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;          &lt;dbl&gt;\n1          253.             1 0 to 15                 35.6          0.0929\n2         6306.             1 15 to 30                 4.54         0.0529\n3         3781.             0 15 to 30                13.6          0.0487\n4         6396.             1 30 to 45                 3.69         0.0402\n5         6488.             1 15 to 30                 3.39         0.0395\n\n\nPodemos verlo visualmente\n\n\n\n\n\n\n\n\n\nNormalmente vemos las cuatro en conjunto para poder hacernos una idea general del desempeño del ajuste:\n\n# Ajusta los graficos 2x2\npar(mfrow = c(2, 2)) \n\nplot(modelo)\n\n\n\n\n\n\n\n\nLos cuatro gráficos de diagnóstico nos muestran que, para este modelo ajustado con los datos de viviendas, la relación lineal es razonable pero no perfecta. En conjunto, el modelo resulta útil para describir la tendencia general y realizar inferencias básicas, aunque si quisieramos mejorar la precisión del ajuste y la estabilidad de los errores estándar, sería recomendable considerar transformaciones o incorporar variables adicionales que expliquen mejor la variabilidad observada en los extremos.\nTodas estas evaluaciones permiten entender las limitaciones del modelo, detectar posibles errores de especificación y son sumamente importantes. Ignorar esta etapa sería asumir que el modelo es correcto solo porque produce coeficientes y valores p, lo cual propondría un error metodológico grave.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "regr.html#regresión-múltiple",
    "href": "regr.html#regresión-múltiple",
    "title": "8  Regresión",
    "section": "8.3 Regresión múltiple",
    "text": "8.3 Regresión múltiple\nComo vimos, la regresión lineal simple busca ajustar un modelo lineal que describa la relación entre una variable respuesta cuantitativa y una sola variable explicativa. Sin embargo, en muchos casos, especialmente en ciencias sociales, esta relación no puede entenderse de forma aislada. La realidad suele estar influida por múltiples factores al mismo tiempo, por lo que se vuelve necesario ampliar el enfoque e incorporar más de una variable explicativa. Esto puede responder a razones teóricas, cuando distintas variables son relevantes para explicar un fenómeno, o a motivaciones analíticas, como explorar mejor la estructura de los datos o evitar interpretaciones sesgadas por omisión de variables importantes.\nCuando establecemos un modelo de regresión con más de una variable explicativa, hablamos de una regresión lineal múltiple. En este tipo de modelos seguimos asumiendo que existe una relación lineal, es decir, de variación proporcional, entre cada variable explicativa y la respuesta \\(y\\), aunque ahora la línea se transforma en un plano o hiperplano, dependiendo del número de predictores. A diferencia del modelo simple, en la regresión múltiple ya no buscamos entender el efecto de una sola variable sobre \\(y\\), sino cómo varias variables, en conjunto, contribuyen a explicar su comportamiento.\nLa representación gráfica del modelo múltiple no es tan directa. Más allá de tres variables, no podemos visualizar de forma intuitiva el espacio geométrico del modelo, por lo que el análisis visual se vuelve más abstracto y la atención se centra en la interpretación de sus coeficientes.\n\n\n\n\n\n\n\n\n\nSabemos que en muchos fenómenos del mundo real, y con aún mayor énfasis en las ciencias sociales, una sola variable rara vez es suficiente para capturar la complejidad de un fenómeno. Por ejemplo, el precio de una vivienda puede depender de su cercanía al metro, pero también del tamaño, el número de habitaciones, el estado de conservación, la ubicación, entre otros factores. Un caso similar se da en estudios sobre rendimiento educativo, donde no basta con analizar solo el nivel socioeconómico del estudiante. También influyen variables como el tipo de colegio, el capital cultural del hogar, la calidad de los docentes, la carga horaria y el acceso a recursos extracurriculares.\nDesde una perspectiva más general, al pasar de un modelo simple a uno múltiple, transitamos de una lógica unidimensional a una lógica multivariable, en la que la variable respuesta se modela como una función lineal de varios predictores. Esto amplía las posibilidades del análisis, pero también introduce nuevos desafíos en término de su interpretación con los coeficientes y la necesidad de verificar supuestos adicionales. Incluir más variables permite construir un modelo que refleje la dinámica del fenómeno de una forma más realista. Y si bien, generalmente mejora también su capacidad explicativa, muchas veces tampoco de garante de un mejor modelo por lo que tendremos que ser cuidadosos.\n\n8.3.1 Modelo de regresión múltiple\nY entonces, ¿qué hay de la ecuación del modelo? Si ya entendimos cómo funciona la regresión lineal simple, ahora toca ver cómo se generaliza cuando agregamos más de una variable explicativa.\nEn la regresión simple trabajamos con dos parámetros clave: el intercepto (\\(\\alpha\\)) y la pendiente (\\(\\beta\\)), donde \\(\\beta\\) representaba el cambio promedio en la variable respuesta \\(y\\) por cada unidad adicional en la variable explicativa \\(x\\). La regresión múltiple toma esta lógica básica y la extiende, permitiéndonos incluir varias variables explicativas a la vez. Desde el punto de vista metodológico, esto significa que ya no estamos observando una relación aislada entre dos variables, sino que queremos entender cómo un conjunto de predictores contribuyen en conjunto a explicar la variación en \\(y\\).\nEste modelo se expresa de la siguiente manera:\n\\[\ny_i = \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_p x_{pi}\n\\]\ndonde: - \\(y_i\\) es el valor observado de la variable respuesta para la observación \\(i\\), - \\(x_{1i}, x_{2i}, \\dots, x_{pi}\\) son los valores de las \\(p\\) variables explicativas en esa observación, - \\(\\alpha\\) es el intercepto, que representa el valor esperado de \\(y\\) cuando todas las variables explicativas valen cero.\nAhora bien, lo interesante de esta versión del modelo no es solo que agregamos más variables, sino que los parámetros adquieren un nuevo sentido. En la regresión simple, la pendiente \\(\\beta\\) representaba el efecto total de \\(x\\) sobre \\(y\\). Pero en este caso, cada \\(\\beta_j\\) representa un efecto parcial, es decir, el cambio promedio en \\(y\\) cuando \\(x_j\\) varía una unidad, manteniendo constantes todas las demás variables del modelo.\nEste matriz cambia por completo la lógica interpretativa: ya no estamos leyendo relaciones directas, sino efectos condicionados, donde cada coeficiente nos dice qué aporta esa variable por sí sola, en el contexto del resto. Es por eso que, en regresión múltiple, la interpretación debe hacerse con más cautela, y debemos considerar cómo interactúan y se correlacionan las variables entre sí. Los coeficientes siguen siendo pendientes, sí, pero ahora pendientes parciales, que solo pueden leerse correctamente entendiendo el entorno estadístico en el que están estimadas.\nEn R, ajustar un modelo de regresión lineal múltiple es tan sencillo como en el caso simple. La función lm() se utiliza de la misma manera, pero ahora incluimos más de una variable explicativa en la fórmula. Por ejemplo, si quisieramos queremos modelar el precio por metro cuadrado (price_twd_msq) como una función de la distancia a la estación de metro (dist_to_mrt_m) y ahora también del número de tiendas de conveniencia cercanas (n_convenience). El código sería el siguiente:\n\nmodelo_multiple = lm(price_twd_msq ~ dist_to_mrt_m + n_convenience, \n                     data = casas)\n\nsummary(modelo_multiple)\n\n\nCall:\nlm(formula = price_twd_msq ~ dist_to_mrt_m + n_convenience, data = casas)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0484  -1.7736  -0.4108   1.4468  23.7786 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   11.837490   0.393194  30.106  &lt; 2e-16 ***\ndist_to_mrt_m -0.001688   0.000143 -11.799  &lt; 2e-16 ***\nn_convenience  0.362360   0.061291   5.912 7.11e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.928 on 411 degrees of freedom\nMultiple R-squared:  0.4966,    Adjusted R-squared:  0.4941 \nF-statistic: 202.7 on 2 and 411 DF,  p-value: &lt; 2.2e-16\n\n\nEste modelo nos permite estimar los efectos condicionales de cada variable sobre el precio. Es decir, evalúa qué ocurre con el precio si una de las variables cambia una unidad, mientras que la otra se mantiene constante.\nLa función summary() nos da un resumen completo del modelo ajustado. Dentro de este resumen, los elementos clave son:\n\nEstimate: estos son los valores estimados de los coeficientes del modelo (\\(\\hat\\beta_j\\)). Cada uno representa una pendiente parcial, es decir, el cambio promedio en el precio por metro cuadrado cuando esa variable aumenta una unidad, manteniendo constante la otra variable.\nStd. Error: error estándar de cada estimación, que nos da una idea de la precisión con la que fue calculado el coeficiente.\nt value: estadístico \\(t\\) para contrastar la hipótesis nula de que el coeficiente es igual a cero. Cuanto mayor sea este valor en valor absoluto, más evidencia hay en contra de la hipótesis nula.\nPr(&gt;|t|): valor \\(p\\) asociado a ese test. Si es menor que 0.05 (u otro umbral que definamos), interpretamos que el coeficiente es estadísticamente distinto de cero.\nResidual standard error: desviación estándar de los residuos, que nos da una medida de cuánto se desvían, en promedio, las observaciones reales del valor ajustado por el modelo.\nMultiple R-squared: proporción de la variabilidad total de \\(y\\) que el modelo logra explicar con todos los predictores.\nAdjusted R-squared: versión ajustada del anterior que penaliza por el número de variables incluidas. Es más realista cuando se comparan modelos con diferente cantidad de predictores (más sobre esto mas adelante).\n\nAl observar los coeficientes estimados, vemos cómo la distancia al metro y el número de tiendas explican el precio no de forma aislada, sino en el contexto del efecto de la otra. Por ejemplo, si el coeficiente de dist_to_mrt_m es negativo, eso nos dice que a igual número de tiendas cercanas, un aumento en la distancia se asocia con una reducción en el precio. Lo mismo ocurre con n_convenience: si su coeficiente es positivo, eso significa que manteniendo fija la distancia al metro, más tiendas cercanas tienden a estar asociadas con precios más altos.\nAsí, el modelo múltiple no solo mejora la capacidad explicativa, sino que nos obliga a repensar la interpretación. Ya no estamos describiendo relaciones simples, sino efectos marginales, que solo cobran sentido cuando entendemos qué otras variables están siendo controladas en simultáneo.\nUna forma clara de ver este cambio es comparar los coeficientes estimados en el modelo simple y en el múltiple. En particular, podemos observar cómo se modifica la pendiente de dist_to_mrt_m al introducir el número de tiendas como variable de control.\n\nmodelo$coefficients\n\n  (Intercept) dist_to_mrt_m \n 13.873351606  -0.002197292 \n\n\n\nmodelo_multiple$coefficients\n\n  (Intercept) dist_to_mrt_m n_convenience \n 11.837489468  -0.001687748   0.362359771 \n\n\nEn el modelo simple, la pendiente estimada para dist_to_mrt_m era de aproximadamente –0.0022, lo que implicaba que, por cada metro adicional de distancia al metro, el precio disminuía en promedio 0.0022 unidades. Sin embargo, al introducir n_convenience como variable explicativa, la pendiente se reduce en magnitud a –0.0017.\nEste cambio se puede interpretar de la siguiente forma: parte del efecto que atribuíamos originalmente a la distancia estaba en realidad capturando también el efecto de la densidad de tiendas. Es decir, al no controlar por n_convenience, el modelo estaba sobreestimando el impacto de dist_to_mrt_m. Una vez que incluimos ambas variables, el modelo “redistribuye” la varianza explicada, asignando a cada predictor su contribución parcial real.\nPor otro lado, el coeficiente de n_convenience es positivo (0.36), lo que sugiere que, manteniendo constante la distancia al metro, una tienda adicional en el entorno está asociada en promedio con un aumento de 0.36 unidades en el precio por metro cuadrado. Esto refuerza la intuición que podemos tener: no solo importa qué tan cerca está una vivienda del transporte público, sino también qué servicios la rodean.\n\n\n8.3.2 Control de variables\nUno de los aportes más importantes de la regresión múltiple es que nos permite controlar por otras variables al analizar la relación entre dos variables específicas. Pero, ¿qué significa exactamente “controlar por”? En términos simples, quiere decir que estamos estimando el efecto de una variable explicativa sobre la variable respuesta, mientras mantenemos constantes las demás. Este matiz es fundamental, porque en muchos casos, si no controlamos por variables relevantes, la estimación que obtenemos puede estar sesgada, ya sea por una relación espuria o por la omisión de un factor que en realidad está explicando parte del patrón observado.\nDesde una perspectiva práctica, lo que hacemos al incluir una variable en el modelo no es simplemente añadir información, sino aislar la parte de la variación en \\(y\\) que se asocia exclusivamente con una de las variables explicativas, neta del efecto de las demás. En vez de preguntarnos simplemente “¿qué tan fuerte es la relación entre \\(x_1\\) e \\(y\\)?”, lo que preguntamos es: “¿qué tan fuerte es esa relación una vez que ya hemos tomado en cuenta lo que explican \\(x_2\\), \\(x_3\\) y demás?”.\nEste tipo de razonamiento es crucial en contextos donde las variables explicativas están correlacionadas entre sí. Por ejemplo, en estudios sobre participación electoral, podríamos observar que las personas mayores tienden a votar más que las personas jóvenes. Sin embargo, si no controlamos por variables como el nivel educativo, el interés político o el acceso a información, podríamos atribuir a la edad un efecto que en realidad está mediado por otros factores. Al incorporar esas variables en un modelo de regresión múltiple, podemos distinguir si la edad tiene un efecto directo sobre la propensión a votar o si su efecto está condicionado por la formación, la socialización política o el contexto institucional. En ese sentido, el control de variables permite aislar el efecto específico de un factor dentro de un fenómeno que, en la práctica, es multidimensional.\nOtro ejemplo aparece en el análisis del mercado inmobiliario. Supongamos que queremos estimar el efecto del número de baños sobre el precio de una vivienda. Si usamos una regresión simple, podríamos encontrar una relación positiva: más baños, mayor precio. Sin embargo, las viviendas con más baños también suelen tener más metros cuadrados, mejor ubicación y otras características que contribuyen al valor final. Al incluir los metros cuadrados como variable de control, lo que estimamos ahora es el efecto de los baños manteniendo constante el tamaño de la vivienda. En ese contexto, el signo del coeficiente puede incluso invertirse: podría ocurrir que, entre dos viviendas del mismo tamaño, la que tiene más baños sea percibida como mal distribuida o incómoda. En ese caso, la relación entre número de baños y precio puede pasar de positiva a negativa al controlar por otra variable clave, lo que se conoce como una paradoja de Simpson. Este fenómeno ocurre cuando la relación observada a nivel general se revierte al examinarla dentro de subgrupos homogéneos o al ajustar por una variable relevante que estaba oculta en el análisis simple.\nDesde el punto de vista matemático, el control ocurre automáticamente al incluir la variable en el modelo. No es que fijamos su valor manualmente como en un experimento, sino que la regresión estima los coeficientes ajustando simultáneamente el aporte de cada variable, teniendo en cuenta su contribución marginal una vez descontado lo que explican las demás. Así, cada \\(\\beta_j\\) se interpreta como el cambio esperado en \\(y\\) ante un cambio de una unidad en \\(x_j\\), manteniendo constantes todas las demás variables del modelo.\nPara ilustrar esto gráficamente, regresemos a lo más básico: el ajuste de una regresión lineal simple que modela el precio por metro cuadrado en función únicamente de la distancia a la estación de metro. Este gráfico nos permite ver la relación promedio entre ambas variables, y cómo el modelo intenta sintetizar esa relación mediante una línea recta.\n\ncasas %&gt;%\n  ggplot(aes(x = dist_to_mrt_m, y = price_twd_msq)) +\n  geom_point(alpha = 0.6, color = 'darkblue') +\n  geom_smooth(method = 'lm', se = F, color = 'gray20') +\n  labs(\n    title = \"Relación entre distancia al Metro y precio por metro cuadrado\",\n    x = \"Distancia a la estación de metro (m)\",\n    y = \"Precio por m² (TWD)\"\n  ) +\n  theme_minimal()  \n\n\n\n\n\n\n\n\nEsta visualización nos ofrece un primer vistazo a cómo varía el precio en función de la cercanía al metro. Sin embargo, como ya mencionamos, este modelo está capturando no solo el efecto de la distancia, sino también todo lo que no hemos incluido en el análisis: características del vecindario, servicios, edad del inmueble, etc. Una de esas variables potencialmente importantes es el número de tiendas de conveniencia cercanas, que podría estar asociada tanto a la distancia como al precio.\nPara comenzar a visualizar el efecto conjunto de ambas variables, podemos mantener el mismo gráfico base, pero ahora coloreando los puntos según el número de tiendas cercanas.\n\ncasas %&gt;% \n  ggplot(aes(x = dist_to_mrt_m, y = price_twd_msq, \n                  color = n_convenience)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray20\") +\n  scale_color_viridis_c(option = \"D\", direction = 1) +\n  labs(title = \"Controlando por número de tiendas cercanas\",\n       x = \"Distancia a la estación de metro (m)\",\n       y = \"Precio por m² (TWD)\",\n       color = \"Tiendas cercanas\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 9))\n\n\n\n\n\n\n\n\nUna observación importante que surge al visualizar nuestros datos es que la distancia a la estación de metro y el número de tiendas cercanas no son independientes entre sí. Más bien, tienden a ir en la misma dirección: a medida que las viviendas se ubican más cerca del metro, también suelen estar rodeadas de una mayor cantidad de tiendas de conveniencia. Comprender esta lógica es fundamental para interpretar correctamente los resultados y para diseñar modelos sólidos. No controlar por ciertas variables puede llevarnos a sobreestimar o subestimar efectos, o incluso a encontrar relaciones que se revierten cuando se incorpora más información.\nPero cuidado, cuando decidimos controlar por una variable, no basta con incluirla automáticamente en el modelo. También es necesario preguntarnos cómo se relaciona con las demás variables explicativas. Si dos predictores van en la misma dirección, como ocurre aquí entre la distancia al metro y el número de tiendas, debemos evaluar con cuidado cuán fuerte es esa asociación, porque a partir de cierto punto puede generar multicolinealidad. Y esto no es un error del modelo, pero sí un riesgo para la interpretación de los coeficientes: lo que parecía un efecto claro en un modelo simple puede diluirse, invertirse o volverse inestable cuando lo analizamos dentro de un sistema más complejo.\n\n\n8.3.3 Evaluando multicolinealidad\nAntes de pasar al modelo formal, conviene retomar brevemente lo que observamos en la visualización anterior. En ese gráfico, modelamos la relación entre el precio por metro cuadrado y la distancia a la estación de metro, coloreando los puntos según el número de tiendas cercanas. A través de ese recurso visual, ya habíamos notado que las dos variables explicativas (la distancia al metro y la densidad de tiendas) parecían ir en la misma dirección: a menor distancia, mayor número de tiendas.\nEsta asociación empírica es coherente con lo que esperaríamos en el contexto urbano, pero también nos plantea una pregunta importante al momento de ajustar el modelo: ¿cómo se comportan estas dos variables cuando se incluyen simultáneamente como predictores? En particular, ¿están aportando información independiente, o están explicando aspectos similares del fenómeno?\nCuando decidimos controlar por más de una variable en un modelo de regresión múltiple, estamos asumiendo que el modelo puede distinguir con claridad el efecto que aporta cada predictor. Sin embargo, si las variables explicativas están demasiado relacionadas entre sí, esa claridad comienza a desdibujarse. Es lo que se conoce como multicolinealidad, una situación en la que el modelo funciona (ajusta, calcula, predice), pero donde la interpretación de los coeficientes pierde precisión y estabilidad. En otras palabras, los coeficientes siguen existiendo, pero ya no está tan claro qué están midiendo realmente.\nUna primera forma de aproximarse a esta pregunta es a través de una matriz de correlación. En nuestro caso, seleccionamos las variables predictoras del modelo (la distancia a la estación de metro y el número de tiendas cercanas) y calculamos su correlación de Pearson. Esto nos permite observar de forma sencilla la fuerza y dirección de la relación lineal entre ellos, y anticipar si existe redundancia en la información explicativa.\n\nlibrary(corrr)\n\ncasas %&gt;%\n  select(dist_to_mrt_m, n_convenience) %&gt;%\n  correlate() \n\n# A tibble: 2 × 3\n  term          dist_to_mrt_m n_convenience\n  &lt;chr&gt;                 &lt;dbl&gt;         &lt;dbl&gt;\n1 dist_to_mrt_m        NA            -0.603\n2 n_convenience        -0.603        NA    \n\n\nAunque esta herramienta no capta relaciones multivariadas, sí ofrece un primer vistazo útil. Si la correlación es moderada o alta (por ejemplo, mayor a 0.6). Por ejemplo, en nuestro caso obtenemos un coeficiente de –0.60 entre la distancia al metro y el número de tiendas cercanas. Este valor nos dice dos cosas importantes. Primero, que la relación es inversa: a medida que una vivienda se ubica más lejos del metro, tiende a tener menos tiendas en sus alrededores. Segundo, que esa relación no es débil: un valor de –0.60 en términos de correlación lineal ya indica una conexión estructural relativamente fuerte entre ambas variables.\nPodemos visualizarlo en un mapa de calor con dlookr.\n\nlibrary(dlookr)\ncasas %&gt;% \n  select(dist_to_mrt_m, n_convenience) %&gt;% \n  plot_correlate()\n\n\n\n\n\n\n\n\nEsto no es sorprendente desde el punto de vista urbano. Las zonas más céntricas y conectadas (cerca del transporte público) suelen estar más densamente equipadas en servicios. Pero desde el punto de vista del modelo, este resultado nos pone en alerta: si ambas variables explicativas se mueven juntas de forma consistente, el modelo tendrá más dificultad para separar sus efectos individuales.\nEste es precisamente el tipo de escenario donde controlar por una variable exige también diagnosticar su relación con las demás. Si no lo hacemos, corremos el riesgo de interpretar como efecto propio lo que en realidad es un efecto compartido. En este caso, si no incluimos las dos variables con cuidado o no evaluamos su interacción, podríamos estar sobrecargando a una con parte del efecto de la otra, y eso distorsionaría por completo la lectura de los coeficientes.\nUna vez observada la correlación entre predictores, el siguiente paso es confirmar formalmente si esa relación está afectando la estabilidad del modelo. Para eso utilizamos el VIF (Variance Inflation Factor), que nos indica cuánto se ve inflada la varianza del estimador de cada coeficiente debido a su correlación con los demás predictores.\n\nlibrary(car)\n\nvif(modelo_multiple)\n\ndist_to_mrt_m n_convenience \n     1.569931      1.569931 \n\n\nLo primero que notamos es que ambos predictores tienen exactamente el mismo valor de VIF. Esto tiene sentido, ya que vimos que su correlación es simétrica y lineal. Y lo segundo, y más importante, es que el valor está muy por debajo de los umbrales críticos comúnmente utilizados. En general, un VIF cercano a 1 indica ausencia de colinealidad, y se empieza a considerar preocupante solo cuando supera 5 (y especialmente 10).\nEntonces, ¿qué nos dice este resultado? Que aunque existe una relación lineal moderada entre distancia y tiendas, no es lo suficientemente fuerte como para causar inflación significativa en la varianza de los coeficientes. En otras palabras, el modelo logra diferenciar ambos efectos de manera estadísticamente aceptable, y podemos interpretarlos sin una alerta roja por colinealidad.\nAhora bien, esto no significa que debamos ignorar la relación entre ellos. La correlación sigue existiendo, y sigue siendo importante para entender cómo se distribuye la varianza explicada en el modelo. Lo que el VIF nos asegura es que esa relación no está comprometiendo la precisión de nuestras estimaciones.\nPor eso es tan importante complementar la exploración visual y la matriz de correlación con un diagnóstico más robusto. Un valor de VIF bajo no niega la existencia de correlación, pero nos indica que esa correlación no está impidiendo que el modelo funcione como debe. Y en este caso, eso nos permite avanzar con mayor confianza en la interpretación de los coeficientes estimados.\nEvaluar la multicolinealidad no es una formalidad técnica, sino parte del proceso de asegurarnos que las interpretaciones que extraemos del modelo son válidas y estables. Si las variables explicativas están correlacionadas, el modelo puede seguir ajustándose bien, pero las inferencias sobre el efecto de cada predictor deben leerse como efectos condicionales, es decir, suponiendo que las demás variables se mantienen constantes.\nEn nuestro caso, aunque no se observa una colinealidad excesiva, el análisis nos recuerda que las variables urbanas suelen estar interrelacionadas, y que esa interdependencia debe ser tenida en cuenta no solo en el ajuste del modelo, sino también en la lectura crítica de sus resultados.\n\n\n8.3.4 R2 en la correlación múltiple\nHasta ahora hemos evaluado cómo se comportan individualmente las variables explicativas, qué aportan por sí solas y cómo se relacionan entre sí. Sin embargo, cuando construimos modelos de regresión múltiple, lo que realmente queremos saber no es solo qué efecto tiene cada variable, sino cuánto mejora el modelo al incluirlas. En otras palabras: ¿vale la pena añadir una variable más? ¿Estamos explicando mejor el fenómeno o solo estamos complicando el modelo?\nAquí entra en juego el coeficiente de determinación, el ya conocido \\(R^2\\), que vimos en el caso de la regresión simple, pero que cobra un nuevo sentido en el contexto múltiple. Recordemos que \\(R^2\\) mide la proporción de la varianza total de la variable respuesta que el modelo logra explicar con sus predictores. Cuanto más cerca esté de 1, mejor será el ajuste. Pero a diferencia del caso simple, en los modelos con múltiples variables \\(R^2\\) siempre tiende a subir, aunque la nueva variable no aporte nada sustancial.\nPor eso, cuando trabajamos con regresión múltiple, no basta con mirar el \\(R^2\\) “a secas”. Debemos mirar también el \\(R^2\\) ajustado, que penaliza la incorporación de predictores que no mejoran significativamente el modelo. Si al añadir una variable el \\(R^2\\) aumenta, pero el \\(R^2\\) ajustado se mantiene igual o incluso baja, eso es una señal clara de que la nueva variable no está ayudando realmente.\nPara ver esto con datos reales, podemos comparar tres modelos:\n\nUn modelo simple, con solo la distancia al metro.\n\nUn modelo múltiple, que agrega el número de tiendas.\n\nUn modelo más completo, que también incluye la edad de la vivienda (house_age_years).\n\nAunque esta última variable es categórica,\n\nunique(casas$house_age_years)\n\n[1] \"30 to 45\" \"15 to 30\" \"0 to 15\" \n\n\npodemos convertirla a factor para incluirla correctamente en el modelo:\n\ncasas = casas %&gt;%\n  mutate(house_age_years = as.factor(house_age_years))\n\nLuego, ajustamos el modelo con las tres variables:\n\nmodelo_completo = lm(price_twd_msq ~ dist_to_mrt_m + n_convenience + house_age_years, \n                     data = casas)\n\nY ahora observamos y comparamos sus \\(R^2\\) y \\(R^2\\) ajustados:\n\nsummary(modelo)$r.squared\n\n[1] 0.4537543\n\nsummary(modelo)$adj.r.squared\n\n[1] 0.4524284\n\nsummary(modelo_multiple)$r.squared\n\n[1] 0.4965684\n\nsummary(modelo_multiple)$adj.r.squared\n\n[1] 0.4941186\n\nsummary(modelo_completo)$r.squared\n\n[1] 0.5365204\n\nsummary(modelo_completo)$adj.r.squared\n\n[1] 0.5319876\n\n\nCon esto obtenemos una lectura clara de cuánto mejora el ajuste al incluir más variables. Si vemos que el \\(R^2\\) crece ligeramente pero el \\(R^2\\) ajustado no cambia o incluso cae, eso sugiere que la variable añadida está “adornando” el modelo pero no explicando realmente más. Por el contrario, si ambos crecen de manera consistente, estamos ante una variable que sí aporta a la capacidad predictiva del modelo.\nEn nuestro caso, los resultados nos permiten leer con claridad el efecto de ir agregando variables al modelo. El modelo simple, con solo la distancia al metro, logra explicar aproximadamente un 45.4% de la variabilidad del precio por metro cuadrado. No es un mal punto de partida, pero sabemos que la realidad del valor de una vivienda no depende únicamente del acceso al transporte.\nCuando añadimos el número de tiendas cercanas, el \\(R^2\\) sube a 49.7%, y lo que es más importante, el \\(R^2\\) ajustado también crece de forma consistente hasta 49.4%. Esto nos indica que la nueva variable realmente está aportando capacidad explicativa al modelo, no solo inflando artificialmente su rendimiento. En otras palabras, tiene sentido mantenerla: no solo tiene coherencia urbana, sino que mejora la calidad del ajuste.\nFinalmente, al incorporar la edad de la vivienda, llegamos a un \\(R^2\\) de 53.7% y un \\(R^2\\) ajustado de 53.2%. De nuevo, el crecimiento es moderado, pero consistente. No estamos forzando forzando un modelo más complejo ni cayendo en sobreajuste: estamos incorporando un factor relevante que ayuda a explicar parte de la variabilidad restante.\nEsto confirma que cada una de las variables agregadas ha mejorado el ajuste de forma útil y justificable. No solo desde el punto de vista del modelo, sino también desde la lógica del fenómeno urbano que estamos tratando de modelar: el valor de una vivienda no depende únicamente de un factor, sino de la combinación entre ubicación, accesibilidad, servicios y características propias del inmueble. La regresión múltiple no nos da respuestas absolutas, pero sí una lectura más honesta de cómo interactúan esos factores al momento de predecir un precio.\n\n\n8.3.5 Inferencia de coeficientes\nComo ya vimos en la regresión lineal simple, cada coeficiente estimado en el modelo representa un valor puntual que aproxima un parámetro poblacional desconocido, y su interpretación va acompañada siempre de incertidumbre. Esa lógica no cambia cuando pasamos a un modelo múltiple. Lo que sí cambia es el contexto en el que se interpreta cada estimación.\nEn el caso del modelo completo que hemos construido, los coeficientes, incluyendo la pendiente de dist_to_mrt_m, el efecto de n_convenience, y las categorías de house_age_years, se entienden ahora como efectos condicionales. Es decir, no nos dicen simplemente “qué pasa cuando una variable cambia”, sino qué pasa cuando esa variable cambia mientras las demás se mantienen constantes. Y eso tiene consecuencias para la inferencia: el valor p que acompaña a cada estimación ya no está evaluando la relación “bruta” entre \\(x\\) e \\(y\\), sino la relación neta, dentro de un sistema de controles.\nPor eso, aunque las pruebas estadísticas siguen siendo técnicamente las mismas (valores t, errores estándar, intervalos de confianza), su lectura cambia. Rechazar la hipótesis nula de que el coeficiente de n_convenience es cero, por ejemplo, no significa que más tiendas siempre aumentan el precio, sino que su efecto permanece significativo incluso cuando controlamos por distancia y edad de la vivienda. Eso le da al resultado un peso distinto, y una interpretación más robusta.\nAdemás, al tratarse de un modelo con múltiples variables, los coeficientes pueden cambiar considerablemente respecto al modelo simple, no solo en magnitud sino incluso en dirección. Esto no es un error, sino una consecuencia directa del control: cuando aislamos el efecto de una variable, descubrimos lo que realmente está aportando por sí sola, y no lo que estaba heredando de su correlación con otras.\nAhora que hemos ajustado el modelo completo, podemos mirar en detalle lo que nos dice summary(modelo_completo).\n\nsummary(modelo_completo)\n\n\nCall:\nlm(formula = price_twd_msq ~ dist_to_mrt_m + n_convenience + \n    house_age_years, data = casas)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.935  -1.669  -0.444   1.312  23.088 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             12.4620937  0.3925612  31.746  &lt; 2e-16 ***\ndist_to_mrt_m           -0.0015699  0.0001406 -11.168  &lt; 2e-16 ***\nn_convenience            0.3982338  0.0593259   6.713 6.40e-11 ***\nhouse_age_years15 to 30 -1.5887298  0.3290985  -4.828 1.96e-06 ***\nhouse_age_years30 to 45 -1.7614724  0.3563492  -4.943 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.816 on 409 degrees of freedom\nMultiple R-squared:  0.5365,    Adjusted R-squared:  0.532 \nF-statistic: 118.4 on 4 and 409 DF,  p-value: &lt; 2.2e-16\n\n\nComencemos por lo que ya conoces. El intercepto:\n(Intercept) = 12.46\nComo ya explicamos, este valor representa el precio promedio por metro cuadrado de una vivienda de entre 0 y 15 años de antigüedad, ubicada a 0 metros de una estación de metro (lo cual es hipotético,), y con cero tiendas de conveniencia cercanas (también poco realista). En modelos múltiples, el intercepto no suele tener un sentido práctico directo, pero funciona como punto de referencia a partir del cual se aplican los efectos de las otras variables.\ndist_to_mrt_m = –0.00157\nEste coeficiente indica que, por cada metro adicional de distancia a la estación de metro, el precio por metro cuadrado disminuye en promedio 0.00157 unidades, manteniendo constantes el número de tiendas cercanas y la edad de la vivienda. Este efecto es coherente con la intuición urbana: la accesibilidad al transporte suele ser un factor valorizador en mercados inmobiliarios densos.\nLo importante aquí es entender que no estamos midiendo una relación bruta, sino un efecto condicionado: incluso si dos viviendas tienen la misma edad y están rodeadas por la misma cantidad de tiendas, la que está más lejos del metro tiende a valer menos. El valor p asociado (&lt; 2e-16) indica que este efecto es altamente significativo.\nn_convenience = 0.398\nEste coeficiente nos dice que, por cada tienda de conveniencia adicional cercana, el precio por metro cuadrado aumenta en promedio 0.398 unidades, siempre que la distancia al metro y la antigüedad de la vivienda se mantengan constantes. En otras palabras, el efecto de las tiendas no está siendo confundido con el de la ubicación ni con el de la edad del inmueble.\nEl valor positivo confirma la idea de que los servicios de cercanía tienen un efecto valorizador, algo ya conocido en economía urbana. Pero lo que el modelo nos permite ver es que este efecto persiste independientemente del acceso al transporte.\nhouse_age_years15 to 30 = –1.59\nAquí ingresamos al terreno de las variables categóricas. Este coeficiente no se interpreta como un precio, sino como una diferencia de precio con respecto a la categoría base, que en este caso es “0 to 15” años. Así, el valor de –1.59 nos indica que una vivienda de entre 15 y 30 años cuesta en promedio 1.59 unidades menos por metro cuadrado que una de entre 0 y 15 años, manteniendo constantes las demás variables.\nEste efecto negativo sugiere que el valor de una vivienda se deprecia con el paso del tiempo, lo cual es esperable, sobre todo si no ha sido renovada o modernizada. Pero lo interesante es que este efecto no está asociado a peores ubicaciones o menos servicios, ya que esos elementos están controlados.\nhouse_age_years30 to 45 = –1.76\nLa lógica aquí es exactamente la misma: esta estimación representa la diferencia promedio de precio entre una vivienda de entre 30 y 45 años y una de entre 0 y 15. El efecto es aún más negativo que el anterior, lo que sugiere una tendencia continua de pérdida de valor conforme avanza la antigüedad del inmueble.\nLo que este modelo nos está mostrando, entonces, es una curva de depreciación parcial del precio de la vivienda por edad, dentro de un contexto urbano en el que el acceso al metro y a tiendas permanece constante. Es decir, el valor no depende solo de dónde estás o qué servicios tienes cerca, sino también de cuán nuevo o viejo es el inmueble mismo.\nTodos los valores \\(p\\) están muy por debajo del umbral convencional de 0.05 (los asteriscos también te informan aquello), lo que significa que todos estos efectos son estadísticamente significativos. Pero, como ya hemos discutido antes, eso no quiere decir que sean “grandes” o “decisivos” por sí solos, sino que tenemos evidencia suficiente en esta muestra como para afirmar que su efecto es significativo y, por tanto, no es producto del azar.\n\n\n8.3.6 Modelos múltiples con interacción\nHasta ahora, hemos trabajado con modelos múltiples que combinan varias variables explicativas para entender cómo se relacionan con una variable respuesta. En todos estos casos, el supuesto de fondo era claro: cada variable tiene un efecto independiente, que se suma a los efectos de las demás. Es lo que se conoce como modelo aditivo. Y si bien este tipo de modelos es útil y muchas veces suficiente, no siempre representa bien la complejidad de los fenómenos reales.\nEn el mundo social (y en muchos otros contextos también) los efectos rara vez son puros y aislados. Lo que una variable hace, muchas veces depende del entorno. O dicho con más precisión: el efecto de una variable puede cambiar según el valor que tome otra. Este tipo de situación es lo que en estadística se llama una interacción.\nEntonces, ¿cómo sabemos si necesitamos modelar una interacción? A veces es una hipótesis teórica, pero muchas veces podemos verlo explorando visualmente los datos. Si trazamos una relación entre dos variables, y esa relación no tiene la misma forma o pendiente para distintos grupos, estamos frente a un caso donde el efecto de una variable cambia según otra.\nSupongamos que estamos investigando la satisfacción de los ciudadanos con su gobierno local, medida en una escala del 1 al 10. Queremos entender cómo esta satisfacción se relaciona con el nivel de ingreso de la persona y con la zona donde reside (urbana o rural). Desde una perspectiva teórica, podríamos hipotetizar que el ingreso mejora la percepción ciudadana del gobierno, pero que ese efecto no es igual en todos los contextos.\nTomemos como ejemplo este conjunto de datos simulado con 200 observaciones, donde el ingreso varía entre 1000 y 5000 soles, y la zona puede ser “urbana” o “rural”.\n\nstf = read_csv('satisfaccion.csv')\n\nglimpse(stf)\n\nRows: 200\nColumns: 4\n$ ...1         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ ingreso      &lt;dbl&gt; 1955, 4849, 3405, 3060, 2610, 4521, 2456, 2153, 1683, 168…\n$ zona         &lt;chr&gt; \"urbana\", \"urbana\", \"urbana\", \"rural\", \"urbana\", \"rural\",…\n$ satisfaccion &lt;dbl&gt; 6, 6, 4, 5, 3, 5, 4, 5, 5, 3, 6, 4, 5, 3, 3, 6, 4, 3, 3, …\n\n\nAntes de modelar, conviene graficar los los datos para ver si hay alguna pista de interacción.\nPrimero la relación global:\n\nstf %&gt;% \nggplot(aes(x = ingreso, y = satisfaccion)) +\n  geom_point(alpha = 0.6) +\n  labs(\n    title = \"Relación entre ingreso y satisfacción por zona\",\n    x = \"Ingreso mensual (S/.)\",\n    y = \"Satisfacción con el gobierno local (1–10)\",\n    color = \"Zona de residencia\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAhora podemos añadir la variable zona diferenciando por color:\n\nstf %&gt;% \nggplot(aes(x = ingreso, y = satisfaccion, color = zona)) +\n  geom_point(alpha = 0.6) +\n  labs(\n    title = \"Relación entre ingreso y satisfacción por zona\",\n    x = \"Ingreso mensual (S/.)\",\n    y = \"Satisfacción con el gobierno local (1–10)\",\n    color = \"Zona de residencia\"\n  ) +\n  theme_minimal() +\n   theme(legend.position = \"bottom\",\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 9))\n\n\n\n\n\n\n\n\nA primera vista, parece haber una relación positiva entre ingreso y satisfacción en ambos contextos. Es decir, a medida que el ingreso aumenta, también lo hace, en promedio, la satisfacción con el gobierno local. Sin embargo, esa relación no es idéntica entre zonas. En el caso de las zonas rurales, la pendiente de la relación parece más empinada, lo que sugiere que los aumentos en ingreso tienen un mayor impacto en la percepción ciudadana. En cambio, en zonas urbanas, la relación también es positiva, pero menos pronunciada.\nCon esa intuición visual en mente, pasamos ahora al ajuste formal de los modelos. Como hemos venido explicando, el modelo aditivo parte de la idea de que el efecto del ingreso sobre la satisfacción es el mismo para todos los grupos, en este caso, para personas de zonas urbanas y rurales. Es decir, se estima una sola pendiente común a ambos grupos, aunque se permita que el nivel promedio de satisfacción (el intercepto) sí pueda variar entre zonas.\nVeamos cómo se ajusta este modelo en R:\n\nmodelo_aditivo = lm(satisfaccion ~ ingreso + zona, data = stf)\nsummary(modelo_aditivo)\n\n\nCall:\nlm(formula = satisfaccion ~ ingreso + zona, data = stf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3061 -0.7584 -0.1117  0.6787  3.4160 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.268e+00  2.567e-01  12.730   &lt;2e-16 ***\ningreso      7.396e-04  7.395e-05  10.001   &lt;2e-16 ***\nzonaurbana  -1.727e+00  1.732e-01  -9.969   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.221 on 197 degrees of freedom\nMultiple R-squared:  0.5208,    Adjusted R-squared:  0.5159 \nF-statistic:   107 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nEs importante aclarar una idea que puede prestarse a confusión: el modelo aditivo sí toma en cuenta la variable zona, pero lo hace de una forma distinta al modelo con interacción. En el modelo aditivo, se reconoce que las personas que viven en zonas urbanas y rurales pueden tener niveles distintos de satisfacción, pero se asume que el efecto del ingreso sobre esa satisfacción es el mismo para ambos grupos.\nEsto significa que el modelo aditivo permite diferencias en el punto de partida (el intercepto) entre zonas, pero no en la pendiente. Es decir, las líneas de regresión para zona urbana y rural pueden estar a distinta altura, pero tienen la misma inclinación. La lógica que subyace aquí es que el ingreso mejora la satisfacción al mismo ritmo sin importar el contexto, y que las diferencias entre grupos se deben solo a un nivel promedio más alto o más bajo de satisfacción.\nEn términos más concretos: si el coeficiente de ingreso en el modelo aditivo es 0.00074, entonces ese valor se aplica tanto a personas urbanas como rurales. Lo único que cambia es el valor inicial desde el cual parte esa relación. En este modelo, lo que zonaurbana está haciendo es ajustar el intercepto: está diciendo cuánto más (o menos) satisfechas están las personas urbanas en comparación con las rurales, cuando ingreso = 0. Pero una vez que empezamos a sumar ingreso, el efecto es el mismo para todos.\nAquí le estamos diciendo a R: “ajusta una recta que relacione ingreso y satisfacción, diferenciando entre zonas solo en el punto de partida (intercepto), pero manteniendo la misma pendiente para ambos grupos”.\n\n# Generamos los valores ajustados del modelo aditivo\nstf = stf %&gt;%\n  mutate(pred_aditivo = predict(modelo_aditivo))\n\nstf %&gt;% \n  ggplot(aes(x = ingreso, y = satisfaccion, color = zona)) +\n  geom_point(alpha = 0.5) +\n  geom_line(aes(y = pred_aditivo), size = 1) +\n  labs(\n    title = \"Modelo aditivo: misma pendiente, distinto intercepto\",\n    x = \"Ingreso mensual (S/.)\",\n    y = \"Satisfacción con el gobierno local\",\n    color = \"Zona\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 9))\n\n\n\n\n\n\n\n\nEl segundo modelo es más flexible. Le decimos a R que permita que la pendiente también cambie según la zona, es decir, que la relación entre ingreso y satisfacción pueda tener una forma diferente en zonas urbanas y rurales. Esto se conoce como un modelo con interacción, y se escribe añadiendo un asterisco (*) entre las dos variables.\n\nmodelo_interaccion = lm(satisfaccion ~ ingreso * zona, data = stf)\nsummary(modelo_interaccion)\n\n\nCall:\nlm(formula = satisfaccion ~ ingreso * zona, data = stf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5425 -0.6968 -0.0845  0.7607  2.9174 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         2.219e+00  3.231e-01   6.866 8.49e-11 ***\ningreso             1.085e-03  9.907e-05  10.949  &lt; 2e-16 ***\nzonaurbana          3.099e-01  4.452e-01   0.696    0.487    \ningreso:zonaurbana -6.882e-04  1.399e-04  -4.920 1.83e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.155 on 196 degrees of freedom\nMultiple R-squared:  0.5735,    Adjusted R-squared:  0.5669 \nF-statistic: 87.84 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\nEste modelo internamente incluye tres cosas:\n\nEl efecto principal del ingreso.\nEl efecto principal de la zona (es decir, la diferencia entre urbana y rural si ingreso fuera 0).\nLa interacción: cómo cambia el efecto del ingreso dependiendo de la zona.\n\nEn este modelo con interacción, lo que nos interesa principalmente es cómo cambia la relación entre ingreso y satisfacción según el tipo de zona. Y para eso, tenemos que prestar atención a dos coeficientes: ingreso y ingreso:zonaurbana.\nEl coeficiente de ingreso nos da la pendiente para el grupo de referencia, que en este caso es la zona rural. Su valor es aproximadamente 0.001085, lo que significa que por cada 1000 soles adicionales de ingreso, la satisfacción promedio con el gobierno local aumenta en 1.085 puntos en zonas rurales. Este efecto es estadísticamente significativo, con un valor p menor a 0.001, lo cual nos indica que la relación positiva entre ingreso y satisfacción en zonas rurales es fuerte y robusta.\nEl término ingreso:zonaurbana, en cambio, no es una pendiente directa, sino una corrección a la pendiente anterior. Su valor es -0.000688, también estadísticamente significativo. Esto quiere decir que, en zonas urbanas, la pendiente ya no es la misma que en zonas rurales, sino que se reduce en 0.000688.\nPara conocer la pendiente en zonas urbanas, basta con sumar ambos coeficientes:\n\\[\n\\text{Pendiente urbana} = 0.001085 - 0.000688 = 0.000397\n\\]\nEsto implica que, para las personas que viven en zonas urbanas, un aumento de 1000 soles en el ingreso mensual solo incrementa la satisfacción en 0.397 puntos, es decir, menos de la mitad del efecto observado en zonas rurales.\nY esto no es menor. Ambos coeficientes son estadísticamente significativos, lo que significa que esta diferencia no puede atribuirse al azar muestral. Lo que estamos viendo aquí es un patrón claro y sostenido: el ingreso tiene un impacto más fuerte sobre la satisfacción ciudadana en zonas rurales que en zonas urbanas. Esto podría deberse a muchas razones estructurales (desigualdades en el acceso a servicios, expectativas distintas, mayor sensibilidad al cambio económico en contextos con menor infraestructura), pero lo esencial es que el modelo con interacción nos permite detectar y cuantificar esa diferencia.\nAquí permitimos que cada grupo tenga su propia recta, es decir, su propia relación entre ingreso y satisfacción.\n\nggplot(stf, aes(x = ingreso, y = satisfaccion, color = zona)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x) +\n  labs(\n    title = \"Modelo con interacción: pendiente diferenciada por zona\",\n    x = \"Ingreso mensual (S/.)\",\n    y = \"Satisfacción con el gobierno local\",\n    color = \"Zona\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_text(size = 10, face = \"bold\"),\n        legend.text = element_text(size = 9))\n\n\n\n\n\n\n\n\nEsta diferencia no es solo visual, tiene una implicancia estadística importante: en el modelo aditivo estamos forzando una media entre dos relaciones distintas, lo que puede resultar algo simplista. En cambio, el modelo con interacción permite que cada grupo siga su propia relación. No siempre es necesario usar interacciones, pero cuando las diferencias entre grupos son significativas, incluirlas mejora la interpretación y la precisión del análisis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "regr.html#un-vistazo-a-otros-modelos-de-regresión",
    "href": "regr.html#un-vistazo-a-otros-modelos-de-regresión",
    "title": "8  Regresión",
    "section": "8.4 Un vistazo a otros modelos de regresión",
    "text": "8.4 Un vistazo a otros modelos de regresión\nHasta ahora hemos trabajado con el modelo lineal clásico y sus variaciones: simple, múltiple, con interacción. Hemos aprendido a leer pendientes, interpretar interceptos y evaluar supuestos. Pero también hemos visto que este modelo funciona bajo ciertas condiciones: que la variable respuesta sea continua, que la relación con los predictores sea aproximadamente lineal, y que los errores se comporten de forma razonable. Sin embargo, cuando salimos de los ejemplos didácticos y nos enfrentamos a fenómenos reales, especialmente en las ciencias sociales, esas condiciones se tensan o directamente se rompen. Hay variables que no son continuas, sino categóricas, ordinales, proporciones, conteos. Hay relaciones que no son lineales, sino curvas o de otra naturaleza. Hay fenómenos donde los errores no son normales, y donde la media no es un bien resumen numérico del comportamiento esperado.\nY entonces uno se da cuenta de algo clave: el mundo de las regresiones no termina en la línea recta, sino que es un campo vasto, flexible, como la realidad misma. Lo que une a todos estos modelos es su propósito: intentar representar, con cierta estructura matemática, una relación que observamos o que queremos cuantificar. Pero el cómo se hace varía, y eso es precisamente lo que exploraremos aquí. En este apartado no voy a mostar en detalle cada técnica (que se puede aplicar en R pero el capítulo se está tornando más largo de lo esperado), pero sí voy a trazar un mapa que pueda ser de tu utilidad para investigar con mayor profundidad. Mostrar que hay otras formas de modelar, otras maneras de construir relaciones entre variables, y que la elección del modelo no es solo una cuestión técnica, sino también teórica y sustantiva: se trata de elegir el modelo que mejor captura la estructura del fenómeno que estamos estudiando.\n\n8.4.1 Más allá de la linealidad\nUna de las primeras cosas que suele fallar en la regresión clásica es la suposición de linealidad. No siempre el cambio en la variable respuesta es proporcional al cambio en la variable explicativa. Y aunque en muchos casos podemos “forzar” linealidad con transformaciones (como logaritmos o raíces cuadradas), esto tiene un costo interpretativo. Por ejemplo, si modelamos log(ingreso) en lugar de ingreso, la pendiente ya no se interpreta como un cambio absoluto, sino como un cambio en la variable respuesta ante un cambio porcentual en ingreso. Las transformaciones son útiles y necesarias, pero deben ser manejadas con cuidado, porque alteran el significado sustantivo del modelo. Otra opción, en lugar de transformar, s elegir otro tipo de regresión que se adapte mejor al tipo de variable o a la forma de la relación.\nRegresión logística (cuando la respuesta es binaria)\nCuando la variable que queremos explicar no es continua, sino dicotómica (sí/no, participa/no participa, vota/no vota), la regresión lineal no es útil. Usamos la regresión logística, que modela la probabilidad de que ocurra el evento de interés a partir de una función que representa los cambios de la probabilidad. Aquí ya no hablamos de cambios en la media, sino de efectos sobre la razón de probabilidades (odds). El coeficiente de una variable nos dice cuánto se multiplican las odds de que ocurra el evento cuando esa variable aumenta una unidad, manteniendo constantes las demás. Un ejemplo clásico en ciencias sociales podría ser: ¿cuáles son las características que aumentan la probabilidad de participar de votar en elecciones municipales?\nRegresión polinómica (cuando la relación es curva)\nA veces, lo que falla no es el tipo de variable, sino su forma de relacionarse con otras. La regresión polinómica permite capturar relaciones no lineales pero suaves, introduciendo términos cuadráticos o cúbicos de una misma variable. Por ejemplo, la relación entre edad y satisfacción política no suele ser lineal: puede subir hasta cierta edad y luego bajar. En esos casos, una línea recta subestima el efecto real. Un modelo polinómico, en cambio, puede representarlo sin cambiar el tipo de variable.\nRegresión de Poisson (cuando contamos eventos)\nHay veces en que la variable respuesta no es continua ni categórica, sino un conteo: número de asistencias, número de reuniones comunitarias, cantidad de conflictos, etc. Estos datos suelen tener una distribución asimétrica, con muchos ceros, y su varianza suele crecer con la media. La regresión de Poisson está pensada para eso. Modela el logaritmo del valor esperado de los conteos en función de los predictores. Es ampliamente usada en estudios de criminalidad, participación, salud pública y movimientos sociales.\nRegresión ordinal (cuando hay orden, pero no distancia)\nEn ciencias sociales abundan las escalas de tipo Likert: de 1 a 5, de “muy en desacuerdo” a “muy de acuerdo”. Aquí las categorías tienen orden, pero no hay una distancia clara entre ellas. Una regresión lineal forzaría un supuesto que no es real (que la diferencia entre 2 y 3 es igual a la de 4 y 5).En estos casos usamos modelos logísticos ordinales, que asumen que hay un continuo subyacente (una “satisfacción latente”), y que cada categoría representa un corte en ese continuo. Así modelamos la probabilidad acumulada de estar en una categoría o inferior.\nRegresión multinomial (cuando las categorías no tienen orden)\nHay veces en que las categorías de la variable respuesta no tienen un orden inherente: tipos de afiliación política (izquierda, centro, derecha), tipos de ocupación, regiones del país, etc. En esos casos, usamos modelos multinomiales, que comparan cada categoría con una de referencia y estiman la probabilidad relativa de estar en cada grupo.\nModelos multinivel\nEn muchos casos, los datos no vienen en observaciones individuales sueltas, sino agrupadas por contextos: estudiantes dentro de escuelas, personas dentro de distritos, ciudadanos dentro de países. Aquí usamos modelos multinivel o mixtos, que permiten modelar tanto los efectos individuales como los contextuales, y estimar la variabilidad entre niveles.\n\n\n\nNo todo se ajusta a un modelo lineal\n\n\n\n\n8.4.2 Modelos lineales generalizados\nPuede ser que, en principio, todos estos modelos (logística, Poisson, ordinal, multinomial) pueden parecer distintas herramientas. Pero en realidad, forman parte de una misma familia: los modelos lineales generalizados, o GLM por sus siglas en inglés (Generalized Linear Models). Todo siguen una misma lógica: todos estos modelos parten de la misma idea básica de la regresión lineal, pero relajan o adaptan dos cosas fundamentales:\n\nEl tipo de distribución que tiene la variable respuesta.\nLa función que conecta esa variable respuesta con la combinación lineal de predictores.\n\nEsta estructura común tiene tres componentes clave:\n\nUna variable respuesta \\(Y\\) que puede seguir distintas distribuciones: normal, binomial, Poisson, etc.\nUn predictor lineal: la parte en común \\(\\eta = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\dots\\)\nUna función de enlace (link function): que conecta el valor esperado de \\(Y\\) con el predictor lineal.\n\n\n\n\n\n\n\n\n\n\nTipo de regresión\nDistribución de \\(Y\\)\nEnlace (link)\nContexto común en ciencias sociales\n\n\n\n\nLineal\nNormal\nIdentidad \\(\\mu = \\eta\\)\nPredicción de ingresos, satisfacción, etc.\n\n\nLogística\nBinomial\nLogit \\(\\log(\\mu / (1-\\mu)) = \\eta\\)\nParticipación, afiliación, apoyo político\n\n\nPoisson\nPoisson\nLog \\(\\log(\\mu) = \\eta\\)\nConteo de eventos: marchas, reuniones, crímenes\n\n\nOrdinal logit\nCategórica ordinal\nLogit acumulado\nOpiniones, niveles de confianza o satisfacción\n\n\nMultinomial logit\nCategórica nominal\nMultilogit\nPreferencias electorales, elección de vivienda\n\n\n\nR nos permite ajustar todos estos modelos usando la misma función: glm(). Lo único que cambia es la familia que indicamos en el argumento family =.\n# Regresión logística \nglm(participa ~ edad + educacion, data = encuesta, family = binomial)\n\n# Regresión de Poisson \nglm(n_conflictos ~ pobreza + desempleo, data = regiones, family = poisson)\n\n# Regresión lineal clásica (\nglm(satisfaccion ~ ingreso + zona, data = datos, family = gaussian)\nDe hecho, muchas veces, cuando el modelo lineal clásico falla (por violaciones de normalidad, varianza desigual, o variable respuesta inapropiada), el camino no es abandonar la regresión, sino moverse dentro de la familia GLM hacia un modelo más adecuado.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "regr.html#resumen-del-capítulo",
    "href": "regr.html#resumen-del-capítulo",
    "title": "8  Regresión",
    "section": "8.5 Resumen del capítulo",
    "text": "8.5 Resumen del capítulo\nLa regresión es una herramienta estadística que permite modelar la relación entre una variable respuesta cuantitativa y una o más variables explicativas. Su forma más básica es la regresión lineal simple, que asume una relación lineal entre \\(x\\) e \\(y\\), expresada como \\(y = \\alpha + \\beta x\\). Los parámetros del modelo (\\(\\alpha\\) y \\(\\beta\\)) se estiman mediante el método de mínimos cuadrados ordinarios (OLS), que busca minimizar la suma de los errores al cuadrado.\nEl coeficiente \\(\\beta\\) representa el cambio promedio en \\(y\\) ante un cambio de una unidad en \\(x\\), mientras que \\(\\alpha\\) indica el valor esperado de \\(y\\) cuando \\(x = 0\\). La calidad del ajuste se evalúa con el coeficiente de determinación \\(R^2\\), que mide qué proporción de la variabilidad en \\(y\\) es explicada por el modelo. El error estándar residual (\\(\\sigma\\)) resume la dispersión de los datos en torno a la recta ajustada.\nLa inferencia sobre los coeficientes se realiza mediante pruebas \\(t\\), intervalos de confianza y valores p. Se parte de la hipótesis nula de que \\(\\beta = 0\\), es decir, que no hay relación lineal en la población. Para interpretar correctamente estos resultados, el modelo debe cumplir con ciertos supuestos: linealidad, independencia, homocedasticidad y normalidad de los errores. Estas condiciones se verifican mediante el análisis gráfico de residuos.\nLa regresión múltiple extiende el modelo lineal para incluir varias variables explicativas. Cada coeficiente estimado representa el efecto de una variable, manteniendo constante el resto. Se introdujo el uso de VIF para evaluar colinealidad entre predictores, y el \\(R^2\\) ajustado como medida más precisa del ajuste cuando se incluyen múltiples variables. Se explicó también cómo interpretar variables categóricas mediante codificación por referencia y cómo controlar por variables para evitar sesgos por omisión. Rcordar además a los modelos con interacción, donde el efecto de una variable puede depender del valor de otra. Estos modelos incluyen un término multiplicativo que modifica la pendiente, permitiendo estimar relaciones diferenciadas según grupos.\nHay una visión general de otros modelos de regresión más allá de la lineal: regresión logística para variables binarias, ordinal para escalas tipo Likert, multinomial para respuestas sin orden, Poisson para conteos, polinómica para relaciones curvas, y modelos multinivel para datos jerárquicos. Todos estos se agrupan dentro de la familia de los Modelos Lineales Generalizados (GLM), que permiten adaptar la regresión a diferentes tipos de distribución y estructura de datos. La elección del modelo debe hacerse en función del tipo de variable respuesta y de los supuestos que se puedan sostener en el análisis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "regr.html#ejercicios",
    "href": "regr.html#ejercicios",
    "title": "8  Regresión",
    "section": "8.6 Ejercicios",
    "text": "8.6 Ejercicios\n1. En un modelo de regresión lineal simple, ¿cuál de las siguientes afirmaciones describe correctamente el significado de la pendiente (\\(\\beta\\))?\n\nEs el valor promedio de la variable respuesta.\n\nEs el valor esperado de \\(y\\) cuando \\(x = 0\\).\n\nEs el cambio promedio en \\(y\\) por cada unidad adicional en \\(x\\).\n\nEs el punto donde la recta cruza el eje \\(x\\).\n\n2. En R, ¿cuál de estas fórmulas se utiliza para ajustar un modelo de regresión lineal múltiple? sin interación?\n\nlm(y = x1 + x2, data = df)\n\nlm(y ~ x1 + x2, data = df)\n\nlm(x1 + x2 ~ y, data = df)\n\nlm(y ~ x1 * x2 * x3, data = df)\n\n3. ¿Qué significa el término \\(\\hat{y}_i\\) en un modelo de regresión?\n\nEs el error cuadrático medio del modelo.\n\nEs el valor observado de la variable dependiente.\n\nEs el valor predicho por el modelo para la observación \\(i\\).\n\nEs el coeficiente estimado para la variable independiente \\(x_i\\).\n\n4. ¿Qué indica un coeficiente de determinación \\(R^2\\) igual a 0.67?\n\nEl modelo es significativo en un 67% de los casos.\n\nEl 67% de la varianza total de \\(y\\) es explicada por el modelo.\n\nEl modelo tiene un 67% de precisión para predecir nuevas observaciones.\n\nEl valor predicho es 0.67 veces el valor observado.\n\n5. En un modelo con interacción como lm(y ~ x * z), ¿qué significa el término x:z?\n\nEs el efecto total combinado de \\(x\\) y \\(z\\).\n\nEs el valor promedio del modelo cuando \\(x = z = 0\\).\n\nRepresenta el cambio en la pendiente de \\(x\\) dependiendo del valor de \\(z\\).\n\nEs el coeficiente de correlación entre \\(x\\) y \\(z\\).\n\n6. ¿Cuál de los siguientes supuestos es necesario para que los valores p del modelo lineal clásico sean confiables?\n\nQue las variables independientes estén normalizadas.\n\nQue los residuos del modelo sigan una distribución normal.\n\nQue todas las variables tengan media cero.\n\nQue \\(R^2\\) sea mayor a 0.5.\n\n7. ¿Qué observación es correcta sobre el uso del gráfico Residuals vs Fitted en regresión?\n\nPermite verificar si los residuos tienen varianza constante.\n\nPermite detectar si hay outliers que afectan el modelo.\n\nPermite evaluar si la relación entre \\(x\\) e \\(y\\) es aproximadamente lineal.\n\nPermite identificar si el modelo cumple con la distribución binomial.\n\n8. ¿Qué función usamos en R para evaluar la multicolinealidad entre variables explicativas?\n\nsummary()\n\ncor()\n\nvif()\n\nresid()\n\n9. Si el modelo lineal simple tiene un error estándar residual (\\(\\sigma\\)) de 3.25, ¿qué interpretación es más adecuada?\n\nEl valor estimado del intercepto es 3.25.\n\nEn promedio, los valores de \\(y\\) se desvían 3.25 unidades de la recta ajustada.\n\nEl valor de la pendiente tiene una significancia de 3.25.\n\n\\(R^2\\) debe ser mayor a 0.325 para ser aceptable.\n\n10. ¿Cuál de las siguientes afirmaciones representa mejor la lógica de “control estadístico” en regresión múltiple?\n\nEs usar una regresión para seleccionar solo la variable más significativa.\n\nEs estimar el efecto de una variable manteniendo constantes las demás.\n\nEs aplicar transformaciones logarítmicas para mejorar el modelo.\n\nEs usar solo variables independientes no correlacionadas.\n\n\n\n\n\nAgresti, Alan. 2018. Statistical methods for the social sciences. Fifth edition. Boston: Pearson.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "glosariofunciones.html",
    "href": "glosariofunciones.html",
    "title": "9  Glosario de funciones y operaciones útiles en R",
    "section": "",
    "text": "9.1 Vectores\nLos vectores son estructuras básicas en R que almacenan datos del mismo tipo.\nFunciones comunes\nmi_vector = c(1, 2, 3) # Vector inicial\nmi_vector = append(mi_vector, c(4, 5)) # Agrega los números 4 y 5 al final\nmi_vector\n\n[1] 1 2 3 4 5\nmi_vector = c(1, 2, 3, 4, 5) # Combina los valores 1, 2, 3, 4 y 5 en un vector\nmi_vector\n\n[1] 1 2 3 4 5\nmi_vector = c(1, 2, 4, 8, 16, 32)\ndiff(mi_vector)\n\n[1]  1  2  4  8 16\nmi_vector = c(1, 2, 3, 4, 5, 6)\nlength(mi_vector)\n\n[1] 6\nmi_vector = c(1, 2, 3, 4, 5)\nrev(mi_vector)\n\n[1] 5 4 3 2 1\nseq(from = 0, to = 50, by = 5)\n\n [1]  0  5 10 15 20 25 30 35 40 45 50\nmi_vector = c(3, 1, 4, 2, 5)\nsort(mi_vector) # Orden ascendente\n\n[1] 1 2 3 4 5\n\nsort(mi_vector, decreasing = TRUE) # Orden descendente\n\n[1] 5 4 3 2 1\nmi_vector = c(1, 1, 2, 3, 3, 4)\nunique(mi_vector)\n\n[1] 1 2 3 4\nOperaciones útiles\nvector1 = c(1, 2, 3)\nvector2 = c(4, 5)\nc(vector1, vector2)\n\n[1] 1 2 3 4 5\nrep(1:3, times = 2)  # Repite la secuencia completa 1, 2, 3 dos veces\n\n[1] 1 2 3 1 2 3\n\nrep(1:3, each = 2)   # Repite cada elemento (1, 2, 3) dos veces\n\n[1] 1 1 2 2 3 3\nsample(1:10, size = 5)  # Selecciona 5 números aleatorios del 1 al 10\n\n[1] 5 7 8 6 3\n\nsample(1:10, size = 5, replace = TRUE)  # Permite que los valores se repitan\n\n[1] 9 7 4 5 7\nvector1 = c(1, 2, 3)\nvector2 = c(4, 5, 6)\nvector1 + vector2  \n\n[1] 5 7 9\nvector1 = c(2, 4, 6)\nvector2 = c(1, 3, 5)\nvector1 * vector2  \n\n[1]  2 12 30\nmi_vector = c(10, 20, 30, 40, 50)\n\n# ¿Cuáles elementos son mayores a 25?\nmi_vector &gt; 25  \n\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n\n# ¿Cuántos elementos son mayores a 25?\nsum(mi_vector &gt; 25) \n\n[1] 3\n\n# Seleccionar elementos mayores a 25\nmi_vector[mi_vector &gt; 25]  \n\n[1] 30 40 50",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Glosario de funciones y operaciones útiles en R</span>"
    ]
  },
  {
    "objectID": "glosariofunciones.html#vectores",
    "href": "glosariofunciones.html#vectores",
    "title": "9  Glosario de funciones y operaciones útiles en R",
    "section": "",
    "text": "append: Agrega elementos al final de un vector. Utiliza esta función para añadir uno o más elementos a un vector existente.\n\n\n\nc: Combina valores para crear un vector. Es la forma más sencilla de crear un vector.\n\n\n\ndiff: Calcula las diferencias entre elementos consecutivos. Devuelve un nuevo vector con la diferencia entre cada par de elementos consecutivos.\n\n\n\nlength: Obtiene la longitud de un vector. Te permite conocer cuántos elementos contiene el vector.\n\n\n\nrev: Invierte el orden de los elementos de un vector. Útil si necesitas trabajar con los elementos en orden inverso.\n\n\n\nseq: Genera una secuencia de números. Permite definir el número inicial (from), el número final (to) y el tamaño del paso (by).\n\n\n\nsort: Ordena un vector. Puedes ordenar un vector en orden ascendente (por defecto) o descendente utilizando el argumento decreasing.\n\n\n\nunique: Devuelve los valores únicos de un vector. Elimina duplicados y muestra solo los valores distintos.\n\n\n\n\nConcatenación de vectores\nSe pueden combinar múltiples vectores utilizando c.\n\n\n\nRepetición de valores\nUsa rep para repetir elementos de un vector. Hay dos opciones principales:\n\n\ntimes: Repite toda la secuencia el número de veces indicado.\neach: Repite cada elemento de la secuencia el número de veces indicado.\n\n\n\nGeneración de números aleatorios\nUsa sample para obtener valores aleatorios de un vector.\n\n\nsize: Especifica cuántos valores quieres seleccionar.\nreplace: Indica si los valores pueden repetirse (TRUE) o no (FALSE).\n\n\n\nSuma de vectores elemento a elemento\nSi los vectores tienen la misma longitud, los elementos correspondientes se suman automáticamente.\n\n\n\nMultiplicación elemento a elemento\nSimilar a la suma, pero multiplica los elementos correspondientes.\n\n\n\nOperaciones lógicas\nSe aplican condiciones a los elementos del vector, devolviendo un vector lógico (TRUE o FALSE) para cada elemento. Además, puedes:\n\n\nContar cuántos elementos cumplen la condición usando sum().\nSeleccionar elementos que cumplen la condición usando [].",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Glosario de funciones y operaciones útiles en R</span>"
    ]
  },
  {
    "objectID": "glosariofunciones.html#data.frames",
    "href": "glosariofunciones.html#data.frames",
    "title": "9  Glosario de funciones y operaciones útiles en R",
    "section": "9.2 Data.frames",
    "text": "9.2 Data.frames\nLos data frames son estructuras de datos bidimensionales en R, similares a tablas, donde cada columna puede contener un tipo diferente de datos (numéricos, caracteres, etc.).\nCreación y visualización\n\nCrear un data frame\nUsa la función data.frame() para crear un data frame combinando vectores como columnas.\n\n\nmi_dataframe = data.frame(\n  Nombre = c(\"Ana\", \"Luis\", \"Pedro\", \"Marco\", \"Fernanda\", \"Felix\", \"Diego\"),\n  Edad = c(23, 36, 35, 27, 34, 24, 41),\n  Ciudad = c(\"Ica\", \"Huaraz\", \"Cusco\", \"Cusco\", \"Lima\", \"Iquitos\", \"Huaraz\")\n)\nmi_dataframe\n\n    Nombre Edad  Ciudad\n1      Ana   23     Ica\n2     Luis   36  Huaraz\n3    Pedro   35   Cusco\n4    Marco   27   Cusco\n5 Fernanda   34    Lima\n6    Felix   24 Iquitos\n7    Diego   41  Huaraz\n\n\n\nVisualizar las primeras y últimas filas\n\n\nhead(): Muestra las primeras filas (por defecto 6).\ntail(): Muestra las últimas filas (por defecto 6).\n\n\nhead(mi_dataframe)\n\n    Nombre Edad  Ciudad\n1      Ana   23     Ica\n2     Luis   36  Huaraz\n3    Pedro   35   Cusco\n4    Marco   27   Cusco\n5 Fernanda   34    Lima\n6    Felix   24 Iquitos\n\ntail(mi_dataframe, 2)\n\n  Nombre Edad  Ciudad\n6  Felix   24 Iquitos\n7  Diego   41  Huaraz\n\n\n\nObtener la estructura del data frame\n\n\nstr(): Muestra la estructura del data frame.\nsummary(): Proporciona estadísticas descriptivas básicas para cada columna.\n\n\nstr(mi_dataframe)\n\n'data.frame':   7 obs. of  3 variables:\n $ Nombre: chr  \"Ana\" \"Luis\" \"Pedro\" \"Marco\" ...\n $ Edad  : num  23 36 35 27 34 24 41\n $ Ciudad: chr  \"Ica\" \"Huaraz\" \"Cusco\" \"Cusco\" ...\n\nsummary(mi_dataframe)\n\n    Nombre               Edad          Ciudad         \n Length:7           Min.   :23.00   Length:7          \n Class :character   1st Qu.:25.50   Class :character  \n Mode  :character   Median :34.00   Mode  :character  \n                    Mean   :31.43                     \n                    3rd Qu.:35.50                     \n                    Max.   :41.00                     \n\n\nAcceso y selección de datos\n\nAcceder a una columna Puedes acceder a una columna específica usando $ o corchetes ([ ]).\n\n\nmi_dataframe$Nombre  # Accede a la columna \"Nombre\"\n\n[1] \"Ana\"      \"Luis\"     \"Pedro\"    \"Marco\"    \"Fernanda\" \"Felix\"    \"Diego\"   \n\nmi_dataframe[[\"Edad\"]]  # Otra forma de acceder a la columna \"Edad\"\n\n[1] 23 36 35 27 34 24 41\n\n\n\nAcceder a filas específicas Usa índices de filas con corchetes ([fila, ]).\n\n\n# Devuelve la segunda fila\nmi_dataframe[2, ]  \n\n  Nombre Edad Ciudad\n2   Luis   36 Huaraz\n\n\n\nAcceder a celdas específicas Usa índices de filas y columnas con corchetes ([fila, columna]).\n\n\n# Devuelve el valor en la fila 2 y columna 3\nmi_dataframe[2, 3]  # Devuelve el valor en la fila 2 y columna 3\n\n[1] \"Huaraz\"\n\n\n\nSeleccionar múltiples columnas o filas Usa índices o nombres con corchetes.\n\n\n# Selecciona columnas por nombre\nmi_dataframe[, c(\"Nombre\", \"Ciudad\")]  \n\n    Nombre  Ciudad\n1      Ana     Ica\n2     Luis  Huaraz\n3    Pedro   Cusco\n4    Marco   Cusco\n5 Fernanda    Lima\n6    Felix Iquitos\n7    Diego  Huaraz\n\n # Selecciona las primeras dos filas\nmi_dataframe[1:2, ] \n\n  Nombre Edad Ciudad\n1    Ana   23    Ica\n2   Luis   36 Huaraz\n\n\nFiltrado de datos\n\nFiltrar filas por condiciones Puedes usar operadores lógicos para filtrar filas.\n\n\n# Filtra filas donde Edad &gt; 25\nmi_dataframe[mi_dataframe$Edad &gt; 25, ]  \n\n    Nombre Edad Ciudad\n2     Luis   36 Huaraz\n3    Pedro   35  Cusco\n4    Marco   27  Cusco\n5 Fernanda   34   Lima\n7    Diego   41 Huaraz\n\n\n\nFiltrar filas usando %in% Filtra filas en base a si un valor pertenece a un conjunto.\n\n\nmi_dataframe[mi_dataframe$Ciudad %in% c(\"Cusco\", \"Lima\"), ]\n\n    Nombre Edad Ciudad\n3    Pedro   35  Cusco\n4    Marco   27  Cusco\n5 Fernanda   34   Lima\n\n\nEdición y manipulación\n\nAñadir una nueva columna Crea una nueva columna asignando valores directamente.\n\n\n# Añade una columna \"Salario\"\nmi_dataframe$Salario = c(1200, 2340, 1300, 2250, \n                         3000, 2600, 1850) \nmi_dataframe\n\n    Nombre Edad  Ciudad Salario\n1      Ana   23     Ica    1200\n2     Luis   36  Huaraz    2340\n3    Pedro   35   Cusco    1300\n4    Marco   27   Cusco    2250\n5 Fernanda   34    Lima    3000\n6    Felix   24 Iquitos    2600\n7    Diego   41  Huaraz    1850\n\n\n\nEliminar una columna Usa la función NULL para eliminar una columna.\n\n\n# Elimina la columna \"Salario\"\nmi_dataframe$Salario = NULL  \nmi_dataframe\n\n    Nombre Edad  Ciudad\n1      Ana   23     Ica\n2     Luis   36  Huaraz\n3    Pedro   35   Cusco\n4    Marco   27   Cusco\n5 Fernanda   34    Lima\n6    Felix   24 Iquitos\n7    Diego   41  Huaraz\n\n\n\nRenombrar columnas Modifica los nombres de las columnas utilizando colnames().\n\n\n# Cambia los nombres de las columnas\ncolnames(mi_dataframe) = c(\"Nombre\", \"Edad\", \"Vivienda\")  \nmi_dataframe\n\n    Nombre Edad Vivienda\n1      Ana   23      Ica\n2     Luis   36   Huaraz\n3    Pedro   35    Cusco\n4    Marco   27    Cusco\n5 Fernanda   34     Lima\n6    Felix   24  Iquitos\n7    Diego   41   Huaraz\n\n\n\nOrdenar el data frame Usa la función order() para ordenar las filas según una columna.\n\n\n# Ordena por la columna \"Edad\"\nmi_dataframe = mi_dataframe[order(mi_dataframe$Edad), ] \n\nmi_dataframe\n\n    Nombre Edad Vivienda\n1      Ana   23      Ica\n6    Felix   24  Iquitos\n4    Marco   27    Cusco\n5 Fernanda   34     Lima\n3    Pedro   35    Cusco\n2     Luis   36   Huaraz\n7    Diego   41   Huaraz\n\n\n\nCombinar data frames\n\n\nrbind(): Combina data frames por filas.\ncbind(): Combina data frames por columnas.\n\n\n# Combinar filas\notro_dataframe = data.frame(Nombre = \"Oscar\", Edad = 28, Vivienda = \"Lima\")\nmi_dataframe = rbind(mi_dataframe, otro_dataframe)\nmi_dataframe\n\n     Nombre Edad Vivienda\n1       Ana   23      Ica\n6     Felix   24  Iquitos\n4     Marco   27    Cusco\n5  Fernanda   34     Lima\n3     Pedro   35    Cusco\n2      Luis   36   Huaraz\n7     Diego   41   Huaraz\n11    Oscar   28     Lima\n\n# Combinar columnas\nnueva_columna = data.frame(Salario = c(1200, 2340, 1300, 2250, \n                         3000, 2600, 1850, 2300))\nmi_dataframe = cbind(mi_dataframe, nueva_columna)\nmi_dataframe\n\n     Nombre Edad Vivienda Salario\n1       Ana   23      Ica    1200\n6     Felix   24  Iquitos    2340\n4     Marco   27    Cusco    1300\n5  Fernanda   34     Lima    2250\n3     Pedro   35    Cusco    3000\n2      Luis   36   Huaraz    2600\n7     Diego   41   Huaraz    1850\n11    Oscar   28     Lima    2300\n\n\n\nEliminar filas Usa índices negativos para eliminar filas.\n\n\n# Elimina la segunda fila\nmi_dataframe = mi_dataframe[-2, ] \n\nmi_dataframe\n\n     Nombre Edad Vivienda Salario\n1       Ana   23      Ica    1200\n4     Marco   27    Cusco    1300\n5  Fernanda   34     Lima    2250\n3     Pedro   35    Cusco    3000\n2      Luis   36   Huaraz    2600\n7     Diego   41   Huaraz    1850\n11    Oscar   28     Lima    2300\n\n\nResúmenes y cálculos\n\nObtener valores únicos Usa unique() para obtener valores únicos en una columna.\n\n\nunique(mi_dataframe$Vivienda)\n\n[1] \"Ica\"    \"Cusco\"  \"Lima\"   \"Huaraz\"\n\n\n\nContar filas y columnas Usa las funciones nrow() y ncol().\n\n\n# Número de filas\nnrow(mi_dataframe)  \n\n[1] 7\n\n# Número de columnas\nncol(mi_dataframe)  \n\n[1] 4\n\n\n\nCalcular estadísticas básicas Aplica funciones como mean(), sum(), min(), max() a columnas numéricas.\n\n\n# Edad promedio\nmean(mi_dataframe$Edad)  \n\n[1] 32\n\n# Suma total de las edades\nsum(mi_dataframe$Edad)   \n\n[1] 224\n\n\n\nTablas de frecuencias Usa table() para crear una tabla de frecuencias.\n\n\ntable(mi_dataframe$Vivienda)  # Frecuencia de valores en la columna \"Ciudad\"\n\n\n Cusco Huaraz    Ica   Lima \n     2      2      1      2 \n\n\nA partir de ahora trabajaremos con una encuesta ficticia diseñada para analizar la relación entre edad, ingreso, nivel educativo y satisfacción personal. Contiene información sobre 500 personas.\n\nlibrary(readr)\nencuesta = read_csv('glosario.csv')",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Glosario de funciones y operaciones útiles en R</span>"
    ]
  },
  {
    "objectID": "glosariofunciones.html#limpieza-y-reestructuración-de-datos",
    "href": "glosariofunciones.html#limpieza-y-reestructuración-de-datos",
    "title": "9  Glosario de funciones y operaciones útiles en R",
    "section": "9.3 Limpieza y reestructuración de datos",
    "text": "9.3 Limpieza y reestructuración de datos\n\nlibrary(tidyverse)\n\nLa limpieza de datos es un paso fundamental en todo análisis cuantitativo, ya que garantiza la calidad, consistencia y fiabilidad de los resultados. Errores como nombres mal escritos, valores faltantes, codificaciones inconsistentes o estructuras duplicadas pueden distorsionar el análisis y generar conclusiones erróneas. T\nTomaremos como punto de referencia un dataset con múltiples problemas intencionados, el cual servirá como ejemplo para aplicar herramientas como janitor, stringr, naniar y tidyr, de gran ayuda para transformar datos sucios en insumos analíticos ordenados y listos para modelar o visualizar.\n\ndf = read_csv('limpieza.csv')\n\n\nhead(df, 12)\n\n# A tibble: 12 × 9\n   `Nombre completo`  Edad sexo  ingreso_mensual region encuesta_fecha puntaje_1\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;\n 1 Ana Perez            29 feme…          1500   lima   2025-04-11             5\n 2 Sofia_Ramos          31 F                 2   Lima   2025-04-11             3\n 3 mario vargas         23 masc…          1500   Arequ… 2025/04/12            NA\n 4 CARLOS DIAZ          NA F              3000   Cusco  2025/04/10            NA\n 5 M@rio V@rg@s         NA masc…          3000   LIMA   10-04-2025             1\n 6 ana perez            31 masc…             1.5 Cusco  2025/04/10             3\n 7 Sofi@ R@mos          23 nan               2   LIMA   10-04-2025            NA\n 8 ANA PEREZ            23 F                 1.5 AREQU… &lt;NA&gt;                  NA\n 9 LUIS GÓMEZ           40 F                NA   lima   2025-04-11             1\n10 An@ Perez            29 nan            3000   Arequ… 2025/04/10             5\n11 An@ Perez            35 masc…            NA   LIMA   2025-04-11             3\n12 Ana Perez            23 F                 2   Lima   2025-04-11             4\n# ℹ 2 more variables: puntaje_2 &lt;dbl&gt;, puntaje_3 &lt;dbl&gt;\n\n\n\nLimpieza de nombre de columnas\n\n\ncolnames(df)\n\n[1] \"Nombre completo\" \"Edad\"            \"sexo\"            \"ingreso_mensual\"\n[5] \"region\"          \"encuesta_fecha\"  \"puntaje_1\"       \"puntaje_2\"      \n[9] \"puntaje_3\"      \n\n\n\nlibrary(janitor)\n\n\ndf = df %&gt;% \n  clean_names()\n\n\ncolnames(df)\n\n[1] \"nombre_completo\" \"edad\"            \"sexo\"            \"ingreso_mensual\"\n[5] \"region\"          \"encuesta_fecha\"  \"puntaje_1\"       \"puntaje_2\"      \n[9] \"puntaje_3\"      \n\n\n\nLimpieza de strings y numericos\n\nLos errores tipográficos, los formatos inconsistentes y las codificaciones irregulares son comunes en datos reales, especialmente en variables de texto como nombres o categorías como el sexo.\nLimpieza de nombre_completo\nPrimero, cargamos el paquete stringr, un paquete dedicado a trabajar con cadenas de texto.\n\nlibrary(tidyverse)\nlibrary(stringr)\n\nLuego, realizamos una transformación paso a paso de la variable nombre_completo para corregir errores comunes, estandarizar escritura y eliminar inconsistencias:\n\ndf = df %&gt;%\n  mutate(\n    # Paso 1: Pasamos todo a minúsculas para evitar duplicidades por mayúsculas\n    nombre_completo = str_to_lower(nombre_completo),\n\n    # Paso 2: Reemplazamos caracteres con tilde o especiales por sus equivalentes planos\n    nombre_completo = str_replace_all(nombre_completo, \"á\", \"a\"),\n    nombre_completo = str_replace_all(nombre_completo, \"é\", \"e\"),\n    nombre_completo = str_replace_all(nombre_completo, \"í\", \"i\"),\n    nombre_completo = str_replace_all(nombre_completo, \"ó\", \"o\"),\n    nombre_completo = str_replace_all(nombre_completo, \"ú\", \"u\"),\n    nombre_completo = str_replace_all(nombre_completo, \"ñ\", \"n\"),\n\n    # Paso 3: Eliminamos símbolos no deseados como @, guiones bajos, espacios múltiples\n    nombre_completo = str_replace_all(nombre_completo, \"@\", \"a\"),\n    nombre_completo = str_replace_all(nombre_completo, \"_\", \" \"),\n    nombre_completo = str_replace_all(nombre_completo, \"[[:space:]]+\", \" \"),\n    nombre_completo = str_trim(nombre_completo),  # elimina espacios al inicio o final\n\n    # Paso 4: Correcciones específicas a errores conocidos por observación del dataset\n    nombre_completo = case_when(\n      str_detect(nombre_completo, \"sofiaramos\") ~ \"sofia ramos\",\n      str_detect(nombre_completo, \"anaperez\") ~ \"ana perez\",\n      str_detect(nombre_completo, \"luciahuerta\") ~ \"lucia huerta\",\n      str_detect(nombre_completo, \"mariovargas\") ~ \"mario vargas\",\n      str_detect(nombre_completo, \"carlosdiaz\") ~ \"carlos diaz\",\n      str_detect(nombre_completo, \"luisgomez\") ~ \"luis gomez\",\n      TRUE ~ nombre_completo\n    ),\n\n    # Paso 5: Finalizamos capitalizando la primera letra de cada palabra \n    nombre_completo = str_to_title(nombre_completo)\n  )\n\nCon estas transformaciones, se evita que registros iguales aparezcan como diferentes por errores de digitación, acentos o diferencias en capitalización.\nHomogeneización de la variable sexo\nEn variables categóricas como sexo, donde pueden coexistir múltiples formas de referirse a una misma categoría (“masculino”, “m”, “hombre”, etc.), es fundamental normalizar los valores. Aplicamos una transformación similar:\n\ndf = df %&gt;%\n  mutate(\n    # Paso 1: Todo a minúsculas\n    sexo = tolower(sexo),\n\n    # Paso 2: Reescribimos variantes en etiquetas estándar\n    sexo = case_when(\n      sexo %in% c(\"masculino\", \"m\", \"hombre\") ~ \"masculino\",\n      sexo %in% c(\"femenino\", \"f\", \"mujer\") ~ \"femenino\",\n      TRUE ~ NA_character_  # valores irreconocibles se dejan como NA\n    )\n  )\n\nEn este punto, la base ya está considerablemente más limpia y lista para ser usada con métodos estadísticos o gráficos.\n\nhead(df)\n\n# A tibble: 6 × 9\n  nombre_completo  edad sexo     ingreso_mensual region encuesta_fecha puntaje_1\n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;\n1 Ana Perez          29 femenino          1500   lima   2025-04-11             5\n2 Sofia Ramos        31 femenino             2   Lima   2025-04-11             3\n3 Mario Vargas       23 masculi…          1500   Arequ… 2025/04/12            NA\n4 Carlos Diaz        NA femenino          3000   Cusco  2025/04/10            NA\n5 Mario Vargas       NA masculi…          3000   LIMA   10-04-2025             1\n6 Ana Perez          31 masculi…             1.5 Cusco  2025/04/10             3\n# ℹ 2 more variables: puntaje_2 &lt;dbl&gt;, puntaje_3 &lt;dbl&gt;\n\n\n\nValores perdidos\n\nEn el análisis de datos reales, lidiar con valores perdidos es inevitable. Ya sea por errores en la recolección, datos incompletos o fallas de ingreso, siempre es necesario diagnosticar y decidir cómo abordarlos.\nEl paquete naniar facilita la visualización y exploración de patrones de NA en un dataset. Permite entender en qué columnas se concentran los valores faltantes y si hay patrones comunes entre ellos.\n\nlibrary(naniar)\n\nUna forma rápida de visualizar cuántos NA hay por variable es:\n\ngg_miss_var(df)\n\n\n\n\n\n\n\n\nY si deseamos identificar observaciones con múltiples valores perdidos, usamos:\n\ngg_miss_case(df)\n\n\n\n\n\n\n\n\nEsto nos ayuda a decidir si queremos eliminar observaciones o columnas con muchos datos ausentes o proceder a imputarlos. A veces basta con eliminar observaciones incompletas si son pocas o si el análisis requiere datos completos. tidyr ofrece funciones útiles como drop_na():\nEliminar observaciones con NA en columnas clave (por ejemplo, nombre o sexo):\n\ndf = df %&gt;%\n  drop_na(nombre_completo, sexo, encuesta_fecha) \n\ndim(df)\n\n[1] 67  9\n\n\nEliminar observaciones con al menos un NA en cualquier columna:\n\ndf %&gt;%\n  drop_na() %&gt;% \n  dim()\n\n[1] 32  9\n\n\nEsto reduce el tamaño de la base pero garantiza integridad completa. Sin embargo, no siempre es deseable perder datos. En esos casos, se puede imputar.\nEl paquete mice (Multivariate Imputation by Chained Equations) permite realizar una imputación más inteligente, modelando los valores ausentes con base en otras variables.\n\nlibrary(mice)\n\nEste paquete nos permite aplicar un algoritmo de imputación. Por ejemplo, para una imputación rápida:\n\nimputado = mice(df, m = 1, method = \"pmm\", seed = 123)\n\n\n iter imp variable\n  1   1  edad  ingreso_mensual  puntaje_1  puntaje_2  puntaje_3\n  2   1  edad  ingreso_mensual  puntaje_1  puntaje_2  puntaje_3\n  3   1  edad  ingreso_mensual  puntaje_1  puntaje_2  puntaje_3\n  4   1  edad  ingreso_mensual  puntaje_1  puntaje_2  puntaje_3\n  5   1  edad  ingreso_mensual  puntaje_1  puntaje_2  puntaje_3\n\n\n\nm = 1: número de datasets imputados. Se puede aumentar si se desea evaluar incertidumbre.\nmethod = \"pmm\": predictive mean matching, útil para imputar variables numéricas.\nseed: asegura que los resultados sean reproducibles.\n\nUna vez realizado el proceso, se extrae el dataset completo con:\n\ndf = complete(imputado)\n\nCon esto, se preservan todas las observaciones, pero con los valores faltantes sustituidos por predicciones consistentes con la estructura de los datos. Este método es especialmente útil cuando hay varias variables correlacionadas o cuando perder datos no es una opción que podamos costear.\n\ngg_miss_var(df)\n\n\n\n\n\n\n\n\n\nhead(df)\n\n  nombre_completo edad      sexo ingreso_mensual   region encuesta_fecha\n1       Ana Perez   29  femenino          1500.0     lima     2025-04-11\n2     Sofia Ramos   31  femenino             2.0     Lima     2025-04-11\n3    Mario Vargas   23 masculino          1500.0 Arequipa     2025/04/12\n4     Carlos Diaz   23  femenino          3000.0    Cusco     2025/04/10\n5    Mario Vargas   29 masculino          3000.0     LIMA     10-04-2025\n6       Ana Perez   31 masculino             1.5    Cusco     2025/04/10\n  puntaje_1 puntaje_2 puntaje_3\n1         5         2         2\n2         3         3         3\n3         3         2         5\n4         5         4         1\n5         1         4         4\n6         3         5         5\n\n\n\nObservaciones duplicados\n\nLos duplicados pueden surgir por errores en la recolección, registros múltiples del mismo individuo o procesos de integración de bases de datos. Si no se detectan y corrigen, pueden sesgar promedios, inflar conteos y distorsionar cualquier análisis posterior.\nLa forma más directa de detectar duplicados es verificar si existen filas exactamente iguales en todas las columnas:\n\ndf %&gt;% \n  duplicated() %&gt;% \n  sum()\n\n[1] 5\n\n\nSi se desea ver cuáles son esos duplicados exactos:\n\ndf %&gt;% \n  filter(duplicated(.))\n\n  nombre_completo edad      sexo ingreso_mensual   region encuesta_fecha\n1     Sofia Ramos   29  femenino          3000.0     LIMA     2025/04/10\n2     Carlos Diaz   29  femenino          1500.0    Cuzco     10-04-2025\n3    Mario Vargas   31 masculino             1.5     Lima     10-04-2025\n4    Mario Vargas   23 masculino          1500.0 Arequipa     2025/04/12\n5       Ana Perez   31 masculino             1.5    Cusco     2025/04/10\n  puntaje_1 puntaje_2 puntaje_3\n1         5         2         1\n2         5         1         1\n3         4         2         2\n4         3         2         5\n5         3         5         5\n\n\nPodemos eliminarlos con distinct(), que retiene únicamente las filas únicas:\n\ndf = df %&gt;% \n  distinct()\n\nLo que nos deja con:\n\ndf %&gt;% \n  duplicated() %&gt;% \n  sum()\n\n[1] 0\n\n\n\nTidyr\n\nUna vez corregidos errores en las variables categóricas y estandarizados los valores faltantes o inconsistentes, es común encontrarse con problemas estructurales. Algunas columnas pueden estar duplicando información, otras pueden combinar múltiples variables en una sola. Aquí es donde entra en juego el paquete tidyr.\nEste paquete permite reorganizar los datos sin alterar su contenido: separar, unir, pivotear, rellenar o completar estructuras incompletas. Por ejemplo:\nSeparar: descomponer nombre_completo en nombre y apellido\n\nlibrary(tidyr)\n\n# Separar nuevamente desde nombre_completo para asegurar consistencia\ndf = df %&gt;%\n  separate(nombre_completo, into = c(\"nombre\", \"apellido\"), sep = \" \", extra = \"merge\", remove = FALSE)\n\n\nsep = \" \": se separa por el primer espacio en blanco.\nextra = \"merge\": en caso haya más de dos componentes (ej. “Maria del Carmen”), los combina en apellido.\nremove = FALSE: conservamos la columna original por trazabilidad.\n\n\nhead(df)\n\n  nombre_completo nombre apellido edad      sexo ingreso_mensual   region\n1       Ana Perez    Ana    Perez   29  femenino          1500.0     lima\n2     Sofia Ramos  Sofia    Ramos   31  femenino             2.0     Lima\n3    Mario Vargas  Mario   Vargas   23 masculino          1500.0 Arequipa\n4     Carlos Diaz Carlos     Diaz   23  femenino          3000.0    Cusco\n5    Mario Vargas  Mario   Vargas   29 masculino          3000.0     LIMA\n6       Ana Perez    Ana    Perez   31 masculino             1.5    Cusco\n  encuesta_fecha puntaje_1 puntaje_2 puntaje_3\n1     2025-04-11         5         2         2\n2     2025-04-11         3         3         3\n3     2025/04/12         3         2         5\n4     2025/04/10         5         4         1\n5     10-04-2025         1         4         4\n6     2025/04/10         3         5         5\n\n\nUnir: combinar puntaje_1, puntaje_2 y puntaje_3 en una sola columna\nEn análisis posteriores puede ser útil pivotear los puntajes para analizar cada respuesta como una fila individual. Esto facilita la creación de gráficos o modelos que trabajen sobre respuestas individuales:\n\ndf = df %&gt;%\n  # Sumamos todas las que empiezan con 'puntaje'\n  pivot_longer(cols = starts_with(\"puntaje_\"),\n               names_to = \"pregunta\",\n               values_to = \"puntaje\")\n\nAhora cada persona aparece tres veces (una por pregunta), lo cual permite comparar puntuaciones medias entre preguntas o agrupar por categorías ahora que tenemos la variable categórica pregunta.\nAsí quedo nuestro dataset luego de la limpieza\n\nhead(df, 12)\n\n# A tibble: 12 × 10\n   nombre_completo nombre apellido  edad sexo      ingreso_mensual region  \n   &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;   \n 1 Ana Perez       Ana    Perez       29 femenino             1500 lima    \n 2 Ana Perez       Ana    Perez       29 femenino             1500 lima    \n 3 Ana Perez       Ana    Perez       29 femenino             1500 lima    \n 4 Sofia Ramos     Sofia  Ramos       31 femenino                2 Lima    \n 5 Sofia Ramos     Sofia  Ramos       31 femenino                2 Lima    \n 6 Sofia Ramos     Sofia  Ramos       31 femenino                2 Lima    \n 7 Mario Vargas    Mario  Vargas      23 masculino            1500 Arequipa\n 8 Mario Vargas    Mario  Vargas      23 masculino            1500 Arequipa\n 9 Mario Vargas    Mario  Vargas      23 masculino            1500 Arequipa\n10 Carlos Diaz     Carlos Diaz        23 femenino             3000 Cusco   \n11 Carlos Diaz     Carlos Diaz        23 femenino             3000 Cusco   \n12 Carlos Diaz     Carlos Diaz        23 femenino             3000 Cusco   \n# ℹ 3 more variables: encuesta_fecha &lt;chr&gt;, pregunta &lt;chr&gt;, puntaje &lt;dbl&gt;",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Glosario de funciones y operaciones útiles en R</span>"
    ]
  },
  {
    "objectID": "glosariofunciones.html#dplyr-para-manipulacion-de-datos",
    "href": "glosariofunciones.html#dplyr-para-manipulacion-de-datos",
    "title": "9  Glosario de funciones y operaciones útiles en R",
    "section": "9.4 Dplyr para manipulacion de datos",
    "text": "9.4 Dplyr para manipulacion de datos\nEl paquete dplyr es uno de los más utilizados en R para manipulación y transformación de datos. El operador %&gt;% (pipe) es una herramienta central que permite encadenar funciones, pasando el resultado de una función como entrada a la siguiente.\n\nfilter: Filtrar filas según una condición\n\nEsta función selecciona solo las filas que cumplen con una o más condiciones.\n\n# Filtrar encuestados mayores de 30 años\nencuesta %&gt;%\n  filter(Edad &gt; 30) %&gt;% \n  head(10)\n\n# A tibble: 10 × 6\n      ID  Edad Genero Ingreso Educacion     Satisfaccion\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1     2    33 M         2595 Secundaria               3\n 2     3    51 F         3806 Secundaria               3\n 3     4    36 M         2907 Universitaria            3\n 4     5    36 M         2707 Secundaria               4\n 5     6    52 F         3540 Universitaria            5\n 6     7    40 M         3506 Secundaria               4\n 7    10    31 F         2452 Secundaria               3\n 8    11    47 F         3620 Universitaria            3\n 9    12    39 F         3258 Universitaria            2\n10    13    39 F         3258 Universitaria            4\n\n\n\n# Filtrar mujeres con educación universitaria\nencuesta %&gt;%\n  filter(Genero == \"F\", Educacion == \"Universitaria\") %&gt;% \n  head(10)\n\n# A tibble: 10 × 6\n      ID  Edad Genero Ingreso Educacion     Satisfaccion\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1     6    52 F         3540 Universitaria            5\n 2    11    47 F         3620 Universitaria            3\n 3    12    39 F         3258 Universitaria            2\n 4    13    39 F         3258 Universitaria            4\n 5    20    30 F         3244 Universitaria            3\n 6    24    28 F         1591 Universitaria            2\n 7    28    37 F         2711 Universitaria            2\n 8    37    41 F         3449 Universitaria            3\n 9    47    31 F         2744 Universitaria            3\n10    58    41 F         4131 Universitaria            3\n\n\n\nselect: Seleccionar columnas específicas\n\nEsta función te permite elegir las columnas necesarias.\n\n# Seleccionar columnas relevantes\nencuesta %&gt;%\n  select(ID, Genero, Satisfaccion) %&gt;% \n  head(10)\n\n# A tibble: 10 × 3\n      ID Genero Satisfaccion\n   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     1 F                 3\n 2     2 M                 3\n 3     3 F                 3\n 4     4 M                 3\n 5     5 M                 4\n 6     6 F                 5\n 7     7 M                 4\n 8     8 F                 2\n 9     9 F                 3\n10    10 F                 3\n\n\n\nmutate: Crear o modificar columnas\n\nAñade nuevas columnas o modifica las existentes.\n\n# Crear una columna que clasifique a las personas como \"Joven\" o \"Adulto\"\nencuesta %&gt;%\n# Personas menores o iguales a 30 como \"Jovenes\"\n  mutate(RangoEdad = \n           ifelse(Edad &lt;= 30, \"Joven\", \"Adulto\")) %&gt;% \n  head(10)\n\n# A tibble: 10 × 7\n      ID  Edad Genero Ingreso Educacion     Satisfaccion RangoEdad\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;    \n 1     1    29 F         3219 Primaria                 3 Joven    \n 2     2    33 M         2595 Secundaria               3 Adulto   \n 3     3    51 F         3806 Secundaria               3 Adulto   \n 4     4    36 M         2907 Universitaria            3 Adulto   \n 5     5    36 M         2707 Secundaria               4 Adulto   \n 6     6    52 F         3540 Universitaria            5 Adulto   \n 7     7    40 M         3506 Secundaria               4 Adulto   \n 8     8    22 F         1999 Secundaria               2 Joven    \n 9     9    28 F         1381 Primaria                 3 Joven    \n10    10    31 F         2452 Secundaria               3 Adulto   \n\n\n\narrange: Ordenar filas\n\nOrdena las filas del data frame según una o más columnas.\n\n# Ordenar por edad de menor a mayor\nencuesta %&gt;%\n  arrange(Edad) %&gt;% \n  head(10)\n\n# A tibble: 10 × 6\n      ID  Edad Genero Ingreso Educacion     Satisfaccion\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1    18    18 M         1788 Secundaria               3\n 2    26    18 F         2839 Primaria                 3\n 3    72    18 M         2271 Secundaria               2\n 4   108    18 M         1725 Universitaria            3\n 5   135    18 M         2171 Universitaria            2\n 6   268    18 M         1269 Universitaria            2\n 7   281    18 F         1302 Universitaria            2\n 8   307    18 F         1497 Universitaria            2\n 9   313    18 M         2458 Secundaria               3\n10   336    18 F         1388 Universitaria            3\n\n\n\n# Ordenar por satisfacción de mayor a menor\nencuesta %&gt;%\n  arrange(desc(Satisfaccion)) %&gt;% \n  head(10)\n\n# A tibble: 10 × 6\n      ID  Edad Genero Ingreso Educacion     Satisfaccion\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1     6    52 F         3540 Universitaria            5\n 2    44    57 M         4017 Secundaria               5\n 3    98    50 M         4023 Secundaria               5\n 4   122    26 F         2372 Universitaria            5\n 5   128    36 M         3722 Universitaria            5\n 6   161    46 F         4588 Universitaria            5\n 7   170    39 M         3747 Universitaria            5\n 8   212    37 M         4000 Universitaria            5\n 9   231    55 F         4903 Universitaria            5\n10   265    58 M         3794 Secundaria               5\n\n\n\n# Ordenar primero por género y luego por nivel educativo\nencuesta %&gt;%\n  arrange(Genero, desc(Educacion)) %&gt;% \n  head(10)\n\n# A tibble: 10 × 6\n      ID  Edad Genero Ingreso Educacion     Satisfaccion\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1     6    52 F         3540 Universitaria            5\n 2    11    47 F         3620 Universitaria            3\n 3    12    39 F         3258 Universitaria            2\n 4    13    39 F         3258 Universitaria            4\n 5    20    30 F         3244 Universitaria            3\n 6    24    28 F         1591 Universitaria            2\n 7    28    37 F         2711 Universitaria            2\n 8    37    41 F         3449 Universitaria            3\n 9    47    31 F         2744 Universitaria            3\n10    58    41 F         4131 Universitaria            3\n\n\n\nResumir datos (agrupados)\n\n\nreframe Calcula métricas agregadas como promedios, totales o máximos en devolver cualquier número de filas por grupo.\ngroup_by() agrupa los datos según una o más columnas. Esto significa que las filas se organizan en grupos definidos por los valores únicos de las columnas seleccionadas sin modificar el contenido del data framepor si solo.\n\nreframe\\: Calculamos la satisfacción promedio y el rango de edades por género, reduciendo los datos a una fila por grupo.\n\n# Calcular métricas resumidas por género\nencuesta %&gt;%\n  group_by(Genero) %&gt;% # Agrupar por género\n  reframe(\n# Promedio de satisfacción\n        SatisfaccionPromedio = mean(Satisfaccion)\n  ) \n\n# A tibble: 2 × 2\n  Genero SatisfaccionPromedio\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 F                      2.90\n2 M                      2.89\n\n\nreframe con group_by: Agrupar y resumir datos.\n\n# Calcular satisfacción promedio por género\nresumen_genero = encuesta %&gt;%\n  group_by(Genero) %&gt;%\n  reframe(SatisfaccionPromedio = mean(Satisfaccion))\nresumen_genero\n\n# A tibble: 2 × 2\n  Genero SatisfaccionPromedio\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 F                      2.90\n2 M                      2.89\n\n\n\n# Calcular edad máxima y mínima por nivel educativo\nresumen_educacion = encuesta %&gt;%\n  group_by(Educacion) %&gt;%\n  reframe(\n    EdadMaxima = max(Edad),\n    EdadMinima = min(Edad)\n  )\nresumen_educacion\n\n# A tibble: 3 × 3\n  Educacion     EdadMaxima EdadMinima\n  &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n1 Primaria              56         18\n2 Secundaria            61         18\n3 Universitaria         65         18\n\n\nPuedes realizar varias operaciones en una sola secuencia. Por ejemplo:\nFiltrar adultos, calcular satisfacción promedio por rango de edad y ordenar resultados.\n\n# Filtrar, agrupar y ordenar\nencuesta %&gt;%\n# Personas menores o iguales a 30 como \"Jovenes\"\n  mutate(RangoEdad = \n           ifelse(Edad &lt;= 30, \"Joven\", \"Adulto\")) %&gt;%\n# Agrupar por género\n      group_by(RangoEdad) %&gt;%   \n# Calcular métricas resumidas\n      summarise(             \n      SatisfaccionPromedio = mean(Satisfaccion)) %&gt;%\n# Ordenar por satisfacción promedio\n  arrange(desc(SatisfaccionPromedio)) \n\n# A tibble: 2 × 2\n  RangoEdad SatisfaccionPromedio\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Adulto                    3.01\n2 Joven                     2.62\n\n\nSí, hay varias funciones interesantes en dplyr que podrías agregar a tu apartado para ampliar las capacidades de manipulación de datos. Aquí te dejo algunas funciones adicionales que pueden mejorar el análisis y limpieza de datos:\n\nrename(): Renombrar columnas\nSi necesitas cambiar el nombre de una columna, puedes usar rename().\n\n\n\nencuesta %&gt;%\n  rename(Satisfaccion_Gestion = Satisfaccion) %&gt;% \n  head(3)\n\n# A tibble: 3 × 6\n     ID  Edad Genero Ingreso Educacion  Satisfaccion_Gestion\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n1     1    29 F         3219 Primaria                      3\n2     2    33 M         2595 Secundaria                    3\n3     3    51 F         3806 Secundaria                    3\n\n\n\nrelocate(): Reordenar columnas\nSi quieres cambiar el orden de las columnas en un data.frame, usa relocate().\n\n\nencuesta %&gt;%\n  relocate(Satisfaccion, .before = ID) %&gt;% \n  head(3)\n\n# A tibble: 3 × 6\n  Satisfaccion    ID  Edad Genero Ingreso Educacion \n         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;     \n1            3     1    29 F         3219 Primaria  \n2            3     2    33 M         2595 Secundaria\n3            3     3    51 F         3806 Secundaria\n\n\n\ndistinct(): Eliminar duplicados\nSi hay valores duplicados en el data.frame, distinct() los elimina y deja solo las observaciones únicas.\n\n\n\nencuesta %&gt;%\n  distinct() %&gt;% \n  head(3)\n\n# A tibble: 3 × 6\n     ID  Edad Genero Ingreso Educacion  Satisfaccion\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1     1    29 F         3219 Primaria              3\n2     2    33 M         2595 Secundaria            3\n3     3    51 F         3806 Secundaria            3\n\n\n\ncount(): Contar frecuencias de una variable categórica\nPuedes contar la cantidad de observaciones por categoría.\n\n\n\nencuesta %&gt;%\n  count(Educacion)\n\n# A tibble: 3 × 2\n  Educacion         n\n  &lt;chr&gt;         &lt;int&gt;\n1 Primaria        100\n2 Secundaria      152\n3 Universitaria   248\n\n\n\nslice(): Seleccionar filas específicas\nSi necesitas extraer ciertas filas según su posición:\n\n\n\n# Seleccionar las primeras 3 filas\nencuesta %&gt;%\n  slice(1:3)\n\n# A tibble: 3 × 6\n     ID  Edad Genero Ingreso Educacion  Satisfaccion\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1     1    29 F         3219 Primaria              3\n2     2    33 M         2595 Secundaria            3\n3     3    51 F         3806 Secundaria            3\n\n# Seleccionar la fila número 5\nencuesta %&gt;%\n  slice(5)\n\n# A tibble: 1 × 6\n     ID  Edad Genero Ingreso Educacion  Satisfaccion\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1     5    36 M         2707 Secundaria            4\n\n\n\ncase_when(): Crear variables con múltiples condiciones\nEs una versión más potente de ifelse(), útil cuando tienes muchas condiciones.\n\n\n\n# Crear una columna con niveles de satisfacción\nencuesta %&gt;%\n  mutate(Satisfaccion_Categoria = case_when(\n    Satisfaccion &gt;= 4 ~ \"Alta\",\n    Satisfaccion == 3 ~ \"Media\",\n    TRUE ~ \"Baja\"\n  )) %&gt;% \n  head(10)\n\n# A tibble: 10 × 7\n      ID  Edad Genero Ingreso Educacion     Satisfaccion Satisfaccion_Categoria\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                 \n 1     1    29 F         3219 Primaria                 3 Media                 \n 2     2    33 M         2595 Secundaria               3 Media                 \n 3     3    51 F         3806 Secundaria               3 Media                 \n 4     4    36 M         2907 Universitaria            3 Media                 \n 5     5    36 M         2707 Secundaria               4 Alta                  \n 6     6    52 F         3540 Universitaria            5 Alta                  \n 7     7    40 M         3506 Secundaria               4 Alta                  \n 8     8    22 F         1999 Secundaria               2 Baja                  \n 9     9    28 F         1381 Primaria                 3 Media                 \n10    10    31 F         2452 Secundaria               3 Media",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Glosario de funciones y operaciones útiles en R</span>"
    ]
  },
  {
    "objectID": "glosariofunciones.html#ggplot-para-la-elaboración-de-gráficos",
    "href": "glosariofunciones.html#ggplot-para-la-elaboración-de-gráficos",
    "title": "9  Glosario de funciones y operaciones útiles en R",
    "section": "9.5 Ggplot para la elaboración de gráficos",
    "text": "9.5 Ggplot para la elaboración de gráficos\nEl paquete ggplot2 es una herramienta poderosa para la visualización de datos en R. Se basa en capas (layers), lo que permite personalizar gráficos de manera progresiva.\n\nlibrary(ggplot2)\n\nHay algunos argumentos generales que pueden usarse en la mayoría de gráficos\n\n\n\nArgumento\nDescripción\n\n\n\n\ncolor\nColor del borde de elementos gráficos\n\n\nfill\nColor de relleno en elementos como barras o áreas\n\n\nalpha\nNivel de transparencia (0 a 1)\n\n\nsize\nTamaño de puntos, líneas o texto\n\n\nshape\nForma de los puntos (en geom_point())\n\n\nlinetype\nTipo de línea (continua, punteada, etc.)\n\n\nstroke\nGrosor del borde en puntos\n\n\n\n\nGráfico de Dispersión (geom_point())\n\n\nggplot(encuesta, aes(x = Edad, y = Ingreso)) +\n  geom_point(size = 3, \n             shape = 16, \n             color = \"blue\", \n             alpha = 0.7, \n             stroke = 1) +\n  labs(title = \"Gráfico de Dispersión\", \n       x = \"Edad\", \n       y = \"Ingreso\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n2.Histograma (geom_histogram())\n\nggplot(encuesta, aes(x = Ingreso)) +\n  geom_histogram(bins = 10, \n                 fill = \"steelblue\", \n                 color = \"black\", \n                 alpha = 0.7, \n                 boundary = 0) +\n  labs(title = \"Histograma de Ingresos\", \n       x = \"Ingreso\", \n       y = \"Frecuencia\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBoxplot (geom_boxplot())\n\n\nggplot(encuesta, aes(x = Genero, \n               y = Ingreso, \n               fill = Genero)) +\n  geom_boxplot(outlier.color = \"darkgreen\") +\n  labs(title = \"Boxplot de Ingreso por Género\", \n       x = \"Género\", \n       y = \"Ingreso\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nGráfico de Barras (geom_bar())\n\n\nggplot(encuesta, aes(x = Educacion, \n               fill = Educacion)) +\n  geom_bar(width = 0.7, \n           alpha = 0.8, \n           color = \"black\") +\n  labs(title = \"Frecuencia de Nivel Educativo\", \n       x = \"Nivel Educativo\", \n       y = \"Cantidad\",\n       fill = \"Niv. Ed.\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nGráfico de Líneas (geom_line())\n\n\nset.seed(123)\nencuesta %&gt;% \n  sample_n(20) %&gt;% \nggplot(aes(x = Edad, y = Ingreso)) +\n  geom_line(color = \"blue\", size = 1, linetype = \"dashed\") +\n  geom_point(size = 3, color = \"red\") +\n  labs(title = \"Relación Edad vs Ingreso\", x = \"Edad\", y = \"Ingreso\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nGráfico de Densidad (geom_density())\n\n\nggplot(encuesta, aes(x = Ingreso)) +\n  geom_density(fill = \"lightblue\", \n               alpha = 0.7, \n               adjust = 1.5, \n               linetype = \"solid\") +\n  labs(title = \"Densidad de Ingresos\", \n       x = \"Ingreso\", \n       y = \"Densidad\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nGráfico de Área (geom_area())\n\n\nggplot(encuesta, aes(x = Edad, \n               y = Ingreso)) +\n  geom_area(fill = \"darkgreen\", \n            alpha = 0.5, \n            linetype = \"dotted\", \n            color = \"black\") +\n  labs(title = \"Área de Relación Edad vs Ingreso\", \n       x = \"Edad\", \n       y = \"Ingreso\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nFacetas con facet_wrap()\n\n\nggplot(encuesta, aes(x = Ingreso, \n                     fill = Genero)) +\n  geom_histogram(bins = 10, \n                 alpha = 0.7, \n                 color = 'black') +\n  facet_wrap(~Genero) +\n  labs(title = \"Histograma de Ingresos por Género\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nGráfico de Barras Apilado (position = \"stack\")\n\n\nggplot(encuesta, aes(x = Educacion, \n                     fill = Genero)) +\n  geom_bar(position = \"stack\", \n           alpha = 0.8, \n           width = 0.7) +\n  labs(title = \"Distribución de Género por Nivel Educativo\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nGráfico de Barras Apilado Proporcional (position = \"fill\")\n\n\nggplot(encuesta, aes(x = Educacion, \n                     fill = Genero)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Proporción de Género por Nivel Educativo\", \n       y = \"Proporción\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nGráfico de Violin (geom_violin())\n\nUn gráfico de violín combina un boxplot y una curva de densidad, mostrando la distribución de una variable numérica a lo largo de una categórica. Permite visualizar la mediana, la dispersión y la forma de la distribución.\n\nggplot(encuesta, aes(x = Genero, \n                     y = Ingreso, \n                     fill = Genero)) +\n  geom_violin(trim = FALSE, \n              draw_quantiles = c(0.25, 0.5, 0.75)) +\n  labs(title = \"Distribución del Ingreso por Género\", \n       x = \"Género\", \n       y = \"Ingreso\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nGráfico de Dispersión con Regresión (geom_smooth())\n\n\nmethod = \"lm\": Usa regresión lineal.\nse = TRUE: Muestra el intervalo de confianza.\n\n\nset.seed(123)\nencuesta %&gt;% \n  sample_n(50) %&gt;% \nggplot(aes(x = Edad, y = Ingreso)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Regresión Lineal de Edad vs Ingreso\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Glosario de funciones y operaciones útiles en R</span>"
    ]
  },
  {
    "objectID": "glosariofunciones.html#complementos-de-ggplot2",
    "href": "glosariofunciones.html#complementos-de-ggplot2",
    "title": "9  Glosario de funciones y operaciones útiles en R",
    "section": "9.6 Complementos de ggplot2",
    "text": "9.6 Complementos de ggplot2\nEl sistema de visualización de datos en R basado en ggplot2 ofrece una gran flexibilidad y control para generar gráficos informativos. Sin embargo, siempre podemos ir un paso más allá, ya sea para mejorar la estética, destacar información clave o combinar varias visualizaciones. Para ello, existen una serie de paquetes que complementan a ggplot2 y amplían significativamente sus capacidades.\nUtilicemos los datos de gapminder filtrados al 2007 como ejemplo:\n\nlibrary(gapminder)\ngm_07 = gapminder %&gt;% filter(year == 2007)\n\n\nggsci(Xiao 2024)\n\nEste paquete proporciona paletas de colores inspiradas en publicaciones científicas reconocidas como Nature, JAMA, Lancet, NEJM, entre otras. Es útil cuando se busca una estética profesional y reconocible, especialmente para presentaciones o publicaciones.\nLa funciones principales son: scale_fill_nejm(), scale_fill_lancet(), scale_fill_jama() permiten aplicar directamente estas paletas en gráficos con fill.\n\nlibrary(ggsci)\n\nNJEJM:\n\nggplot(gm_07, aes(x = continent, \n                  y = lifeExp, \n                  fill = continent)) +\n  geom_boxplot() +\n  scale_fill_nejm() +\n  theme_minimal() +\n  ggtitle(\"Boxplot con paleta NEJM\")\n\n\n\n\n\n\n\n\nLancet:\n\nggplot(gm_07, aes(x = continent, \n                  y = lifeExp, \n                  fill = continent)) +\n  geom_boxplot() +\n  scale_fill_lancet() +\n  theme_minimal() +\n  ggtitle(\"Boxplot con paleta Lancet\")\n\n\n\n\n\n\n\n\nJAMA:\n\nggplot(gm_07, aes(x = continent, \n                  y = lifeExp, \n                  fill = continent)) +\n  geom_boxplot() +\n  scale_fill_jama() +\n  theme_minimal() +\n  ggtitle(\"Boxplot con paleta JAMA\")\n\n\n\n\n\n\n\n\n\nviridis\n\nviridis ofrece escalas de color perceptualmente uniformes, es decir, los colores cambian de manera coherente incluso para personas con discapacidad visual o en impresión en blanco y negro. Estas paletas fueron desarrolladas originalmente para Python y se adaptaron a R.\n\nLas funciones principales son: scale_fill_viridis_d() para variables categóricas y scale_fill_viridis_c() para variables continuas.\nLos argumentos más importantes son:\n\noption permite seleccionar entre variantes (“A”, “B”, “C”, “D”, “E”).\ndirection para invertir el orden de los colores.\nguide = 'none' oculta la leyenda, útil en gráficos facetados.\n\n\n\nlibrary(viridis)\n\nTema ‘plasma’:\n\nggplot(gm_07, aes(x = lifeExp, \n                  fill = continent)) +\n  geom_histogram(binwidth = 3, \n                 color = \"black\", \n                 alpha = 0.8) +\n  scale_fill_viridis_d(guide = 'none', \n                       option = 'C') +\n  facet_wrap(~ continent) +\n  theme_minimal() +\n  ggtitle(\"Histograma de esperanza de vida por continente\")\n\n\n\n\n\n\n\n\nTema ‘cividis’:\n\nggplot(gm_07, aes(x = lifeExp, \n                  fill = continent)) +\n  geom_histogram(binwidth = 3, \n                 color = \"black\", \n                 alpha = 0.8) +\n  scale_fill_viridis_d(guide = 'none', \n                       option = 'E') +\n  facet_wrap(~ continent) +\n  theme_minimal() +\n  ggtitle(\"Histograma de esperanza de vida por continente\")\n\n\n\n\n\n\n\n\nPuede consultar las paletas usando ?scale_fill_viridis\n\npatchwork (Pedersen 2024)\n\nEste paquete permite combinar múltiples gráficos generados con ggplot2 de forma intuitiva mediante operadores aritméticos. Es especialmente útil para mostrar comparaciones o construir narrativas visuales. - Los gráficos se combinan con / (vertical), | (horizontal), y + para anotaciones conjuntas. - No requiere transformar los gráficos en objetos de grid. Basta con construir gráficos individuales y luego combinarlos directamente.\n\nlibrary(patchwork)\n\n\n# Boxplot de lifeExp \np1 = ggplot(gm_07, aes(x = lifeExp, \n                       y = '')) +\n  geom_boxplot(fill = \"darkgreen\", \n               width = 0.4) +\n  theme_minimal() +\n  labs(title = \"Boxplot de esperanza de vida\", \n       x = NULL, \n       y = \"lifeExp\")\n\n# Densidad de lifeExp\np2 = ggplot(gm_07, aes(x = lifeExp)) +\n  geom_density(fill = \"darkgreen\", \n               alpha = 0.6) +\n  theme_minimal() +\n  labs(title = \"Densidad de esperanza de vida\", \n       x = \"lifeExp\", \n       y = \"Densidad\")\n\n# Histograma de lifeExp\np3 = ggplot(gm_07, aes(x = lifeExp)) +\n  geom_histogram(binwidth = 3, \n                 fill = \"darkgreen\", \n                 color = \"black\", \n                 alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Histograma de esperanza de vida\", \n       x = \"lifeExp\", \n       y = \"Frecuencia\")\n\n# Gráficos apilados verticalmente\n(p1 / p2 / p3)\n\n\n\n\n\n\n\n\n\nggthemes (Arnold 2024)\n\nProporciona temas visuales inspirados en medios de comunicación como The Economist, Wall Street Journal o FiveThirtyEight. Estos temas cambian fondo, ejes, tipografía y estilo general del gráfico. Son ideales para producir gráficos estilizados sin necesidad de personalizar manualmente cada elemento gráfico.\n\nlibrary(ggthemes)\n\nThe Economist:\n\nggplot(gm_07, aes(x = continent, \n                  y = lifeExp, \n                  fill = continent)) +\n  geom_boxplot() +\n  theme_economist() +\n  scale_fill_economist() +\n  labs(fill = 'Continente') +\n  ggtitle(\"Estilo The Economist\")\n\n\n\n\n\n\n\n\nWall Street Journal:\n\nggplot(gm_07, aes(x = continent, \n                  y = lifeExp, \n                  fill = continent)) +\n  geom_boxplot() +\n  theme_wsj() +\n  scale_fill_economist() +\n  labs(fill = 'Continente') +\n  ggtitle(\"Estilo Wall Street Journal\")\n\n\n\n\n\n\n\n\n\nggtext (Wilke y Wiernik 2022)\n\nEste paquete permite utilizar etiquetas enriquecidas con HTML y Markdown dentro de ggplot2. Esto incluye títulos con colores, negritas, saltos de línea y estilos diferenciados por palabra.\n\nFunciones clave:\n\nelement_markdown() reemplaza a element_text() en theme() para permitir etiquetas enriquecidas.\n\nRequiere que las etiquetas sean pasadas a ggtitle() o labs() como texto procesable por glue::glue() o expresiones con HTML/Markdown.\n\n\nlibrary(ggtext)\n\nEstablecemos la relación entre color y continente:\n\n# Colores personalizados para cada continente\ncolores_continente = c(\n  Africa = \"#E63946\",\n  Americas = \"#457B9D\",\n  Asia = \"#2A9D8F\",\n  Europe = \"#F4A261\",\n  Oceania = \"#A17DB2\"\n)\n\n# Creamos el título con etiquetas coloreadas en HTML\ntitulo_color = glue::glue(\n  \"Esperanza de vida en 2007 por continente&lt;br&gt;\",\n  \"&lt;span style='color:{colores_continente['Africa']}'&gt;**África**&lt;/span&gt;, \",\n  \"&lt;span style='color:{colores_continente['Americas']}'&gt;**Américas**&lt;/span&gt;, \",\n  \"&lt;span style='color:{colores_continente['Asia']}'&gt;**Asia**&lt;/span&gt;, \",\n  \"&lt;span style='color:{colores_continente['Europe']}'&gt;**Europa**&lt;/span&gt;, \",\n  \"&lt;span style='color:{colores_continente['Oceania']}'&gt;**Oceanía**&lt;/span&gt;\"\n)\n\nGraficamos:\n\n# Gráfico final\nggplot(gm_07, aes(x = continent, \n                  y = lifeExp, \n                  fill = continent)) +\n  geom_boxplot() +\n  scale_fill_manual(values = colores_continente) +\n  labs(y = 'Esperanza de vida') +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    plot.title = element_markdown(size = 14, \n                                  lineheight = 1.2),\n    legend.position = \"none\"\n  ) +\n  ggtitle(titulo_color)\n\n\n\n\n\n\n\n\n\nscales (Wickham, Pedersen, y Seidel 2023)\n\nOfrece herramientas para transformar y dar formato a valores numéricos en los ejes o leyendas de ggplot2. Muy útil para mostrar cifras monetarias, porcentajes o unidades más legibles.\n\nExisten funciones muy comunes como: dollar_format(), percent_format(), comma_format(), entre otras.\nAlgunos argumentos importantes son: prefix o suffix permiten añadir símbolos o unidades y accuracy controla el número de decimales.\n\n\nlibrary(scales)\n\nPodemos utilizarlo para monedas:\n\nggplot(gm_07, aes(x = gdpPercap,\n                  y = lifeExp)) +\n  geom_point(color = \"steelblue\", \n             size = 3, \n             alpha = 0.8) +\n  scale_x_continuous(labels = dollar_format(prefix = \"US$\")) +\n  theme_minimal() +\n  ggtitle(\"Relación entre PIB per cápita y esperanza de vida (USD, 2007)\") +\n  labs(x = \"PIB per cápita (US$)\", \n       y = \"Esperanza de vida (años)\")\n\n\n\n\n\n\n\n\nO para porcentajes:\n\n# Clasificamos esperanza de vida\ngm_07 = gm_07 %&gt;%\n  mutate(nivel = cut(lifeExp, breaks = c(0, 60, 75, Inf), \n                     labels = c(\"Baja\", \"Media\", \"Alta\")))\n\n# Gráfico debarra horizontal dividida por proporción y continente\nggplot(gm_07, aes(y = continent, \n                  fill = nivel)) +\n  geom_bar(position = \"fill\", \n           color = \"black\") +\n  scale_x_continuous(labels = percent_format(accuracy = 1), \n                     trans = \"reverse\") +\n  labs(title = \"Distribución proporcional de esperanza de vida por continente (2007)\",\n       x = \"Porcentaje\",\n       y = NULL,\n       fill = \"Esperanza de vida\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ngghighlight (Yutani 2023)\n\nFacilita la tarea de destacar subconjuntos de datos directamente en los gráficos sin tener que filtrar manualmente. Ideal para llamar la atención sobre casos atípicos, valores extremos o categorías específicas. - gghighlight() se añade como capa dentro del ggplot y permite condicionar el resaltado a una expresión lógica. - Argumentos útiles: - label_key permite mostrar etiquetas automáticas para los datos resaltados. - use_direct_label = TRUE mejora la legibilidad de gráficos con muchos elementos.\n\nlibrary(gghighlight)\n\n\nggplot(gm_07, aes(x = log(gdpPercap), \n                  y = lifeExp, \n                  label = country)) +\n  geom_point(size = 3, \n             alpha = 0.8) +\n  gghighlight(lifeExp &gt; 82, \n              label_key = country) +\n  theme_minimal() +\n  ggtitle(\"Países con esperanza de vida mayor a 82 en 2007\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Glosario de funciones y operaciones útiles en R</span>"
    ]
  },
  {
    "objectID": "glosariofunciones.html#dlookr-para-análisis-exploratorio",
    "href": "glosariofunciones.html#dlookr-para-análisis-exploratorio",
    "title": "9  Glosario de funciones y operaciones útiles en R",
    "section": "9.7 Dlookr para análisis exploratorio",
    "text": "9.7 Dlookr para análisis exploratorio\nEl análisis exploratorio de datos (EDA) es una etapa crucial antes de aplicar cualquier modelo o técnica inferencial. Permite comprender la estructura de los datos, detectar valores atípicos, evaluar supuestos y descubrir patrones relevantes. El paquete dlookr fue diseñado específicamente para facilitar esta etapa.\n\nlibrary(dlookr)\n\n\nResumen de Datos y Diagnóstico con dlookr\n\nEl paquete dlookr permite realizar diagnósticos detallados sobrela información en cada una de las variables en un data.frame.\n\nNúmero de observaciones\nCantidad de valores perdidos por variable\nNúmero de valores únicos\nTipo de dato de cada variable\n\n\ndiagnose(encuesta)\n\n# A tibble: 6 × 6\n  variables    types     missing_count missing_percent unique_count unique_rate\n  &lt;chr&gt;        &lt;chr&gt;             &lt;int&gt;           &lt;dbl&gt;        &lt;int&gt;       &lt;dbl&gt;\n1 ID           numeric               0               0          500       1    \n2 Edad         numeric               0               0           44       0.088\n3 Genero       character             0               0            2       0.004\n4 Ingreso      numeric               0               0          451       0.902\n5 Educacion    character             0               0            3       0.006\n6 Satisfaccion numeric               0               0            5       0.01 \n\n\n\nResumen de Variables Numéricas con dlookr\n\nPara obtener un resumen detallado de las variables numéricas, usamos diagnose_numeric().\n\nMedia, mediana, desviación estándar y varianza\nValores mínimo y máximo\nCoeficiente de asimetría (Skewness) y curtosis\nValores atípicos detectados\n\n\ndiagnose_numeric(encuesta)\n\n# A tibble: 4 × 10\n  variables      min    Q1    mean median    Q3   max  zero minus outlier\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n1 ID               1  126.  250.     250.  375.   500     0     0       0\n2 Edad            18   29    35.5     35    42     65     0     0       1\n3 Ingreso       1000 2322. 2785.    2746. 3237.  4903     0     0       1\n4 Satisfaccion     1    2     2.89     3     3      5     0     0      15\n\n\n\nResumen de Variables Categóricas con dlookr\n\nPara describir las variables categóricas, usamos diagnose_category().\n\n# Diagnóstico de variables categóricas\ndiagnose_category(encuesta)\n\n# A tibble: 5 × 6\n  variables levels            N  freq ratio  rank\n  &lt;chr&gt;     &lt;chr&gt;         &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n1 Genero    F               500   251  50.2     1\n2 Genero    M               500   249  49.8     2\n3 Educacion Universitaria   500   248  49.6     1\n4 Educacion Secundaria      500   152  30.4     2\n5 Educacion Primaria        500   100  20       3\n\n\nEste diagnóstico proporciona:\nFrecuencias absolutas y relativas de cada categoría Cantidad de valores únicos en cada variable categórica\nSi queremos visualizar mejor la distribución de una variable categórica, podemos utilizar count():\n\ncount(encuesta, Educacion)\n\n# A tibble: 3 × 2\n  Educacion         n\n  &lt;chr&gt;         &lt;int&gt;\n1 Primaria        100\n2 Secundaria      152\n3 Universitaria   248\n\n\n\nIdentificación de Valores Atípicos con dlookr\n\ndlookr también nos permite detectar valores atípicos en las variables numéricas.\nEsto mostrará:\n\nCuántos valores atípicos existen\nCuáles son esos valores extremos\n\n\ndiagnose_outlier(encuesta)\n\n# A tibble: 4 × 6\n  variables    outliers_cnt outliers_ratio outliers_mean with_mean without_mean\n  &lt;chr&gt;               &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 ID                      0            0             NaN    250.         250.  \n2 Edad                    1            0.2            65     35.5         35.4 \n3 Ingreso                 1            0.2          4903   2785.        2780.  \n4 Satisfaccion           15            3               5      2.89         2.83\n\n\n\nGráficos Descriptivos con dlookr*\n\nPodemos graficar la distribución de nuestras variable numérica con plot_hist_numeric().\n\nplot_hist_numeric(encuesta)\n\n\n\n\n\n\n\n\nPodemos usar también plot_box_numeric() para ver valores atípicos y la distribución de la variables numéricas.\n\nplot_box_numeric(encuesta)\n\n\n\n\n\n\n\n\nPodemos usar también plot_normality para evaluar la normalidad de cada variable numérica y sus tranformaciones.\n\nplot_normality(encuesta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos visualizar la frecuencia del variables categóricas con plot_bar_category().\n\nplot_bar_category(encuesta)\n\n\n\n\n\n\n\n\n\nAnálisis Exploratorio de Correlaciones con correlate\n\nPara examinar relaciones entre variables numéricas, podemos usar correlate().\n\n# Matriz de correlaciones\nencuesta %&gt;% \n  select(Edad, Ingreso, Satisfaccion) %&gt;% \n  correlate()\n\n# A tibble: 6 × 3\n  var1         var2         coef_corr\n  &lt;fct&gt;        &lt;fct&gt;            &lt;dbl&gt;\n1 Ingreso      Edad             0.682\n2 Satisfaccion Edad             0.284\n3 Edad         Ingreso          0.682\n4 Satisfaccion Ingreso          0.500\n5 Edad         Satisfaccion     0.284\n6 Ingreso      Satisfaccion     0.500\n\n\nPara visualizar las correlaciones, utilizamos plot_correlate():\n\n# Gráfico de correlaciones\nencuesta %&gt;% \n  select(Edad, Ingreso, Satisfaccion) %&gt;% \n  plot_correlate()\n\n\n\n\n\n\n\n\n\n\n\n\nArnold, Jeffrey B. 2024. «ggthemes: Extra Themes, Scales and Geoms for ’ggplot2’». https://CRAN.R-project.org/package=ggthemes.\n\n\nPedersen, Thomas Lin. 2024. «patchwork: The Composer of Plots». https://CRAN.R-project.org/package=patchwork.\n\n\nWickham, Hadley, Thomas Lin Pedersen, y Dana Seidel. 2023. «scales: Scale Functions for Visualization». https://CRAN.R-project.org/package=scales.\n\n\nWilke, Claus O., y Brenton M. Wiernik. 2022. «ggtext: Improved Text Rendering Support for ’ggplot2’». https://CRAN.R-project.org/package=ggtext.\n\n\nXiao, Nan. 2024. «ggsci: Scientific Journal and Sci-Fi Themed Color Palettes for ’ggplot2’». https://CRAN.R-project.org/package=ggsci.\n\n\nYutani, Hiroaki. 2023. «gghighlight: Highlight Lines and Points in ’ggplot2’». https://CRAN.R-project.org/package=gghighlight.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Glosario de funciones y operaciones útiles en R</span>"
    ]
  },
  {
    "objectID": "solucionario.html",
    "href": "solucionario.html",
    "title": "Solucionario",
    "section": "",
    "text": "Capítulo 4\n\na\nb\nd\nb\na\na y b\nb\nc\n\nCapítulo 5\n\nd\na\nb\na\na\nd\nb\nd\nc\nb\n\nCapítulo 6\n\nc\nb\nc\nb\nc\nc\nb\na\nb\n\nCapítulo 7\n\nd\na\nb\nb\na\na\nc\nb\nc\nb\nd\nc\n\nCapítulo 8\n\na\nc\nb\na\nb\nd\nb\nb\nb\na\nb\na\nc\nd\nb\n\nCapítulo 9\n1. c\n2. b\n3. b\n4. b\n5. a\n6. d\n7. d\n8. b\n9. a\n10. b\n11. b\n12. b\n13. d\n14. a\n15. b\nCapítulo 10\n\nc\na\nb\nd\na\nb\na\nc\nb\na\nd\na\nb\na\nd\n\nCapítulo 11\n\nc\nb\nc\nb\nc\nb\nc\nc\nb\nb",
    "crumbs": [
      "Solucionario"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bilbiografía",
    "section": "",
    "text": "Agresti, Alan. 2018. Statistical Methods for the Social\nSciences. Fifth edition. Boston: Pearson.\n\n\nAgresti, Alan, Christine A. Franklin, and Bernhard Klingenberg. 2023.\nStatistics: the art and science of learning from data. Fifth\nedition, global edition. Harlow, United Kingdom: Pearson.\n\n\nArnold, Jeffrey B. 2024. “Ggthemes: Extra Themes, Scales and Geoms\nfor ’Ggplot2’.” https://CRAN.R-project.org/package=ggthemes.\n\n\nBrunson, Jason Cory. 2020. Ggalluvial: Alluvial Plots in\n’Ggplot2’. https://CRAN.R-project.org/package=ggalluvial.\n\n\nBryan, Jennifer. 2023. “Gapminder: Data from Gapminder.” https://CRAN.R-project.org/package=gapminder.\n\n\nCasella, George, and Roger W. Berger. 2024. Statistical\nInference. CRC Press.\n\n\nÇetinkaya-Rundel, Mine, and Joanna Hardin. 2021. Introduction to\nModern Statistics.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski,\nBenjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021.\n“Infer: An r\nPackage for Tidyverse-Friendly Statistical Inference” 6: 3661. https://doi.org/10.21105/joss.03661.\n\n\nDataCamp. 2023. “All about r: A Popular Language for Data\nScience.” https://www.datacamp.com/blog/all-about-r.\n\n\nOtzen, Tamara, and Carlos Manterola. 2017. “Técnicas de Muestreo\nSobre Una Población a Estudio.” International Journal of\nMorphology 35 (1): 227–32. https://doi.org/10.4067/s0717-95022017000100037.\n\n\nPedersen, Thomas Lin. 2024. “Patchwork: The Composer of\nPlots.” https://CRAN.R-project.org/package=patchwork.\n\n\nRagin, Charles C. 2007. Construcción de La Investigación Social:\nIntroducción a Los Métodos y Su Diversidad. Bogotá, Colombia: Siglo\ndel Hombre Editores.\n\n\nRyu, Choonghyun. 2024. “Dlookr: Tools\nfor Data Diagnosis, Exploration, Transformation.” https://CRAN.R-project.org/package=dlookr.\n\n\nSpeegle, Deborah, and Benjamin Clair. 2021. Probability, Statistics,\nand Data: A Fresh Approach Using r. Boca Raton, FL: CRC Press.\n\n\nWickham, Hadley. 2016. “Ggplot2: Elegant Graphics for Data\nAnalysis.” https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse” 4:\n1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. “Readxl: Read Excel\nFiles.” https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. “Dplyr: A Grammar of Data Manipulation.” https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. “Haven:\nImport and Export ’SPSS’, ’Stata’ and ’SAS’ Files.” https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2023.\n“Scales: Scale Functions for Visualization.” https://CRAN.R-project.org/package=scales.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2024.\n“Tidyr: Tidy Messy Data.” https://CRAN.R-project.org/package=tidyr.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. “Ggtext: Improved\nText Rendering Support for ’Ggplot2’.” https://CRAN.R-project.org/package=ggtext.\n\n\nXiao, Nan. 2024. “Ggsci: Scientific Journal and Sci-Fi Themed\nColor Palettes for ’Ggplot2’.” https://CRAN.R-project.org/package=ggsci.\n\n\nYutani, Hiroaki. 2023. “Gghighlight: Highlight Lines and Points in\n’Ggplot2’.” https://CRAN.R-project.org/package=gghighlight.",
    "crumbs": [
      "Bilbiografía"
    ]
  }
]